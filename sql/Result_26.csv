paper_id,abstract
5cee90b85b88e4de1d51b2963613a48b68916ac7,"Abstract This article does not describe a working system. Instead, it presents a single idea about representation that allows advances made by several different groups to be combined into an imaginary system called GLOM.1 The advances include transformers, neural fields, contrastive representation learning, distillation, and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy that has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language."
87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86,How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?
01a17af53eb629878ec9757086f344bb042730c9,"Capsule networks are designed to parse an image into a hierarchy of objects, parts and relations. While promising, they remain limited by an inability to learn effective low level part descriptions. To address this issue we propose a novel self-supervised method for learning part descriptors of an image. During training, we exploit motion as a powerful perceptual cue for part definition, using an expressive decoder for part generation and layered image formation with occlusion. Experiments demonstrate robust part discovery in the presence of multiple objects, cluttered backgrounds, and significant occlusion. The resulting part descriptors, a.k.a. part capsules, are decoded into shape masks, filling in occluded pixels, along with relative depth on single images. We also report unsupervised object classification using our capsule parts in a stacked capsule autoencoder."
01c304c4c731705f371e0d0024a95b136a805d41,"The most important unsolved problem with artificial neural networks is how to do unsupervised learning as effectively as the brain. There are currently two main approaches to unsupervised learning. In the first approach, exemplified by BERT and Variational Autoencoders, a deep neural network is used to reconstruct its input. This is problematic for images because the deepest layers of the network need to encode the fine details of the image. An alternative approach, introduced by Becker and Hinton in 1992, is to train two copies of a deep neural network to produce output vectors that have high mutual information when given two different crops of the same image as their inputs. This approach was designed to allow the representations to be untethered from irrelevant details of the input. The method of optimizing mutual information used by Becker and Hinton was flawed (for a subtle reason that I will explain) so Pacannaro and Hinton (2001) replaced it by a discriminative objective in which one vector representation must select a corresponding vector representation from among many alternatives. With faster hardware, contrastive learning of representations has recently become very popular and is proving to be very effective, but it suffers from a major flaw: To learn pairs of representation vectors that have N bits of mutual information we need to contrast the correct corresponding vector with about 2N incorrect alternatives. I will describe a novel and effective way of dealing with this limitation. I will also show that this leads to a simple way of implementing perceptual learning in cortex."
16fd3bcea628a20d273c35d40447fba3b3aa4774,"We propose an unsupervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. In doing so, we require neither classification labels nor manually-aligned training datasets to train. Yet, by learning an object-centric representation in an unsupervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, registration, and unsupervised classification. We will release the code and dataset to reproduce our results as soon as the paper is published."
1ac1d3eb086c42d72a0509a42f744f626e8b5711,"Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, meta-learned information helpful for training on a particular task or dataset. We present an efficient and scalable gradient-based method to learn commentaries, leveraging recent work on implicit differentiation. We explore diverse applications of commentaries, from learning weights for individual training examples, to parameterizing label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. In these settings, we find that commentaries can improve training speed and/or performance and also provide fundamental insights about the dataset and training process."
34733eaf66007516347a40ad5d9bbe1cc9dacb6b,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
3e7f5f4382ac6f9c4fef6197dd21abf74456acd1,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels."
547c32472f6574de37cacd6a0d626155a1b1d48f,"Digital Image Processing is an expeditiously emerging field possessing a large number of applications in science and engineering aspects. One of the most used applications in almost every sector is Optical Character Recognition (OCR). OCR is the electronic conversion of handwritten text into digital format which makes information processing from printed papers to data records easy, thus helping to electronically edit, search and store printed texts into machines. This text can then be used in variety of applications like machine translation, speech-to-text, pattern recognition etc. OCR as a piece of software applies pre-processing to improve the recognition in images. This pre-processing step includes skewness correction, despeckling, layout analysis and line and word detection. OCR saves tons of manual effort by recognizing handwritten text with word level detection resulting in an accuracy of 81% to 90%. With form processing, one can capture information in digital format that can save time, labor and money. This helps in achieving a better accuracy in detection. Such systems range from minor application forms to large scale survey forms. Deep Learning algorithms dealing with computer vision related tasks can be used to build a recognition engine."
598658595e887f677967769ed11ba28158f6ea8a,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems."
a0d7101f1798098467c946c21fde078495b9a7f4,"Spacing presentations of learning items across time improves memory relative to massed schedules of practice – the well-known spacing effect. Spaced practice can be further enhanced by adaptively scheduling the presentation of learning items to deliver customized spacing intervals for individual items and learners. ARTS Adaptive Response-time-based Sequencing (Mettler, Massey, & Kellman 2016) determines spacing dynamically in relation to each learner’s ongoing speed and accuracy in interactive learning trials. We demonstrate the effectiveness of ARTS when applied to chemistry nomenclature in community college chemistry courses by comparing adaptive schedules to fixed schedules consisting of continuously expanding spacing intervals. Adaptive spacing enhanced the efficiency and durability of learning, with learning gains persisting after a two-week delay and generalizing to a standardized assessment of chemistry knowledge after 2-3 months. Two additional experiments confirmed and extended these results in both laboratory and community college settings."
bb2bc99f8220fc681320c541940c99ae30b286d6,"This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generative model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER."
e5f3569873903e308a8d55586b9d6110e840d011,"This paper is about the dark realm of knowledge that is nuclear science. I discuss the uses and abuses of this knowledge over the last century, and an installation project I developed that seeks to manifest such dark knowledge, titled Demon Core (2019). To enframe this discussion, I shall start with some quotes from a somewhat dark materialist philosopher Martin Heidegger and his key text, The Question Concerning Technology (1977). Heidegger says that technology is no mere means. It's a way of “ revealing ” , which is a sort of knowing in the wider material sense - through our interactions with matter, working with matter, making matter do things, one gains material knowledge. Technology, the tools and techniques we use to work with matter, is a mode of “ revealing ” aspects or attributes of nature. Heidegger asks ‘ What is modern technology? It too is a revealing. …And yet … The revealing that rules in modern technology is a challenging’ .[1] Con-temporary technology is different to traditional technologies, as it's based on modern science and engineering. Technology challenges nature and drives it to do certain things, such as taking the energy that naturally exists within a river and using it through the technology of a dam. Technology based upon modern physics takes this to a much more fundamental level: ‘ The Earth is now set upon to yield uranium. Uranium is set upon to yield Atomic Energy, which can be released either for destruction or for peaceful use … . This setting upon that challenges for the energies of nature is an expediting in two ways. It unlocks and exposes … That challenging happens in that the energy concealed in nature is unlocked, what is unlocked is trans-formed…’ .[2] Through such challenging and transforming of materials and energies, technology enframes nature, challenges it, summoning it forth and forcing it to work for us, in ways against its innate tendencies . A darkly archetypal example of this is the atom bomb. The 20th and"
13de3c06ef6dac1c296ada45df2be590f843edb7,"Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first learns a large net and then prunes away connections or hidden units. But standard training does not necessarily encourage nets to be amenable to pruning. We introduce targeted dropout, a method for training a neural network so that it is robust to subsequent pruning. Before computing the gradients for each weight update, targeted dropout stochastically selects a set of units or weights to be dropped using a simple self-reinforcing sparsity criterion and then computes the gradients for the remaining weights. The resulting network is robust to post hoc pruning of weights or units that frequently occur in the dropped sets. The method improves upon more complicated sparsifying regularisers while being simple to implement and easy to tune."
1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8,"We explore and expand the $\textit{Soft Nearest Neighbor Loss}$ to measure the $\textit{entanglement}$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that $\textit{maximizing}$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class."
2adc55831bbb3586855cd48cca55187c7382bdce,"To generalize to novel visual scenes with new viewpoints and new object poses, a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3D graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. It is possible to learn to invert the process that converts 3D graphics representations into 2D images, provided the 3D graphics representations are available as labels. When only the unlabeled images are available, however, learning to derender is much harder. We consider a simple model which is just a set of free floating parts. Each part has its own relation to the camera and its own triangular mesh which can be deformed to model the shape of the part. At test time, a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. Each part can be viewed as one head of a multi-headed derenderer. During training, the extracted parts are used as input to a differentiable 3D renderer and the reconstruction error is backpropagated to train the neural net. We make the learning task easier by encouraging the deformations of the part meshes to be invariant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. Cerberus, our multi-headed derenderer, outperforms previous methods for extracting 3D parts from single images without part annotations, and it does quite well at extracting natural parts of human figures."
7da2fef4667cb2dcb2efb48bdab3a2e2d7870d1c,
46c53faeaf2f52215adb165559c5ce056a71146b,"Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%). The code is available at this https URL"
497b30dfe1edb3c534f0579784c4806ee6a2d8bf,
4c017ad500ce2170c2ff45b71150c2106fd4695b,"Living things on Earth depend on heme – the ironcyclic tetrapyrrole complex that harnesses iron’s oxidizing powers. Heme is toxic, but Nature has evolved ways to control it. One way is breaking it with heme oxygenase which lowers its levels and begins the formation of linear tetrapyrroles called bilins. Bilins occur in many variations, often colorful, sometimes in abundance, and in animals, plants and microbes. Contrary to early notions, bilins are not only waste products of heme degradation. They are increasingly appreciated for their diverse roles such as sensing and gathering light, regulating growth and aging, responding to inflammatory conditions, and influencing behavior. The diverse functions of bilins are exploited with discoveries and uses of bioactive bilins for salutary benefits in medicine and agriculture. Opportunities for finding new bioactive bilins and applications will grow as knowledge of bilin biology and capabilities for producing bilins continue to expand."
600be3dde18d1059c6b56170bd04ee65ce79a848,"The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank."
726320cdbd04804ffa8f3a78c095bd1b55a2a695,"Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations."
76ec395f445c5ec6ad4a97bbf0bb6f1312b77f10,
d2b62f77cb2864e465aa60bca6c26bb1d2f84963,"Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models."
d4599b177559dd5ede4dda9d6d96aa149fc71942,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned."
e33cbb25a8c7390aec6a398e36381f4f7770c283,"Most current speech recognition systems use hidden Markov models ( HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternati ve way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients a s input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech rec ognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and repres nts the shared views of four research groups who have had recent successes in using deep neural networks for a coustic modeling in speech recognition."
ed332c92664cd64843a7ba9373d992e9547230f6,"A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set."
01a382bc3778d8eaf1083a6e9dee5139e2321ec6,"Abstract In higher animals, complex and robust behaviors are produced by the microscopic details of large structured ensembles of neurons. I describe how the emergent computational dynamics of a biologically based neural network generates a robust natural solution to the problem of categorizing time-varying stimulus patterns such as spoken words or animal stereotypical behaviors. The recognition of these patterns is made difficult by their substantial variation in cadence and duration. The neural circuit behaviors used are similar to those associated with brain neural integrators. In the larger context described here, this kind of circuit becomes a building block of an entirely different computational algorithm for solving complex problems. While the network behavior is simulated in detail, a collective view is essential to understanding the results. A closed equation of motion for the collective variable describes an algorithm that quantitatively accounts for many aspects of the emergent network computation. The feedback connections and ongoing activity in the network shape the collective dynamics onto a reduced dimensionality manifold of activity space, which defines the algorithm and computation actually performed. The external inputs are weak and are not the dominant drivers of network activity."
ac164903c111cfdebad6c8b5e8c37b4cd3e5cb9b,"There has been an ongoing cycle where stronger defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach towards ending this cycle where we ""deflect'' adversarial attacks by causing the attacker to produce an input that semantically resembles the attack's target class. To this end, we first propose a stronger defense based on Capsule Networks that combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense often perceptually resemble the adversarial target class by performing a human study where participants are asked to label images produced by the attack. These attack images can no longer be called ""adversarial'' because our network classifies them the same way as humans do."
cc5ba823330e57facf8ea04c873c824874dd13fd,"Any solid object can be decomposed into a collection of convex polytopes (in short, convexes). When a small number of convexes are used, such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental in computer graphics, where it provides one of the most common ways to approximate geometry, for example, in real-time physics simulation. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull, or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training, as they abstract away from the topology of the geometry they need to represent. However, at testing time, convexes can also generate explicit representations – polygonal meshes – which can then be used in any downstream application. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an auto-encoding process. We investigate the applications of this architecture including automatic convex decomposition, image to 3D reconstruction, and part-based shape retrieval."
da44a855047d704452bc8ddf0a615a35ee2d4ef0,"To generalize to novel visual scenes with new viewpoints and new object poses, a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3D graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. It is possible to learn to invert the process that converts 3D graphics representations into 2D images, provided the 3D graphics representations are available as labels. When only the unlabeled images are available, however, learning to derender is much harder. We consider a simple model which is just a set of free floating parts. Each part has its own relation to the camera and its own triangular mesh which can be deformed to model the shape of the part. At test time, a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. Each part can be viewed as one head of a multi-headed derenderer. During training, the extracted parts are used as input to a differentiable 3D renderer and the reconstruction error is backpropagated to train the neural net. We make the learning task easier by encouraging the deformations of the part meshes to be invariant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. Cerberus, our multi-headed derenderer, outperforms previous methods for extracting 3D parts from single images without part annotations, and it does quite well at extracting natural parts of human figures."
f368de6a7f90daec66e1eef7922773390b75fb9d,"Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples."
f8de25118af2abc4c48afb947d6ec298e05ef1e5,"The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
35ebe95db7ab148e25904604d3b06a9412f6b4a4,"We introduce Picturebook, a large-scale lookup operation to ground language via ‘snapshots’ of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings."
5bc3f4b4f976ac888ea767cc12326b4e1348e03e,"We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images, the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the $l2$ distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger, white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class, the attack must typically make the ""adversarial"" image resemble images of the other class."
603caed9430283db6c7f43169555c8d18e97a281,
80536dcccf0adb2a5132541d558e865ad00ef59e,"A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network."
a2b02b8a0052987062bafc1de1b133c96d155708,"In 2017, 10,748 patients in the acute care setting experienced a sentinel event (The Joint Commission, 2017). As reported by the Joint Commission, 70% of these events could be avoided with a clear understanding and proper communication of the roles and responsibilities of each member of the healthcare team. With clearly defined roles and responsibilities of each team member, nurses can better utilize each member to increase the quality of patient-centered care delivery. To facilitate rich interprofessional communication among students of various health care disciplines (nursing, social work, nutrition, pre-OT/PT, and child life), an Interprofessional Case Study Event was conducted. Participants worked through three phases of care from a case study scenario in which fostering effective communication among the students was vital. We hypothesize that participation in an Interprofessional Case Study Event at the undergraduate level will increase students’ knowledge of the roles and responsibilities of the various health care team members.  Design: A quantitative one-group, pre/post test case study event was conducted with various undergraduate health care students enrolled at Tennessee Technological University. Data were collected using the SPICE-R2 self-report questionnaire.  Results: Our analysis supports a statistically significant difference in students’ perceptions of the roles and responsibilities for collaborative practice pre/post Interprofessional Case Study Event. Conclusion: The implications of this study suggests that providing collaborative learning opportunities for students of health disciplines will increase the understanding of roles and responsibilities of each health professional, resulting in more effective interdisciplinary communication that will lead to improved patient care."
c7d3e126617e18b23337e678ffcfbc8cb2353884,
c898161e3691625bcef780b78100f70e8bb94154,"The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward."
cc59b4b1eb7d4629f753bc24f029c5cced301381,"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data."
f725fdfc0d771272b3fce058ca7c7525813c9fcc,"Widespread application of artificial intelligence in health care has been anticipated for half a century. For most of that time, the dominant approach to artificial intelligence was inspired by logic: researchers assumed that the essence of intelligence was manipulating symbolic expressions, using rules of inference. This approach produced expert systems and graphical models that attempted to automate the reasoning processes of experts. In the last decade, however, a radically different approach to artificial intelligence, called deep learning, has produced major breakthroughs and is now used on billions of digital devices for complex tasks such as speech recognition, image interpretation, and language translation. The purpose of this Viewpoint is to give health care professionals an intuitive understanding of the technology underlying deep learning. In an accompanying Viewpoint, Naylor1 outlines some of the factors propelling adoption of this technology in medicine and health care."
14898d3fad28202dd4330165bf6ccef4f3b01d45,"
 
 Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010); Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.
 
"
4465cd4b6a931f680bfc458fc07ee471c5785db2,"The problem of finding the multiple locations for new facilities with respect to the multiple existing facilities in a given environment is known as Multifacility Location Problem (MLP).Every location problem is normally bounded by some sort of area constraint. But the fact that much of the work carried out in the literature has almost neglected the area constraint which has motivated us to work on Multifacility Location Problem taking the area constraint into consideration. The mathematical model of the multifacility location problem with area constraint has been developed and the solution has been obtained using Kuhn-Tucker theory. This mathematical analysis and solution procedure is highly complex and time consuming. Hence, an attempt has been made to get the solution of a complex, constrained multifacility location problem using Scaled Conjugate Gradient Algorithm (SCGA) in Artificial Neural Networks (ANN). With the help of Numerical examples, it has been established that the solution obtained through ANN model compares well within the acceptable limits with those obtained through analytical method."
510e26733aaff585d65701b9f1be7ca9d5afc586,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
6ce1922802169f757bbafc6e087cc274a867c763,"We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers."
9d1733ba80347066f1ad9768b2c1ee08780ef998,
bbfa39ebb84d40a5e8152546213510bc597dea4d,"Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data."
c4c06578f4870e4b126e6837907929f3c900b99f,"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule."
2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1,"We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization."
f41872eac65a9ff218ae8a75b97532c4654f9c71,
97fb4e3d45bb098e27e0071448b6152217bd35a5,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques."
c91ae35dbcb6d479580ecd235eabf98374acdb55,"Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ``fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns."
08ee53dac878ee6ab1e5cf06824713bed614e17c,
0c908739fbff75f03469d13d4a1a07de3414ee19,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
0e3a068bc95149b88564ed52312844ac806d8d9b,"This report documents the program and the outcomes of Dagstuhl Seminar 14381 “NeuralSymbolic Learning and Reasoning”, which was held from September 14th to 19th, 2014. This seminar brought together specialist in machine learning, knowledge representation and reasoning, computer vision and image understanding, natural language processing, and cognitive science. The aim of the seminar was to explore the interface among several fields that contribute to the effective integration of cognitive abilities such as learning, reasoning, vision and language understanding in intelligent and cognitive computational systems. The seminar consisted of contributed and invited talks, breakout and joint group discussion sessions. Seminar September 14–19, 2014 – http://www.dagstuhl.de/14381 1998 ACM Subject Classification I.2 Artificial Intelligence, I.2.4 Knowledge Representation Formalisms and Methods, I.2.6 Learning, I.2.10 Vision and Scene Understanding, I.2.11 Distributed Artificial Intelligence"
b843a02895b9cf5710970fe20c86aa1ed6d80c4d,"Subfields of physics are born, expand, and develop in intellectual scope, then can spawn new offspring by subdividing, can disappear by being absorbed in new definitions of the fields of physics, or may merely decline in vigor and membership. Textbooks, seminar programs, graduate courses, and the chosen structure of industrial laboratoriesallcontributedtomakingsolidstatephysicsavibrantsubfield for 30 years, to ultimately disappear into regroupings with names such as condensed matter, materials science, biological physics, complexity, and quantum optics. This review traces the trajectory of the subfieldsolidstatephysicsthroughtheexperiencesoftheauthorinrelationshiptomajoruniversitydepartmentsandBellLabs,withdigressionsintohowhebecameaphysicist,physicseducation,andchoosing research problems."
6011ec0c6c927cee50fbcc9d6f0bda8a96e56bcf,"A main aim of this study was to test the claim that individuals with Williams syndrome have selectively impaired memory for spatial as opposed to visual information. The performance of 16 individuals with Williams syndrome (six males, 10 females; mean age 18y 7mo [SD 7y 6mo], range 9y 1mo‐30y 7mo) on tests of short‐term memory for item and location information was compared with that shown by individuals with moderate learning difficulties (12 males, four females; mean age 10y 3mo [SD 1y], range 8y 6mo‐11y 7mo) and typically developing children (six males, 10 females; mean age 6y 8mo [SD 7mo], range 5y 10mo‐7y 9mo) of an equivalent level of visuospatial ability. A second aim was to determine whether individuals had impaired ability to ‘bind’ visual spatial information when required to recall ‘item in location’ information. In contrast to previous findings, there was no evidence that individuals with Williams syndrome were more impaired in the spatial than the visual memory condition. However, individuals with both Williams syndrome and moderate learning difficulties showed impaired memory for item in location information, suggesting that problems of binding may be generally associated with learning disability. *"
a04a8acfbbd9817d8c723b6ab3b52b6dd34bed58,
d46b81707786d18499f911b4ab72bb10c65406ba,"Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem."
11de5b70918e4a118e8e49a3ae09110bbf59ea83,"It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights."
34f25a8704614163c4095b3ee2fc969b60de4698,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ""thinned"" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
47570e7f63e296f224a0e7f9a0d08b0de3cbaf40,"Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation."
51ab06b7bb4a33473979fa16ff1d398559189380,"Information regarding bone mineral density and fracture characteristics of the equine metacarpus are lacking. The aim of this study was to characterize the relationship between mechanical properties of the equine metacarpal bone and its biomechanical and morphometric properties. Third metacarpal bones were extracted from horses euthanized unrelated to musculoskeletal conditions. In total, bone specimens from 26 front limbs of 13 horses (7.8 ± 5.8 years old) including Lipizzaner (n = 5), Hungarian Warmblood (n = 2), Holsteiner (n = 2), Thoroughbred (n = 1), Hungarian Sporthorse (n = 1), Friesian (n = 1), and Shagya Arabian (n = 1) were collected. The horses included 7 mares, 4 stallions and 2 geldings. Assessment of the bone mineral density of the whole bone across four specific regions of interest was performed using dual-energy X-ray absorptiometry. The bones were scanned using a computer tomographic scanner to measure cross-sectional morphometric properties such as bone mineral density and cross-sectional dimensions including cortical area and cortical width. Mechanical properties (breaking force, bending strength, elastic modulus) were determined by a 3-point bending test. Significant positive linear correlations were found between the breaking force and bone mineral density of the entire third metacarpal bones (P < 0.001, r = 0.72), the medial cortex region of interest (P < 0.001, r = 0.68) and the transverse region of interest (P < 0.001, r = 0.61). The correlation between the breaking force and bone mineral density of the equine third metacarpal bone found in this study warrants in vivo investigations. Third metacarpal bone, densitometry, biomechanical properties, CT, horse Although bone mineral densitometry is an essential diagnostic method to determine increased fracture risk in human patients, application in the veterinary medicine is negligible. In horses,"
76a33bd1593bd3c86238ef4b7e94b8d65c663180,"Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting."
7aafe096a817298de24f7e8ebf4f0b5f1e67d687,
87394e49da317d4d71f3740820b230230a888826,
c1b05fd52dca7ff0c4f45e29ec119d22e31a9ec3,"We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural NetworkHidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax unit for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However, if we average the predictions for each frame from the different contexts it is associated with we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional architectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set (testdev93) and 9.3% on test set (test-eval92)."
1a3c74c7b11ad5635570932577cdde2a3f7a6a5c,"Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2% relative improvement over a DNN trained with sigmoid units, and a 14.4% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code."
32726a7be11bb2ab48bc4dde26e85f77865ac9b3,"We report the performance in four recognition memory experiments of Jon, a young adult with early-onset developmental amnesia whose episodic memory is gravely impaired in tests of recall, but seems relatively preserved in tests of recognition, and who has developed normal levels of performance in tests of intelligence and general knowledge. Jon's recognition performance was enhanced by deeper levels of processing in comparing a more meaningful study task with a less meaningful one, but not by task enactment in comparing performance of an action with reading an action phrase. Both of these variables normally enhance episodic remembering, which Jon claimed to experience. But Jon was unable to support that claim by recollecting what it was that he remembered. Taken altogether, the findings strongly imply that Jon's recognition performance entailed little genuine episodic remembering and that the levels-of-processing effects in Jon reflected semantic, not episodic, memory."
5967e6a25ef7a0deb3bb0e7f929ecf027474579d,
328760482f4d33b6b1422b8312dca37dc4b62b1f,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings."
32ef19e90e7834ec09ef19fcef7cd2aa6eff85a9,"This paper describes a Markov Random Field for real-valued image modeling that has two sets of latent variables. One set is used to gate the interactions between all pairs of pixels, while the second set determines the mean intensities of each pixel. This is a powerful model with a conditional distribution over the input that is Gaussian, with both mean and covariance determined by the configuration of latent variables, which is unlike previous models that were restricted to using Gaussians with either a fixed mean or a diagonal covariance matrix. Thanks to the increased flexibility, this gated MRF can generate more realistic samples after training on an unconstrained distribution of high-resolution natural images. Furthermore, the latent variables of the model can be inferred efficiently and can be used as very effective descriptors in recognition tasks. Both generation and discrimination drastically improve as layers of binary latent variables are added to the model, yielding a hierarchical model called a Deep Belief Network."
3fcb6838c2249d3cb4b1560f48ec3907ce6ff801,"In this paper we show how we can discover non-linear features of frames of spectrograms using a novel autoencoder. The autoencoder uses a neural network encoder that predicts how a set of prototypes called templates need to be transformed to reconstruct the data, and a decoder that is a function that performs this operation of transforming prototypes and reconstructing the input. We demonstrate this method on spectrograms from the TIMIT database. The features are used in a Deep Neural Network Hidden Markov Model (DNN-HMM) hybrid system for automatic speech recognition. On the TIMIT monophone recognition task we were able to achieve gains of 0.5% over Mel log spectra, by augmenting traditional the spectra with the predicted transformation parameters. Further, using the recently discovered ‘dropout’ training, we were able to achieve a phone error rate (PER) of 17.9% on the dev set and 19.5% on the test set, which, to our knowledge is the best reported number on this task using a hybrid system."
4177ec52d1b80ed57f2e72b0f9a42365f1a8598d,"Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
43c0b8309d05102aa75980f6cd53e2e77f222a17,"We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This enables an efficient pretraining algorithm and a state initialization scheme for fast inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks."
54c32d432fb624152da7736543f2685840860a57,"We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks."
64da1980714cfc130632c5b92b9d98c2f6763de6,"Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data."
c517a2f8c85a8c79c5dc855a689796192e345fd9,
6eabf6e67c29778265bc9fef3b58b2756c739c83,"Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. 
We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with semantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incomplete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. 
We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods."
aa7bfd2304201afbb19971ebde87b17e40242e91,"Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. 
 
Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods."
b562a67ff021d0449ff984a1e4f284de937d38d9,"La presente invention a trait a un reseau neuronal classique parallele. Le CNN est mis en œuvre au moyen d'une pluralite de reseaux neuronaux classiques, chacun se situant sur un nœud de traitement respectif. Chaque CNN est dote d'une pluralite de couches. Un sous-ensemble de couches est interconnecte entre les nœuds de traitement de sorte que des activations sont soumises a une action directe d'un bout a l'autre des nœuds. Le sous-ensemble restant n'est pas interconnecte de la sorte."
eb9243a3b98a819539ad57b7b4f05b969510d075,"In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models."
ed8a4531768c0ef297112f956bd14a4a2d6f45f9,"L'invention concerne un systeme d'apprentissage d'un reseau de neurones. Un commutateur est associe a des detecteurs de caracteristique dans au moins certaines des couches du reseau de neurones. Pour chaque cas d'apprentissage, le commutateur desactive de facon selective et aleatoire chacun des detecteurs de caracteristique en fonction d'une probabilite preconfiguree. Les ponderations de chacun des cas d'apprentissage sont ensuite normalisees pour appliquer le reseau de neurones aux donnees de test."
f25a89ac1bc70042d07fcdcca864485e2dd9707e,"We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We propose an approximate inference method that interacts with learning in a way that makes it possible to train the DBM more eciently than previously proposed methods. Even though the model has two hidden layers, it can be trained just as eciently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classication tasks."
732c567a2c323686a043b08bda442e9895e95b58,"The episodic buffer component of working memory is assumed to play a role in the binding of features into chunks. A series of experiments compared memory for arrays of colors or shapes with memory for bound combinations of these features. Demanding concurrent verbal tasks were used to investigate the role of general attentional processes, producing load effects that were no greater on memory for feature combinations than for the features themselves. However, the binding condition was significantly less accurate with sequential rather than simultaneous presentation, especially for items earlier in the sequence. The findings are interpreted as evidence of a relatively automatic but fragile visual feature binding mechanism in working memory. Implications for the concept of an episodic buffer are discussed."
f34607a6aa2d984e34a5ce7f0bdac4a860fa98a4,"Recurrent Neural Networks (RNNs) are powerful sequence models that were believed to be difficult to train, and as a result they were rarely used in machine learning applications. This thesis presents methods that overcome the difficulty of training RNNs, and applications of RNNs to challenging problems. 
We first describe a new probabilistic sequence model that combines Restricted Boltzmann Machines and RNNs. The new model is more powerful than similar models while being less difficult to train. 
Next, we present a new variant of the Hessian-free (HF) optimizer and show that it can train RNNs on tasks that have extreme long-range temporal dependencies, which were previously considered to be impossibly hard. We then apply HF to character-level language modelling and get excellent results. 
We also apply HF to optimal control and obtain RNN control laws that can successfully operate under conditions of delayed feedback and unknown disturbances. 
Finally, we describe a random parameter initialization scheme that allows gradient descent with momentum to train RNNs on problems with long-term dependencies. This directly contradicts widespread beliefs about the inability of first-order methods to do so, and suggests that previous attempts at training RNNs failed partly due to flaws in the random initialization."
06c152df89ca6a1f1b8f8e139ddda82cd4539415,Deep Belief Networks (DBNs) are a very competitive alternative to Gaussian mixture models for relating states of a hidden Markov model to frames of coefficients derived from the acoustic input. They are competitive for three reasons: DBNs can be fine-tuned as neural networks; DBNs have many non-linear hidden layers; and DBNs are generatively pre-trained. This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN.
1366de5bb112746a555e9c0cd00de3ad8628aea8,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
31868290adf1c000c611dfc966b514d5a34e8d23,"Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition."
43d36a22629114e14a0952675e15c9c76f1f024c,"Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition."
47285cf9cd87c9818a39ff5c0870305c52b618c7,"Visiorwork i n A1 has made p rog res s d t h r e l a t i v e l y smal l probierri , We a r e not aware of any system i n hick^ many d i f f e r e n t k inds of knowledge co-operate, Often t h e r e i s e s s e n t i d _ l y one k ind of s t r u c t u r e , e ,g , a n e t m r k of l i n e s o r regions, and t h e problem i s simply t o segment i t , and/or t o l a b e l p a r t s of i t , Sometimes models of known o b j e c t s a r e used t o gulde t h e a n a l y s l s and i n t e r p r e t a t i o n of an image, as i n t h e work of Roberts (190j), but u s u a l l y t h e r e a r e few such m d e l s , and t h e r e i s n ' t a v e r y deep h i e r a r z h y of o b j e c t s c o q o s e d o f o b j e c t s composed of o b j e c t s ... By con t r a s t , r ecen t speech ~understairding systems, l i k e SAY (Lesser 1977, Hayes-Roth 19779, d e a l wi th m r e complex k inds of i n t e r a c t i o n s between d i f f e r e n t s o r t s of knowledge, They a r e s t i l l n o t v e r y impressive compared with people, bu t t h e r e a r e some s o l i d achievements, Is t h e Lack of s i m i l a r success i n v l s i o n due t o i n h e r e n t l y m r e d i f f i c u l t problems?"
b6998b9d7071fc6e82d94ecb5decd96557b0f435,"Seven- and eight-year-old skilled and less-skilled comprehenders were compared on a sentence recognition task in two conditions varying in memory load and retention interval. Integration of story information during comprehension was indexed by inflated recognition errors of foils that had been constructed by integrating information across original story sentences. Skilled comprehenders exhibited more accurate memory for sentences than less-skilled comprehenders. However, the groups did not differ in the degree to which they integrated information with minimal memory demand, or in their tendency to integrate information and retain the integrated representations with increased memory demand. These results were interpreted as evidence that integration deficits do not lie at the root of reading comprehension difficulties in mainstream children."
53f0982422af3901346159d6ab11523c248f08e0,"While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, the Robust Boltzmann Machine (RoBM), which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition, the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms, the RoBM is significantly better at recognition and denoising on several face databases."
56258bf991577bd0bc9cbdc524864b5ff4ca342d,
57236bcb830af797396fefb0ac26fea9f0caeeba,"Current speech recognition systems, for example, typically use Gaussian mixture models (GMMs), to estimate the observation (or emission) probabilities of hidden Markov models (HMMs), and GMMs are generative models that have only one layer of latent variables. Instead of developing more powerful models, most of the research effort has gone into finding better ways of estimating the GMM parameters so that error rates are decreased or the margin between different classes is increased. The same observation holds for natural language processing (NLP) in which maximum entropy (MaxEnt) models and conditional random fields (CRFs) have been popular for the last decade. Both of these approaches use shallow models whose success largely depends on the use of carefully handcrafted features."
9e2fd6034db8d0c733cd17f8e76372b86b0d35bf,"We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models."
abd1c342495432171beb7ca8fd9551ef13cbd0ff,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
cdfdaccd946dc2fe4863aed048b12b5d2282f285,"When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider."
e95d3934e51107da7610acd0b1bcb6551671f9f1,
f50dfcc143bc1cc8ded1d88d31a59140b0a0ebd8,"An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets."
00e95b65a41cc5d07b601276d76ef47b8c7f8efe,
05c6b2a59b021f2d5e5a580ded1681f8a1ae2a50,"We describe a deep generative model in which the lowest layer represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top-down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF-IDF we achieve higher accuracy than TF-IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document."
1e56612557364dc130f09fce00d7de0b9290491c,"Most speech recognition systems still use Mel Frequency Cepstral Coefficients (MFCC’s) or Perceptual Linear Prediction Coefficients because these preserve a lot of the information required for recognition while being much more compact than a high-resolution spectrogram. As computers get faster and methods of modeling high-dimensional data improve, however, high-resolution spectrograms or other very high-dimensional representations of the sound wave become more attractive. They have already surpassed MFCC’s for some tasks [1]. Psychologists have argued that high-quality recognition would be facilitated by finding acoustic events or landmarks that have well-defined onset times, amplitudes and rates in addition to being present or absent. We introduce a new way of learning such acoustic events by using a new type of autoencoder that is given both a spectrogram and a desired global transformation and learns to output the transformed spectrogram. By specifying the global transformation in the appropriate way, we can force the autoencoder to extract accoustic events that, in addition to a probability of being present, have explicit onset times, amplitudes and rates. This makes it much easier to compute relationships between acoustic events."
20f0357688876fa4662f806f985779dce6e24f3c,
46af78834358337447001241cd2e18828ed926f0,"State of the art speech recognition systems rely on preprocessed speech features such as Mel cepstrum or linear predictive coding coefficients that collapse high dimensional speech sound waves into low dimensional encodings. While these have been successfully applied in speech recognition systems, such low dimensional encodings may lose some relevant information and express other information in a way that makes it difficult to use for discrimination. Higher dimensional encodings could both improve performance in recognition tasks, and also be applied to speech synthesis by better modeling the statistical structure of the sound waves. In this paper we present a novel approach for modeling speech sound waves using a Restricted Boltzmann machine (RBM) with a novel type of hidden variable and we report initial results demonstrating phoneme recognition performance better than the current state-of-the-art for methods based on Mel cepstrum coefficients."
4ca91c58eb35395e4a5fb5ffaee925e7c4f1ae81,"This paper considers application of Deep Belief Nets (DBNs) to natural language call routing. DBNs have been successfully applied to a number of tasks, including image, audio and speech classification, thanks to the recent discovery of an efficient learning technique. DBNs learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms; Support Vector machines (SVM), Boosting and Maximum Entropy (MaxEnt). The DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models even though it currently uses an impoverished representation of the input."
c131291246e8f548172827af1303d3fdca1f3d57,"In the late 1970s/early 1980s, Baddeley and colleagues conducted a series of experiments investigating the role of eye movements in visual working memory. Although only described briefly in a book (Baddeley, 1986), these studies have influenced a remarkable number of empirical and theoretical developments in fields ranging from experimental psychology to human neuropsychology to nonhuman primate electrophysiology. This paper presents, in full detail, three critical studies from this series, together with a recently performed study that includes a level of eye movement measurement and control that was not available for the older studies. Together, the results demonstrate several facts about the sensitivity of visuospatial working memory to eye movements. First, it is eye movement control, not movement per se, that produces the disruptive effects. Second, these effects are limited to working memory for locations and do not generalize to visual working memory for shapes. Third, they can be isolated to the storage/maintenance components of working memory (e.g., to the delay period of the delayed-recognition task). These facts have important implications for models of visual working memory."
f45c7520acdc549992ae90dc8f8e08054f26d3c5,
f9bb0d6fb85973f333228cbcb896467edf495dd1,
6065f5326263b0de28028012e6f797b801fdb18c,"and the Computers and Thought Award are awarded by the IJCAI Board of Trustees, upon recommendation by the IJCAI-16 Awards Selection Committee, which consists this year of Hector Levesque, University of Toronto (Canada); Joelle Pineau, McGill University (Canada); Peter Stone, University of Texas at Austin (USA); Sebastian Thrun, Udacity and Stanford University (Chair) (USA); and Qiang Yang, Hong Kong University of Science and Technology (Hong Kong, China) The IJCAI Awards Selection Committee receives advice from members of the IJCAI-16 Awards Review Committee, who comment on the accuracy of the nomination material and provide additional information about the nominees. The IJCAI-16 Awards Review Committee is the union of the former Trustees of IJCAI, the IJCAI-16 Advisory Committee, the Program Chairs of the last three IJCAI conferences, and the past recipients of the IJCAI Award for Research Excellence and the IJCAI Distinguished Service Award, with nominees excluded."
6c036729284a73075d4f9b2ce7b87cd05fe2bbba,"We describe a generative model of the relationship between two images. The model is defined as a factored three-way Boltzmann machine, in which hidden variables collaborate to define the joint correlation matrix for image pairs. Modeling the joint distribution over pairs makes it possible to efficiently match images that are the same according to a learned measure of similarity. We apply the model to several face matching tasks, and show that it learns to represent the input images using task-specific basis functions. Matching performance is superior to previous similar generative models, including recent conditional models of transformations. We also show that the model can be used as a plug-in matching score to perform invariant classification."
78c6ada93d06ee849b9efc23a3d99d3013b87600,
79ef1a3843a2dc01bde67c3a9a17c6deb352e285,"One way to design an object recognition system is to define objects recursively in terms of their parts and the required spatial relationships between the parts and the whole. A natural way for a neural network to implement this knowledge is by using a matrix of weights to represent each part-whole relationship and a vector of neural activities to represent the pose of each part or whole relative to the viewer [10]. This leads to neural networks that can recognize objects over a wide range of viewpoints using neural activities that are “equivariant” rather than invariant: as the viewpoint varies the neural activities all vary even though the knowledge in the weights is viewpoint-invariant. The “capsules” that implement the lowest-level parts in the shape hierarchy need to extract explicit pose parameters from pixel intensities. This paper shows that these capsules are quite easy to learn from pairs of transformed images if the neural net has direct, non-visual access to the transformations."
88080d28536f36588740737f3b7a1f6c1a409654,"We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple di erent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes."
92e0f569d8fb17559d580d1c3b16a70e682b48b9,"In this paper we develop a class of nonlinear generative models for high-dimensional time series. We first propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued ""visible"" variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This ""conditional"" RBM (CRBM) makes on-line inference efficient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line filling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them. Videos and source code can be found at http://www.cs.nyu.edu/~gwtaylor/publications/jmlr2011."
94b0e8e97c19ad0977d26e3e355d3ae09ad49365,"Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems."
0fa924dca8fa546759c91eff8380b9e23ee75e3b,
e9201ebd3e5e0601fd6a10147e6cec68cea5a2e2,"Abstract This paper reports the findings of a study that investigated the relationship between phonological short-term memory (PSTM), working memory capacity (WMC), and the level of mastery of L2 grammar. Grammatical mastery was operationalized as the ability to produce and comprehend English passive voice with reference to explicit and implicit (or highly automatized) knowledge. Correlational analysis showed that PSTM was related to implicit productive knowledge while WMC was linked to explicit productive knowledge. However, regression analysis showed that those relationships were weak and mediated by overall mastery of target language grammar, operationalized as final grades in a grammar course."
234f413118e586b17eea5fdffd61c319c0b6a1db,"The concept of locus of control shows the relationship between events and people’s reactions to them, depending on whether they attribute their outcomes to internal or external factors. Accordingly, people can be divided into two main groups. Those who feel personally responsible for what happens to them are labelled internals, whilst those who believe that external forces such as fate, luck or objective difficulties determine their life are termed externals. This paper presents the results of a study conducted on 41 English philology undergraduate participants with a view to investigating their locus of control, that is, whether they perceive their academic outcomes as the result of their skills and abilities or rather fate or luck, in other words, if they feel personally responsible for their academic successes and failures."
b0e80465aa61ad1448027e01200f4dcf5466342d,"Problem One One of the critical properties of a valid Markov chain Monte Carlo transition operator is that it leave the target distribution invariant. That is, if the target distribution is fX(x), then the operator T (x ′ ← x) must satisfy fX(x ′) = ∫ fX(x)T (x ′ ← x) dx. A stronger property that is a sufficient condition for T (x′ ← x) to leave fX(x) invariant is detailed balance: fX(x ′)T (x← x′) = fX(x)T (x′ ← x). Prove that the Metropolis–Hastings algorithm satisfies detailed balance. (5 points)"
be53d4def5e0601f2416e9345babc7ef1b30a664,"Deep Belief Networks (DBNs) are multi-layer generative models. They can be trained to model windows of coefficients extracted from speech and they discover multiple layers of features that capture the higher-order statistical structure of the data. These features can be used to initialize the hidden units of a feed-forward neural network that is then trained to predict the HMM state for the central frame of the window. Initializing with features that are good at generating speech makes the neural network perform much better than initializing with random weights. DBNs have already been used successfully for phone recognition with input coefficients that are MFCCs or filterbank outputs [1, 2]. In this paper, we demonstrate that they work even better when their inputs are speaker adaptive, discriminative features. On the standard TIMIT corpus, they give phone error rates of 19.6% using monophone HMMs and a bigram language model and 19.4% using monophone HMMs and a trigram language model."
d8320ddb8de4fec39f115d3673b65bcec9d1a3b8,"A TYPICAL MACHINE learning program uses weighted combinations of features to discriminate between classes or to predict real-valued outcomes. The art of machine learning is in constructing the features, and a radically new method of creating features constitutes a major advance. In the 1980s, the new method was backpropagation, which uses the chain rule to backpropagate error derivatives through a multilayer, feed-forward, neural network and adjusts the weights between layers by following the gradient of the backpropagated error. This worked well for recognizing simple shapes, such as handwritten digits, especially in convolutional neural networks that use local feature detectors replicated across the image. 5 For many tasks, however, it proved extremely difficult to optimize deep neural nets with many layers of non-linear features, and a huge number of labeled training cases was required for large neural networks to generalize well to test data. In the 1990s, Support Vector Machines (SVMs) 8 introduced a very different way of creating features: the user defines a kernel function that computes the similarity between two input vectors, then a judiciously chosen subset of the training examples is used to create "" landmark "" features that measure how similar a test case is to each training case. SVMs have a clever way of choosing which training cases to use as landmarks and deciding how to weight them. They work remarkably well on many machine learning tasks even though the selected features are non-adaptive. The success of SVMs dampened the earlier enthusiasm for neural networks. More recently, however, it has been shown that multiple layers of feature detectors can be learned greedily, one layer at a time, by using unsupervised learning that does not require labeled data. The features in each layer are designed to model the statistical structure of the patterns of feature activations in the previous layer. After learning several layers of features this way without paying any attention to the final goal, many of the high-level features will be irrelevant for any particular task, but others will be highly relevant because high-order correlations are the signature of the data's true underlying causes and the labels are more directly related to these causes than to the raw inputs. A subsequent stage of fine-tuning using backpropagation then yields neural networks that work much better than those trained by backpropagation alone and better than SVMs for important tasks such as object or speech recognition. The neural …"
e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de,"Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or ""gated"") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling – a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date."
f93844c68d96f2f01da973b2ed3c236c8a369e57,"The most popular way to use probabilistic models in vision is first to extract some descriptors of small image patches or object parts using well-engineered features, and then to use statistical learning tools to model the dependencies among these features and eventual labels. Learning probabilistic models directly on the raw pixel values has proved to be much more difficult and is typically only used for regularizing discriminative methods. In this work, we use one of the best, pixel-level, generative models of natural images–a gated MRF–as the lowest level of a deep belief network (DBN) that has several hidden layers. We show that the resulting DBN is very good at coping with occlusion when predicting expression categories from face images, and it can produce features that perform comparably to SIFT descriptors for discriminating different types of scene. The generative ability of the model also makes it easy to see what information is captured and what is lost at each level of representation."
0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3,"We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the ""glimpse"" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images."
0eb2e4a205a628ab059cab41d3b772f614ad29f2,"To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images, Memisevic and Hinton (2007) introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned, symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters, which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors, each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter, the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We also show how learning about image transformations allows the model to perform a simple visual analogy task, and we show how a completely unsupervised network trained on transformations perceives multiple motions of transparent dot patterns in the same way as humans."
1603a40b7bb56d563d9401f0d24c67d428e509f2,"For decades, Hidden Markov Models (HMMs) have been the state-of-the-art technique for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. Conditional Restricted Boltzmann Machines (CRBMs) have recently proved to be very effective for modeling motion capture sequences and this paper investigates the application of this more powerful type of generative model to acoustic modeling. On the standard TIMIT corpus, one type of CRBM outperforms HMMs and is comparable with the best other methods, achieving a phone error rate (PER) of 26.7% on the TIMIT core test set."
1ca4f8711e0c5ac77a7dfd8e5916d9d4e4268719,"We describe a ""log-bilinear"" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets."
20b97fd491a05b289dfd666a87c545664b25bb67,"We introduce a new class of probabilistic latent variable model called the Implicit Mixture of Conditional Restricted Boltzmann Machines (imCRBM) for use in human pose tracking. Key properties of the imCRBM are as follows: (1) learning is linear in the number of training exemplars so it can be learned from large datasets; (2) it learns coherent models of multiple activities; (3) it automatically discovers atomic “movemes” and (4) it can infer transitions between activities, even when such transitions are not present in the training set. We describe the model and how it is learned and we demonstrate its use in the context of Bayesian filtering for multi-view and monocular pose tracking. The model handles difficult scenarios including multiple activities and transitions among activities. We report state-of-the-art results on the HumanEva dataset."
297e9ddd0121569bbaa79fa236845e4073ac7ffe,
321e03f67a8127960d337f9f2e9360230b2fe2ba,
4f30bf42d23f34a2243f89b51fa0d2fea432bb6c,
5a9d4b904e114788a099c3be6f192df8a8e0ebd1,
7c7169d674551a81839c8d7fec17b2d14e7e9819,
86bb88d0d6edee6bb4662ba48bb9bbde34896727,"Memory for past events can differ, sometimes in striking ways, from how they were initially experienced (Bartlett, 1932; Kopelman, 1999; Loftus et al., 1978; Neisser, 1967; Roediger, 1996; Schacter, 1995, 2001). Even exceptionally vivid and emotional memories, such as “ﬂashbulb memories”, are not immune from memory distortion. Schmolck, Buffalo, and Squire (2000) examined individuals’ memory for the events surrounding their learning the verdict of the O.J. Simpson trial, such as where they were, who they were with, what they felt, etc. They tested their participants’ memory 3 days after the announcement of the verdict and then either 15 months or 32 months later. Surprisingly, nearly 43% of the individuals tested at the 32 month interval reported memories that contained major distortions, as compared to the account they gave just 3 days after learning the verdict. For instance, one individual reported at the 3 day interval that he learned about the verdict while in a lounge at college. When tested 32 months later, this individual remembered being at home with his sister and father when he learned about the verdict. Ironically, many individuals expressed conﬁdence in the accuracy of their recollection, despite its actual inaccuracy (for similar results, see Bohannon & Symons, 1992; Neisser & Harsch, 1992). How can a remembered episode differ so dramatically from how that episode was initially experienced? As Roediger & McDermott (2000, p. 149) ask in their review of memory distortion, “Where would the recollection come from, if not from stored traces of actual events?” This chapter will review cognitive and neuropsychological research that illustrates mechanisms that promote or minimize the occurrence of distorted memories. First, we will outline a general framework for understanding the processes that contribute to constructive memory phenomena. We then will consider how false memories are inﬂuenced by factors operating primarily at the encoding or retrieval stages of memory."
b3e1b61d55405f1d6f77339ffba9d2bd71c3ec88,"The term 'episodic memory' refers to our memory for unique, personal experiences, that we can date at some point in our past - our first day at school, the day we got married. It has again become a topic of great importance and interest to psychologists, neuroscientists, and philosophers. How are such memories stored in the brain, why do certain memories disappear (especially those from early in childhood), what causes false memories (memories of events we erroneously believe have really taken place)? Since Endel Tulving's classic book 'Episodic memory' (OUP, 1983) very few books have been published on this topic. In recent years however, many of the assumptions made about episodic memory have had to be reconsidered as a result of new techniques, which have allowed us a far deeper understanding of episodic memory. 
 
In 'Episodic memory: new directions in research' three of the worlds leading researchers in the topic of memory have brought together a stellar team of contributors from the fields of cognitive psychology, neuropsychology, and neuroscience, to present an account of what we now know about about this fundamentally important topic. The list of contributors includes, amongst others, Daniel Schacter, Richard Morris, Fareneh Vargha-Khadem, and Endel Tulving. The work presented within this book will have a profound effect on the direction that future research in this topic will take."
756db84f76d745211464b5686a67bfdc23e18c19,"Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A better way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks."
773f0addad7d7c3e4733d57a0b7d9d4b729ef1c4,"We compare 10 methods of classifying fMRI volumes by applying them to data from a longitudinal study of stroke recovery: adaptive Fisher's linear and quadratic discriminant; gaussian naive Bayes; support vector machines with linear, quadratic, and radial basis function (RBF) kernels; logistic regression; two novel methods based on pairs of restricted Boltzmann machines (RBM); and K-nearest neighbors. All methods were tested on three binary classification tasks, and their out-of-sample classification accuracies are compared. The relative performance of the methods varies considerably across subjects and classification tasks. The best overall performers were adaptive quadratic discriminant, support vector machines with RBF kernels, and generatively trained pairs of RBMs."
779e5beb515ed26c47dbfc08304fe49233063c1b,"Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks."
79d8007123429ec2b36b793c211787f862ecd81c,
7a4f376b133eedeb9e0a3b5c911bcd46054bc13b,
57444678be8a7d2b57f96da0b55aa8e48081888a,
bb90430bfb7edd282f44b0bc6ef5c0cd302ff8d1,The current study explored three possible explanations of poor verbal short-term memory performance among individuals with Down syndrome in an attempt to determine whether the condition is associated with a fundamental verbal short-term memory deficit. The short-term memory performance of a group of 19 children and young adults with Down syndrome was contrasted with that of two control groups matched for level of receptive vocabulary. The specificity of a deficit was assessed by comparing memory for verbal and visuo-spatial information. The effect of auditory problems on performance was examined by contrasting memory for auditorily presented material with that for material presented both auditorily and visually. The influence of speech-motor difficulties was investigated by employing both a traditional recall procedure and a serial recognition procedure that reduced spoken response demands. Results confirmed that individuals with Down syndrome do show impaired verbal short-term memory performance for their level of receptive vocabulary. The findings also indicated that this deficit is specific to memory for verbal information and is not primarily caused by auditory or speech-production difficulties.
8b16106e35ff9ff97ffb73eb9c84b73ce1264c67,
90b63e917d5737b06357d50aa729619e933d9614,"Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date."
97aa287b136907463c12523956a3ccb17ee6d1ab,
a538b05ebb01a40323997629e171c91aa28b8e2f,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ""Stepped Sigmoid Units"" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors."
bc74c99a772b9c4908418a1ab25921f21f58ff38,
cb2e747d3bff80d84ad28b2408fc060aa36bb6b5,
d6b441a5af472b0f0e494a2977589ffb558dced8,"Although it is generally accepted that the word length effect in short-term memory operates through output delay or interference, there is less agreement on whether it also influences performance through its impact on rehearsal. We investigated this issue by studying the effect of word length on recall and on a recognition task in which output delay was controlled. Word sequences were repeated exactly, or with one pair of words reversed. Two experiments using auditory presentation showed clear word length effects for both recall and serial recognition, although the magnitude of the effect tended to be less for recognition. A third experiment using visual presentation studied the effect of articulatory suppression during the recognition test; again we found a clear word length effect. It is concluded that the word length effect can influence retention through both rehearsal and output factors, as proposed by the phonological loop hypothesis."
f897751da2ccffe1d090cbae44061b3c1e8280a7,"The current state of A. D. Baddeley and G. J. Hitch's (1974) multicomponent working memory model is reviewed. The phonological and visuospatial subsystems have been extensively investigated, leading both to challenges over interpretation of individual phenomena and to more detailed attempts to model the processes underlying the subsystems. Analysis of the controlling central executive has proved more challenging, leading to a proposed clarification in which the executive is assumed to be a limited capacity attentional system, aided by a newly postulated fourth system, the episodic buffer. Current interest focuses most strongly on the link between working memory and long-term memory and on the processes allowing the integration of information from the component subsystems. The model has proved valuable in accounting for data from a wide range of participant groups under a rich array of task conditions. Working memory does still appear to be working."
da5d2b6344fce4314e5f05356d06cdf1aafeef1a,
e2c04849a3802715d5a9d89179c9f161014d6c2a,"Learning a generative model of natural images is a useful way of extracting features that capture interesting regularities. Previous work on learning such models has focused on methods in which the latent features are used to determine the mean and variance of each pixel independently, or on methods in which the hidden units determine the covariance matrix of a zero-mean Gaussian distribution. In this work, we propose a probabilistic model that combines these two approaches into a single framework. We represent each image using one set of binary latent features that model the image-specific covariance and a separate set that model the mean. We show that this approach provides a probabilistic framework for the widely used simple-cell complex-cell architecture, it produces very realistic samples of natural images and it extracts features that yield state-of-the-art recognition accuracy on the challenging CIFAR 10 dataset."
e3c1bf806c325f306e5084c3bd332b83d2077e2a,"This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pretrained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-bylayer pre-training we “unroll” the generative model to form a deep auto-encoder, whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram, individual spectrogram segments predicted by their respective binary codes are combined using an overlapand-add method. Experimental results on speech spectrogram coding demonstrate that the binary codes produce a logspectral distortion that is approximately 2 dB lower than a subband vector quantization technique over the entire frequency range of wide-band speech. Index Terms: deep learning, speech feature extraction, neural networks, auto-encoder, binary codes, Boltzmann machine"
e7c64258997838087c9ba4e87225627b015122b2,"Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model."
fe862c49248d8898b4712ba58e6bec13ea586732,"One of the central problems in computational neuroscience is to understand how the object-recognition pathway of the cortex learns a deep hierarchy of nonlinear feature detectors. Recent progress in machine learning shows that it is possible to learn deep hierarchies without requiring any labelled data. The feature detectors are learned one layer at a time and the goal of the learning procedure is to form a good generative model of images, not to predict the class of each image. The learning procedure only requires the pairwise correlations between the activations of neuron-like processing units in adjacent layers. The original version of the learning procedure is derived from a quadratic ‘energy’ function but it can be extended to allow third-order, multiplicative interactions in which neurons gate the pairwise interactions between other neurons. A technique for factoring the third-order interactions leads to a learning module that again has a simple learning rule based on pairwise correlations. This module looks remarkably like modules that have been proposed by both biologists trying to explain the responses of neurons and engineers trying to create systems that can recognize objects."
03057ea57d9f2d9bbc8a141d51f76d5bbc715234,"The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent ""fantasy particles"" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of ""fast weights"" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model."
050f5cba0979d045a300d633ab28b77756ebde1b,
0f6911bc1e6abee8bbf9dd3f8d54d40466429da7,"We consider the problem of zero-shot learning, where the goal is to learn a classifier f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes. As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words."
346fbcffe4237aa60e8bcb3d4294a8b99436f1d0,"The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N3) to O(N2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them."
3470bf95fd9e77e8aa45002d5b60c6baeaff0ed3,"In an effort to better understand the complex courtship be- haviour of pigeons, we have built a model learned from motion capture data. We employ a Conditional Restricted Boltzmann Machine (CRBM) with binary latent features and real-valued visible units. The units are conditioned on information from previous time steps to capture dynam- ics. We validate a trained model by quantifying the characteristic ""head- bobbing"" present in pigeons. We also show how to predict missing data by marginalizing out the hidden variables and minimizing free energy."
5a2668bf420d8509a4dfa28e1cdcdac14c649975,"We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error."
79dc4da8f131ff26046d5564e5dedb1c5ce72c6a,"The important aspect of this layer-wise training procedure is that, provided the number of features per layer does not decrease, [6] showed that each extra layer increases a variational lower bound on the log probability of data. So layer-by-layer training can be repeated several times1 to learn a deep, hierarchical model in which each layer of features captures strong high-order correlations between the activities of features in the layer below. We will discuss three ideas based on greedily learning a hierarchy of features:"
0145525edb3943e7743a5eabb149ee4d004e6160,"We report the performance on recognition memory tests of Jon, who, despite amnesia from early childhood, has developed normal levels of performance on tests of intelligence, language, and general knowledge. Despite impaired recall, he performed within the normal range on each of six recognition tests, but he appears to lack the recollective phenomenological experience normally associated with episodic memory. His recall of previously unfamiliar newsreel event was impaired, but gained substantially from repetition over a 2-day period. Our results are consistent with the hypothesis that the recollective process of episodic memory is not necessary either for recognition or for the acquisition of semantic knowledge."
30b8d22eeb660ce7a56bfcc9172e9fe2bd18ada5,"Abstract Motivation plays a critical role in L2 language learning and has proven to be a strong predictor of success in learning a foreign language (Biedroń & Pawlak, 2016). The Second Language Motivational Self System (L2MSS) is one of the most prominent theories developed by Dörnyei (2009), which has been studied in relation to different variables affecting language learning motivation. The aim of the present study is to examine the relationship between L2MSS components, international posture, and socioeconomic status among university students. The participants of this study were 134 non-English major university students. The results suggest that the ideal L2 self, and the L2 learning experience are related to international posture insert a comma after posture whereas the L2 learning experience is a stronger predictor of students’ motivated behavior. Future research should investigate the development of future selves in instructed language learning contexts conducive to enhancing and increasing motivation to learn English."
14827d7afb9ef31a3ff807f4ee137a431596761d,The pu η ） oses of this paper are a ） to investigate whether the Lunic Language Marathon items difficulty c investigate whether the results of the LLM show the selection effectS of college entrance examinations The this study show a ） and as ； and c ） some the of a diagnostic test at a college ， and points out that 丘 cally the scores on Part l will tell the instructors muGh about the leamers ’ variety
7c488cbc4103524b27f42254e9455429b23d92ca,
85021c84383d18a7a4434d76dc8135fc6bdc0aa6,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks."
94f2264140de0f778d57f0fd7bd4556ab585faf3,"Building intelligent systems that are capable of extractin g high-level representations from high-dimensional sensory data lies at the core of solving many AI related tasks , including object recognition, speech perception, and language understanding. Theoretical and biological ar guments strongly suggest that building such systems requires deep architectures that involve many layers of non li ear processing. Hinton et.al. [2] introduced a fast, greedy learning algorithm for Deep Belief Networks that wou ld learn one layer of features at a time. This new learning algorithm has generated substantial interest in a cademia and many variants of it have been successfully applied in many application domains. However, a crucial dis a vantage of these deep probabilistic models is that the approximate inference is very limited, because it i s performed in a single bottom-up pass, and will fail to adequately account for uncertainty when interpreting am biguous sensory inputs."
972cbb8c9bf4a1d7a54de45420cf51a2dd55303b,"In an effort to better understand the complex courtship behaviour of pigeons, we have built a model learned from motion capture data. We employ a Conditional Restricted Boltzmann Machine with binary latent features and real-valued visible units. The units are conditioned on information from previous time steps in a sequence to learn long-term effects and infer current features. We validate a trained model by quantifying the characteristic “head-bobbing” present in generated pigeon motion. We also introduce a method of predicting missing data by marginalizing out the hidden variables and minimizing the free energy of the model. An alternative prediction method using forward and reverse passes over gaps of missing markers was presented as well. Lastly, the effects of head and foot motion on prediction results were analyzed. Website: http://www.matthewzeiler.com/videos/"
aa7d1cd5a750f4cfcf15f642bc788d4c8411795c,"Products of Hidden Markov Models (PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This may be in part due to their more computationally expensive gradient-based learning algorithm, and the intractability of computing the log likelihood of sequences under the model. In this paper, we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks."
08cc2e06791b9deb9a132242c556aa748082dc9e,"We present nine experiments, in three study phases, which test the hypothesis that learning methods which prevent the making of errors (“errorless learning”) will lead to greater learning than “trial-and-error” learning methods amongst individuals who are memory impaired as a result of acquired brain injury. Results suggest that tasks and situations which facilitate retrieval of implicit memory for the learned material (such as learning names with a first letter cue) will benefit from errorless learning methods, whilst those that require the explicit recall of novel associations (such as learning routes or programming an electronic organiser) will not benefit from errorless learning. The more severely amnesic patients benefit to a greater extent from errorless learning methods than those who are less severely memory impaired, but this may only apply when the interval between learning and recall is relatively short."
b32de117302258dd29919435cd001a8bcdfee3b3,"We introduce a two-layer undirected graphical model, called a ""Replicated Softmax"", that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy."
cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a,
d15cdfe4bb19c7da564d4f9a8c8bae41e4f146f7,"We evaluate the ability of the popular Field-of-Experts (FoE) to model structure in images. As a test case we focus on modeling synthetic and natural textures. We find that even for modeling single textures, the FoE provides insufficient flexibility to learn good generative models ‐ it does not perform any better than the much simpler Gaussian FoE. We propose an extended version of the FoE (allowing for bimodal potentials) and demonstrate that this novel formulation, when trained with a better approximation of the likelihood gradient, gives rise to a more powerful generative model of specific visual structure that produces significantly better results for the texture task."
ec39869789e3e9df23c412642139a9eaff6f0c53,"Models that represent words as points in a semantic space are subject to fundamental limitations of metric spaces. These limitations prevent semantic space models from faithfully representing, for example, the pairwise similarities between word meanings as revealed by word association data. In particular, semantic space models cannot faithfully represent intransitive pairwise similarities or the similarities of words that have multiple meanings. In this paper, we present a model that alleviates the limitations of semantic space models by constructing a collection of maps that represent complementary structure in the similarity data. Our model is a variant of a similarity choice model known as Stochastic Neighbor Embedding that constructs multiple maps and allows each object to occur as a point in several different maps. We apply the model to a set of word association data, demonstrating that it can successfully represent intransitive semantic relations as well as words with multiple meanings, and that it outperforms traditional semantic space models in the prediction of word associations. We compare the model to alternative representations of semantic structure, such as topic models and semantic networks. Modeling Semantic Similarities in Multiple Maps Laurens van der Maaten ICT Group, Delft University of Technology Geoffrey Hinton Department of Computer Science, University of Toronto"
504d9160e81f6b4cab252402f7db49cb2695003e,
0b8e0f2d73662b0570309798db06b103ded1ae2a,
f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5,"Hidden Markov Models (HMMs) have been the state-of-the-art techniques for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the research community for deeper models that are capable of modeling the many types of variability present in the speech generation p r cess. Deep Belief Networks (DBNs) have recently proved to be very effective fo r a variety of machine learning problems and this paper applies DBNs to acous ti modeling. On the standard TIMIT corpus, DBNs consistently outperform ot her techniques and the best DBN achieves a phone error rate (PER) of 23.0% on the T IMIT core test set."
0228810a988f6b8f06337e14f564e2fd3f6e1056,"The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls."
0b718a3f9dae8abc741411aed5fe5d423079200f,"We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden discrete variable that represents the cluster label. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data."
1c46943103bd7b7a2c7be86859995a4144d1938b,"We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets."
20fc405a5cd0f7994a79e8791481499949d11a75,"Realistic facial expression animation requires a powerful “animator” (or graphics program) that can represent the kinds of variations in facial appearance that are both possible and likely to occur in a given context. If the goal is fully determined as in character animation for film, knowledge can be provided in the form of human higher-level descriptions. However, for generating facial expressions for interactive interfaces, such as animated avatars, correct expressions for a given context must be generated on the fly. A simple solution is to rely on a set of prototypical expressions or basis shapes that are linearly combined to create every facial expression in an animated sequence (Kleiser, 1989; Parke, 1972). An innovative algorithm for fitting basis shapes to images was proposed by Blanz and Vetter (1999). The main problem with the basis shape approach is that the full range of appearance variation required for convincing expressive behavior is far beyond the capacity of what a small set of basis shapes can accommodate. Moreover, even if many expression components are used to create a repertoire of basis shapes (Joshi et al., 2007; Lewis et al., 2000), the interface may need to render different identities or mixtures of facial expressions not captured by the learned basis shapes. A representation of facial appearance for animation must be powerful enough to capture the right constraints for realistic expression generation yet flexible enough to accommodate different identities and behaviors. Besides the obvious utility of such a representation to animated facial interfaces, a good model of facial expression generation would be useful for computer vision tasks because the model’s representation would likely be much richer and more informative than the original pixel data. For example, inferring the model’s representation corresponding to a given image might even allow transferring an expression extracted from an image of a face onto a different character as illustrated by the method of expression cloning (Noh & Neumann, 2001). In this chapter we introduce a novel approach to learning to generate facial expressions that uses a deep belief net (Hinton et al., 2006). The model can easily accommodate different constraints on generation. We demonstrate this by restricting it to generate expressions with a given identity and with elementary facial expressions such as “raised eyebrows.” The deep"
111979a66d80aeadb416cfa9b04e351effd5a409,"This article is a transcription of an electronic symposium in which some active researchers were invited by the Brazilian Society for Neuroscience and Behavior (SBNeC) to discuss the last decade's advances in neurobiology of learning and memory. The way different parts of the brain are recruited during the storage of different kinds of memory (e.g., short-term vs long-term memory, declarative vs procedural memory) and even the property of these divisions were discussed. It was pointed out that the brain does not really store memories, but stores traces of information that are later used to create memories, not always expressing a completely veridical picture of the past experienced reality. To perform this process different parts of the brain act as important nodes of the neural network that encode, store and retrieve the information that will be used to create memories. Some of the brain regions are recognizably active during the activation of short-term working memory (e.g., prefrontal cortex), or the storage of information retrieved as long-term explicit memories (e.g., hippocampus and related cortical areas) or the modulation of the storage of memories related to emotional events (e.g., amygdala). This does not mean that there is a separate neural structure completely supporting the storage of each kind of memory but means that these memories critically depend on the functioning of these neural structures. The current view is that there is no sense in talking about hippocampus-based or amygdala-based memory since this implies that there is a one-to-one correspondence. The present question to be solved is how systems interact in memory. The pertinence of attributing a critical role to cellular processes like synaptic tagging and protein kinase A activation to explain the memory storage processes at the cellular level was also discussed."
17b0bc5e2aa60cfe0021ef5e20c0e5a0682818ca,"Data are presented on the development of tests of reading skill for primary school pupils in rural Tanzania. Instruction in these schools is in Kiswahili, a regularly spelled language. Using a translation of a standard reading test, children can read aloud all words once they have learned the sound– letter correspondences, regardless of comprehension. In addition, children can pass traditional comprehension tasks by decoding only some of the words. Three graded tests were developed to test children who had only some letter knowledge, could read single words, or were proficient readers. The tests required children both to decode and to understand the reading material in order to achieve high scores. The tests correlated well with scores on other educational achievement tests and showed age and school grade differences. It is suggested that these tests are useful measures of reading development in a regularly spelled language. Their adaptation to English and validation against standardized instruments are planned."
340c9c1857af3300ac5079974c08fec695a34189,"Introduction : Tests of semantic fluency require participants to produce as many examples as possible of a category in a limited time. It has been suggested that individuals with Williams syndrome perform relatively well on such tasks, and in addition produce particularly unusual category exemplars. This may reflect an abnormal organisation of semantic information. Methods : A study is reported which tests these claims, by comparing the semantic fluency of a group of individuals with Williams syndrome with that of controls matched for level of vocabulary. Individuals' responses are analysed in terms of absolute number of items produced, frequency and typicality of these items as category exemplars, and the ordering of semantically related items. Results : The results show that individuals with Williams syndrome produce as many items as controls, and that the items produced are not particularly unusual or atypical. However, the ordering of items provides evidence that individuals with Williams syndrome may have less sophisticated underlying semantic structures. Conclusions : The implications of these findings are discussed with reference to the development of conceptual knowledge in Williams syndrome."
22b5988462414ad98151bf5e6e9ebb340a0bfbb8,"In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way."
4e3aaac4439825650480f9cb914aa895d55d1e13,"Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classification task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training."
669496b0cf0b2ad5e4ab29c1175bc71793d2edc0,
70c9a8972e189d3a27f55394587f6386beb47ca1,"We show how to improve a state-of-the-art neural network language model that converts the previous ""context"" words into feature vectors and combines these feature vectors to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using higher-level features to modulate the effects of the con- text words. This is more effective than using the higher-level features to directly predict the feature vector of the next word, but it is also possible to combine both methods."
f2e95236f0fccc0b70e757ac2ebbc79b7f51de0a,"We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel."
8b1d79fd4db235be3920be043215664e1f36754b,
a3582fe13111f03de6481145ef1d27c85d7440f5,
a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb,"Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models."
c50dca78e97e335d362d6b991ae0e1448914e9a3,"http://www.sciencemag.org/cgi/content/full/313/5786/504 version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/cgi/content/full/313/5786/504/DC1 can be found at: Supporting Online Material found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/313/5786/504#related-content http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles , 6 of which can be accessed for free: cites 8 articles This article 15 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles 4 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining"
f5a6a59433eddd58db99e891134c4cd28a210837,"We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as(2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3,+3) ∈ plus and (+3,−3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations +3 or has wife even though it has not been trained on any first-order examples involving these relations."
55014072875d16b1c1cf0671fe810eee69504bb7,"The data presented by Kemps, De Rammelaere, and Desmet (2000, this issue) appear to have some aspects that fit most readily into our own model (Baddeley & Hitch, 1974), while others appear to support that of Pascual-Leone (1970). We accept that our initial model said little about development and was better able to account for relatively simple memory-based tasks than more complex cognitive activities. More recent elaborations of the model are, however, able to throw new light on the processes underlying cognitive development, offering a better account than that provided by existing neo-Piagetian interpretations. Meanwhile, the addition of a fourth component to the model, namely the episodic buffer, offers a way of dealing with more complex cognitive activities. Given the major differences between our own model and that of Pascual-Leone in basic assumptions, and in theoretical style, we suggest that any attempt to combine the two would be premature."
f85bc8fb86979c8b1b5f7d145c37930c10ace7ce,"We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has_wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, -3) ∈ inverse or (has_husband, has_wife) ∈ higher_oppsex. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations +3 or has_wife even though it has not been trained on any first-order examples involving these relations."
0fe5c42e0821f6580eb8ab4c4261771f0d0472bd,
1626c940a64ad96a7ed53d7d6c0df63c6696956b,"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system."
1fa265cca12dc92d5f1850d47e1bd338f924adf1,"We show how to visualize a set of pairwise similarities between objects by using several different two-dimensional maps, each of which captures different aspects of the similarity structure. When the objects are ambiguous words, for example, different senses of a word occur in different maps, so “river” and “loan” can both be close to “bank” without being at all close to each other. Aspect maps resemble clustering because they model pair-wise similarities as a mixture of different types of similarity, but they also resemble local multi-dimensional scaling because they model each type of similarity by a twodimensional map. We demonstrate our method on a toy example, a database of human wordassociation data, a large set of images of handwritten digits, and a set of feature vectors that represent words."
990fd8d28c9e16bed0dc77481923665d5c333bc6,"
 While there is copious evidence concerning the effectiveness of different instructional options in teaching
 grammar (e.g., Nassaji, 2017; Pawlak, 2017),
 less is known about the extent to which the contribution of pedagogical intervention is mediated by individual factors. The same
 can be said about the product of instructed but also uninstructed second language acquisition, that is the knowledge of target
 language grammar. The paper attempts to shed light on one such variable, that is working memory, which has recently been an object
 of intensive empirical inquiry (e.g., Li, 2017; Wen, Biedroń, & Skehan, 2016). It reports the results of a study that investigated the role of verbal working
 memory in the development of explicit and implicit knowledge of the English passive voice. Participants were 156 Polish university
 students enrolled in a three-year BA program in English. The data on verbal working memory were collected by means of the
 Polish Listening Span Test (PLSPAN), developed by Zychowicz, Biedroń and
 Pawlak (2017). Explicit knowledge was tapped by means of an untimed grammatically judgment test, which focused on
 reception, and a traditional grammar test, which targeted production. Implicit knowledge was tapped through a timed grammaticality
 judgment test for reception and a focused communication task (Ellis, 2003) for
 production. Correlational analysis demonstrated that verbal working memory was a weak predictor of explicit productive and
 receptive knowledge but not implicit knowledge."
82e90dd635373d099e3bb4dc82f9c1e340d9a15f,"The phonological loop, which is a component of working memory, is considered to be one of the most significant factors affecting L1 and L2 learning. In order to measure this construct properly, a reliable instrument in the native language of the participants is needed. The purpose of this paper is to present the Polish Nonword Span PNWSPAN , which is a tool constructed to measure verbal working memory, in particular the phonological loop, in the case of adults. The article presents the theoretical framework of the study and the process of construction of the test, namely its structure, scoring and validation procedure."
2ac91e028cdc602695b46bd1f372c03b4d2776cf,"We describe a probabilistic model for learning rich, distributed representations of image transformations. The basic model is defined as a gated conditional random field that is trained to predict transformations of its inputs using a factorial set of latent variables. Inference in the model consists in extracting the transformation, given a pair of images, and can be performed exactly and efficiently. We show that, when trained on natural videos, the model develops domain specific motion features, in the form of fields of locally transformed edge filters. When trained on affine, or more general, transformations of still images, the model develops codes for these transformations, and can subsequently perform recognition tasks that are invariant under these transformations. It can also fantasize new transformations on previously unseen images. We describe several variations of the basic model and provide experimental results that demonstrate its applicability to a variety of tasks."
3f0b88c45b16aa85652b1e1cc531eec378fc32a9,
51ff037291582df4c205d4a9cbe6e7dcec8f5973,
80c330eee12decb84aaebcc85dc7ce414134ad61,"We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images."
29b44744e70e7ac1428770c03af2198be2331128,
ad33d1fa8628cb55c32fb52feb537f65184c3b29,"We show how to pretrain and fine-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classification performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classification, our method uses these dimensions to explicitly represent transformations of the digits that do not affect their identity."
bd7d93193aad6c4b71cc8942e808753019e87706,The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.
c1004e96c8f95dca6df0acb4fad0bd2bcfba25db,"A persistent worry with computational models of unsupervised learning is that learning will become more difficult as the problem is scaled. We examine this issue in the context of a novel hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We first demonstrate that the model can extract a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms. We then investigate some of the scaling properties of the algorithm on this problem and find that: (1) Increasing the image size leads to faster and more reliable learning; (2) Increasing the depth of the network from one to two hidden layers leads to better representations at the first hidden layer, and (3) Once one part of the network has discovered how to represent depth, it “supervises” other parts of the network, greatly speeding up their learning."
c74e230a5a6fd5e2db6ace765ce38afe65f96214,"We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box."
64a9e1363947b6d7ce5eef44bd2f121f84f5b88c,"As the machine learning community tackles more complex and harder problems, the graphical models needed to solve such problems become larger and more complicated. As a result performing inference and learning exactly for such graphical models become ever more expensive, and approximate inference and learning techniques become ever more prominent. 
There are a variety of techniques for approximate inference and learning in the literature. This thesis contributes some new ideas in the products of experts (PoEs) class of models (Hinton, 2002), and the Bethe free energy approximations (Yedidia et al., 2001). 
For PoEs, our contribution is in developing new PoE models for continuous-valued domains. We developed RBMrate, a model for discretized continuous-valued data. We applied it to face recognition to demonstrate its abilities. We also developed energy-based models (EBMs)—flexible probabilistic models where the building blocks consist of energy terms computed using a feed-forward network. We show that standard square noiseless independent components analysis (ICA) (Bell and Sejnowski, 1995) can be viewed as a restricted form of EBMs. Extending this relationship with ICA, we describe sparse and over-complete representations of data where the inference process is trivial since it is simply an EBM. 
For Bethe free energy approximations, our contribution is a theory relating belief propagation and iterative scaling. We show that both belief propagation and iterative scaling updates can be derived as fixed point equations for constrained minimization of the Bethe free energy. This allows us to develop a new algorithm to directly minimize the Bethe free energy, and to apply the Bethe free energy to learning in addition to inference. We also describe improvements to the efficiency of standard learning algorithms for undirected graphical models (Jirousek and Preucil, 1995)."
01ae3c7989911f9ec16dd56fb044b7e632d6394c,"Individual differences in second language acquisition (SLA) encompass differences in working memory capacity, which is believed to be one of the most crucial factors influencing language learning. However, in Poland research on the role of working memory in SLA is scarce due to a lack of proper Polish instruments for measuring this construct. The purpose of this paper is to discuss the process of construction and validation of the Polish Listening Span (PLSPAN) as a tool intended to measure verbal working memory of adults. The article presents the requisite theoretical background as well as the information about the PLSPAN, that is, the structure of the test, the scoring procedures and the steps taken with the aim of validating it."
a1eddd801056a020b41e7249ec8b9a9c117ef1b2,"While a substantial body of empirical evidence has been accrued about the role of individual differences in second language acquisition, relatively little is still known about how factors of this kind can mediate the effects of instructional practices as well as how empirically-derived insights can inform foreign language pedagogy, both with respect to shaping certain variables and adjusting instruction to individual learner profiles. The present paper is an attempt to shed light on the interface between research on individual difference factors and teaching practice, focusing upon variables which do not easily lend themselves to external manipulation, namely intelligence, foreign language aptitude, working memory and personality, with the role of the last of these in language learning being admittedly the least obvious. In each case, the main research findings will briefly be outlined, their potential for informing instruction will be considered, and, in the final part, the caveats concerning practical applications of research on the variables in question will be spelled out."
46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e,"High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
497a80b2813cffb17f46af50e621a71505094528,"We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued ""visible"" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture."
6de83fe0ca1378259fe2bae1c5914cf110113256,"In the pelletization of particles of carbonaceous material improvements in pellet strength are obtained by applying a first water soluble or swellable thermo-hardening binder and a second binder which is an emulsion of a heavy hydrocarbon in such a way that the concentration of the first binder decreases from the interior to the exterior of the pellet, and the concentration of the second binder increases from the interior to the exterior of the pellet."
8978cf7574ceb35f4c3096be768c7547b28a35d0,"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
98a3c337a435553add253eb1af71eb9fc998bf5e,"We describe a way of modeling high-dimensional data vectors by using an unsupervised, nonlinear, multilayer neural network in which the activity of each neuron-like unit makes an additive contribution to a global energy score that indicates how surprised the network is by the data vector. The connection weights that determine how the activity of each unit depends on the activities in earlier layers are learned by minimizing the energy assigned to data vectors that are actually observed and maximizing the energy assigned to ""confabulations"" that are generated by perturbing an observed data vector in a direction that decreases its energy under the current model."
b35c86d987913c35daea3057e615e5c9a2c3fdda,"Overview Document Retrieval – Present layer-by-layer pretraining and the fine-tuning of the multi-layer network that discovers binary codes in the top layer. This allows us to significantly speed-up retrieval time. – We also show how we can use our model to allow retrieval in constant time (a time independent of the number of documents). Show how to perform nonlinear embedding by preserving class neighbouthood structure (supervised, semi-supervised)."
d8d01934cb26064b253dbd0f1627519133c3df3e,"We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to natural data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of our new approach to previous workin particular, gaussian scale mixture models and variants of independent components analysis."
c4e145b4ea81e479edeed5a2b25c25039922cc03,"This state-of-the art paper focuses on the issue of linguistic giftedness, somewhat neglected in the second language acquisition (SLA) literature, attempting to reconceptualize, expand and update this concept in response to latest developments in the fields of psychology, linguistics and neurology. It first discusses contemporary perspectives on foreign language aptitude, concentrating in particular on the models proposed by Skehan (1998), Robinson (2002) and Sternberg (2002). This is followed by a discussion of the definitions of talented individuals and the criteria for their selection, as well as an overview of empirical research on gifted language learners, divided into early studies with those focusing on the ultimate attainment of post-pubescent learners, and those dealing with super-learners of foreign languages. The subsequent sections touch upon such issues as the relationship between first language (L1) ability and second language (L2) aptitude, and linguistic giftedness and intelligence, memory, personality factors and language learning strategies, as well as neurolinguistic research on brain functioning in gifted learners. The paper closes with the discussion of the limitations of current research, its future directions and methodological considerations."
3ffc193177ee33a92f3acf3f8e607a2c861461df,"We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables. The learning procedure uses a variational approximation for the posterior distribution over the hidden variables. Despite the intractable partition function of the MRF, the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data. The parameters of the MRF are learned by using the mean field version of contrastive divergence [1]. We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images. We discuss the extension to multiple MRF’s linked into in a chain graph by directed connections."
4698012eb886f76bdbbebb8bcdba31f9c1b09d4b,"First page of article 
 
 
Keywords: 
 
artificial intelligence: neural networks; 
action and perceptual system"
878f90055f639711f29dd566ab341697a13f9a1b,
921c234f9f4085af14cf984d3f3ac605fb945020,"We describe anapproach toimprove iterative di- mensionality reduction methods byusing information contained intheleading eigenvectors ofa dataaffinity matrix. Using an insight fromtheareaofspectral clustering, we suggest modifying thegradient ofaniterative method, sothatlatent spaceelements belonging tothesamecluster areencouraged to moveinsimilar directions during optimization. Wealso describe waytoachieve this without actually having toexplicitly perform aneigendecomposition. Preliminary experiments showthat our approach makesitpossible tospeed upiterative methods and helps themtofind better local minima oftheir objective function."
e90c7dfdafd21bf30aca3129e645f1c57f5c469f,
a4a5bef06587350604c7a9857ca09d91bd95763e,"If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep, multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1:7 million connections into a very good generative model of handwritten digits. After learning, the model gives classification performance that is comparable to the best discriminative methods."
e270bfa5b662c531a61a5b274da636603c23a734,"Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called “contrastive divergence” (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution. Consider a probability distribution over a vector x (assumed discrete w.l.o.g.) and with parameters W p(x;W) = 1 Z(W) e (1) where Z(W) = ∑ x e −E(x;W) is a normalisation constant and E(x;W) is an energy function. This class of random-field distributions has found many practical applications (Li, 2001; Winkler, 2002; Teh et al., 2003; He et al., 2004). Maximum-likelihood (ML) learning of the parameters W given an iid sample X = {xn}n=1 can be done by gradient ascent: W = W + η ∂L(W;X ) ∂W ∣"
e3d4f463823b5a50963073f71d3c8ea29d6005fb,"We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits. The inferred motor programs can be used directly for digit classification, but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods. We can also use the motor programs as additional, highly informative outputs which reduce overfitting when training a feed-forward classifier."
064dd12847dcaa69ae758c93bfd026f54d575599,"Under-complete models, which derive lower dimensional representations of input data, are valuable in domains in which the number of input dimensions is very large, such as data consisting of a temporal sequence of images. This paper presents the under-complete product of experts (UPoE), where each expert models a one-dimensional projection of the data. Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components. The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete independent component analysis (UICA) models. This paper also derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA), projection pursuit density estimation, and feature induction algorithms for additive random field models. This paper demonstrates the efficacy of these novel algorithms on high-dimensional continuous datasets."
1a29597ec321e940748fd2afb2905bd47ef2fc15,
2184fb6d32bc46f252b940035029273563c4fc82,"Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these ""exponential family harmoniums"" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords."
c938b2d67433b7777d72d2b44114a939ae0b39bb,"Deformable models are an attractive approach to recognizing objects which have considerable within-class variability such as handwritten characters. However, there are severe search problems associated with fitting the models to data which could be reduced if a better starting point for the search were available. We show that by training a neural network to predict how a deformable model should be instantiated from an input image, such improved starting points can be obtained. This method has been implemented for a system that recognizes handwritten digits using deformable models, and the results show that the search time can be significantly reduced without compromising recognition performance. © 1997 Academic Press."
e738c99333e1ca3d50590206fa089902c472e53a,"Foreign language (FL) aptitude generally refers to a specific talent for learning a foreign or second language (L2). After experiencing a long period of marginalized interest, FL aptitude research in recent years has witnessed renewed enthusiasm across the disciplines of educational psychology, second language acquisition (SLA) and cognitive neuroscience. This paper sets out to offer a historical and an updated account of this recent progress in FL aptitude theory development and research. As its subtitle indicates, the paper centres on three major issues: following the introduction and clarification of basic concepts, Section 1 traces the early conceptions of FL aptitude dominated by John Carroll's pioneering work. Section 2 summarizes and examines more recent theoretical perspectives and FL aptitude models proposed by researchers from multiple disciplines that have significantly broadened the conventional research traditions associated with Carroll's original conception. Based on the research synthesis of current FL aptitude models, Section 3 suggests the directions FL aptitude theory and research might take in coming years. We conclude that a working memory perspective on FL aptitude presents one promising avenue for advance, as does the development of new aptitude tests to predict speed of automatization, implicit learning and greater control over an emerging language system. In addition, it is argued that issues of domain-specificity versus domain-generality for aptitude tests may lead to aptitude theory and research becoming more central in applied linguistics."
39ce7f26601db48e302bc970cfa7e4742e8528bf,"This state-of-the art paper focuses on the poorly explored issue of foreign language aptitude, attempting to present the latest developments in this field and reconceptualizations of the construct from the perspective of neuroscience. In accordance with this goal, it first discusses general directions in neurolinguistic research on foreign language aptitude, starting with the earliest attempts to define the neurological substrate for talent, sources of difficulties in the neurolinguistic research on foreign language aptitude and modern research methods. This is followed by the discussion of the research on the phonology of foreign language aptitude with emphasis on functional and structural studies as well as their consequences for the knowledge of the concept. The subsequent section presents the studies which focus on lexical and morphosyntactic aspects of foreign language aptitude. The paper ends with a discussion of the limitations of contemporary research, the future directions of such research and selec ed methodological issues."
24c287d97982216c8f35c8d326dc2ec2d2475f3e,"In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction."
60922d2ca51acbebff794f4c43f6daadf4b8d103,A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.
9c8fe61598b103ef1a3bdd2bef044439ddf71ed4,"We describe a way of using multiple different types of similarity relationship to learn a low-dimensional embedding of a dataset. Our method chooses different, possibly overlapping representations of similarity by individually reweighting the dimensions of a common underlying latent space. When applied to a single similarity relation that is based on Euclidean distances between the input data points, the method reduces to simple dimensionality reduction. If additional information is available about the dataset or about subsets of it, we can use this information to clean up or otherwise improve the embedding. We demonstrate the potential usefulness of this form of semi-supervised dimensionality reduction on some simple examples."
18164e7bb84117509fa15d0af1c8ff713fb3b943,One of the main shortcomings of Markov chain Monte Carlo samplers is their inability to mix between modes of the target distribution. In this paper we show that advance knowledge of the location of these modes can be incorporated into the MCMC sampler by introducing mode-hopping moves that satisfy detailed balance. The proposed sampling algorithm explores local mode structure through local MCMC moves (e.g. diffusion or Hybrid Monte Carlo) but in addition also represents the relative strengths of the different modes correctly using a set of global moves. This “mode-hopping” MCMC sampler can be viewed as a generalization of the darting method [1].
c0232ae85a776c53c4499cfa052464637c254184,"Abstract Replication failures were among the triggers of a reform movement which, in a very short time, has been enormously useful in raising standards and improving methods. As a result, the massive multilab multi-experiment replication projects have served their purpose and will die out. We describe other types of replications – both friendly and adversarial – that should continue to be beneficial."
c6d8e218a309631094a00b26ec3e28430425aeec,"Some of the best and most influential papers by Amos Tversky, one of the most brilliant social science thinkers of the twentieth century. Amos Tversky (1937-1996) was a towering figure in the cognitive and decision sciences. His work was ingenious, exciting, and influential, spanning topics from intuition to statistics to behavioral economics. His long and extraordinarily productive collaboration with his friend and colleague Daniel Kahneman was the subject of Michael Lewis's best-selling book, The Undoing Project: A Friendship that Changed Our Minds. The Essential Tversky offers a selection of Tversky's best, most influential and accessible papers, ""classics"" chosen to capture the essence of Tversky's thought. The impact of Tversky's work is far reaching and long-lasting. In 2002, Kahneman, who drew on their joint work in his much-praised 2013 book, Thinking, Fast and Slow (and who contributes an afterword to this collection), was awarded the Nobel Prize in Economics for work done with Tversky. In The Undoing Project, Lewis (who contributes a foreword to this collection) describes his discovery that Tversky and Kahneman's thinking laid the foundation for Moneyball, his own ode to number-crunching. The papers collected in The Essential Tversky cover topics that include cognitive and perceptual bias, misguided beliefs, inconsistent preferences, risky choice and loss aversion decisions, and psychological common sense. Together, they offer nonspecialist readers an introduction to one of the most brilliant social science thinkers of the twentieth century."
51f4a068e466010d9dd4cc000f4261c360407764,
9952caa963e3afb173505b8caf232c2e0511223b,"In models that define probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model's distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate."
b95799a25def71b100bd12e7ebb32cbcee6590bf,"We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces."
bd05feae0feb756bace09d6eedcd4d5fb7edff45,"Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the ""undercomplete product of experts"" (UPoE), where each expert models a one dimensional projection of the data. The UPoE may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models."
eed2c48ddb158e4b2a558ae37a059540009ff308,"Abstract Modelers have come up with many different learning rules for neural networks. When a teacher specifies the correct output, error-driven rules work better than pure Hebb rules in which the changes in synapse strength depend on the correlation between pre and postsynaptic activities. But for unsupervised learning, Hebb rules can be very effective if they are combined with suitable normalization or ""unlearning"" terms to prevent the synapses growing without bound. Hebb rules that use rates of change of activity instead of activity itself are useful for discovering perceptual invariants and may also provide a way of implementing error-driven learning. It would be truly wonderful if randomly connected neural networks could turn themselves into useful computing devices by using some simple rule to modify the strengths of synapses. This was the hope that lay behind the original Hebb learning rule and it is the vision that has driven neural network modelers for half a century. Initially, researchers tried simulating various rules to see what would happen. After a decade or two of messing around, researchers realized that there was a much better way to explore the space of possible learning rules: First write down an objective function (a quantitative definition of how well the network is performing) and then use elementary calculus to derive a learning rule that will improve the objective function. For the last few decades, the big theoretical advances in learning rules for neural networks have been associated with new optimization methods and new ideas about what objective function should be optimized. If we think of a neural network as a device for converting input vectors into output vectors, it is obvious that one sensible objective is to minimize some measure of the difference between the output the network actually produces and the output it ought to produce. This approach led to effective ""error-driven"" learning rules such as the Widrow-Hoff rule (Widrow & Hoff, 1960) and the perceptron convergence procedure (Rosenblatt, 1961) and it was later generalized to multilayer networks by using backpropagation of the errors to get training signals for intermediate ""hidden"" layers (Rumelhart, Hinton, & Williams, 1986). Within the neural network community, the ""Hebbian"" approach of using the product of pre and postsynaptic activities to drive learning was seen as inferior to error-- driven methods that use the product of the presynaptic activity and the postsynaptic activity derivative - the rate at which the objective function changes as the postsynaptic activity is changed. Even when the task was merely to associate random input vectors with random output vectors, it was shown that an error-driven rule worked much better than a Hebbian rule. Unfortunately, error-driven learning has some serious drawbacks. It requires a teacher to specify the right answer and it is hard to see how neurons could implement the backpropagation required by multilayer versions. It is possible to get the teaching signal from the data itself by trying to predict the next term in a temporal sequence (Elman, 1991) or by trying to reconstruct the input data at the output (Hinton, 1989) but it is also possible to use quite different objective functions for learning. Some of these alternative objective functions lead to learning rules that are far more Hebbian in flavour. A common objective in processing high-dimensional data is to reduce the dimensionality without losing the ability to reconstruct the raw data from the reduced representation. If we measure the accuracy of the reconstruction by the squared error, the optimal strategy is to extract the principal components - the dominant directions of variation in the data. Oja (1982) showed how to extract the first principal component using Hebbian learning to maximize the squared output of a neuron combined with normalization of the synapse strengths to prevent them growing without bound. …"
0f18e18d436c51868a2cba5c7df3859986d6ba40,"Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of ""negative examples"" generated from the model's current estimate of the data density. Training in each boosting round proceeds in three stages: first we sample negative examples from the model's current Boltzmann distribution. Next, a feature is trained to improve classification performance between data and negative examples. Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data."
115832475f7dda7f0b29cca0e98f26294c855669,"Ray dedicated his life to his research with the wonder of a child, the fearlessness of an explorer, the precision of a mathematician, and the tirelessness of a researcher who found shallowness and confusion intolerable. He leaves a legacy of groundbreaking, deep insights that have changed the course of AI."
8db407cbefc6a37a9d903997772aed056952d39b,"Design involves an account of expertise which foregrounds implicit, heuristic skills. Most models of policy making have a stronger interest in structural and exogenous pressures on decision making. Research suggests that high-level experts develop unique capacities to process data, read a situation, and see imaginative solutions. By linking some of the key attributes of a design model of decision making to an account of expertise, it is possible to formulate a stronger model of public policy design expertise. While other approaches often concern themselves with constraints and structural imperatives, a design approach has a focus upon the capacities of individual actors such as policy experts. Such an approach rests upon central propositions in regard to goal emergence, pattern recognition, anticipation, emotions engagement, fabulation, playfulness, and risk protection. These provide a starting point for further research and for the professional development of policy specialists."
f8bfcbe3133bfe96ed6c8ff7f92b496203fa903b,............................................................................................................................. ii Dedication ........................................................................................................................ iii Acknowledgments ............................................................................................................. iv Vita ......................................................................................................................................v List of Figures ................................................................................................................... vii Chapter 1: Introduction to Anchoring .................................................................................1 Chapter 2: Study 1 .............................................................................................................13 Chapter 3: Study 2 .............................................................................................................19 Chapter 4: Discussion and Concluding Remarks...............................................................27 References ..........................................................................................................................30
127bf1f99d9ec32833183c2c8160903151cfafcf,"The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data."
14d2d9b2e4c29fe105bfbb31f9749b60690303a7,"We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Student-t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the ""iterated Wiener filter"" for the purpose of denoising images."
14d46c6396837986bb4b9a14024cb64797b8c6c0,"We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional ""images"" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ""bank"", to have versions close to the images of both ""river"" and ""finance"" without forcing the images of outdoor concepts to be located close to those of corporate concepts."
2ca21ac1bd2aa1d1ce92c9fda2da11a425cc2a15,"We present a novel input device and interface for interactively controlling the animation of graphical human character from a desktop environment. The trackers are embedded in a new physical design, which is both simple yet also provides significant benefits, and establishes a tangible interface with coordinate frames inherent to the character. A layered kinematic motion recording strategy accesses subsets of the total degrees of freedom of the character. We present the experiences of three novice users with the system, and that of a long-term user who has prior experience with other complex continuous interfaces."
36d4755ee4e44f29a4cc23b5f3541ce6fddb44ff,"This is a summary of the presentation by Professor Daniel Kahneman, Eugene Higgins Professor of Psychology and Professor Public Affairs at the Woodrow Wilson School, Princeton University, Princeton, NJ, USA, given at the ISPOR “Building a Pragmatic Road: Moving the QALY Forward” Consensus Development Workshop, as a continuation of the discussion on an experience-based approach to health state valuation, as presented on May 2005 at the ISPOR 10th Annual International Meeting First Plenary Session, “Determinants of Health Economic Decisions in Actual Practice: The Role of Behavioral Economics” at: http://www.ispor.org /meetings / Invitational /Plenary%20 Presentation%20by%20D%20Kahneman.pdf [1,2], and debated at the Invited Issue Panel, “Will the QALY Survive” [3,4] at the ISPOR 11th Annual International Meeting, May 2006."
48b79bfcc1e8856c8505e53c90bb009572310138,"F or at least two hundred years, people have asked of a society 'how happy are its people?' and likewise of a policy 'will it make people happier?' Until recently, there was very little scientific information to answer these questions. But in the past few decades, things have changed radically, mainly due to progress in social surveys, in psychology and in medical science. With a few important exceptions, most of the best work has been done in the United States. With a view to launching a major research programme in Britain, Professors Richard Layard and Paul Dolan convened an international workshop on happiness research at CEP in October 2008. The programme would combine fundamental research and applied work on the effectiveness of different policy interventions, and include researchers from economics, psychology, medical science, philosophy, politics and sociology. It would make use of the vast mass of unexploited data on happiness as well as collecting new data, both experimental and non-experimental. The opening presentation at the workshop was by Professor Daniel Kahneman of Princeton University, who is a pioneer in this field of research and the only psychologist to be awarded the Nobel Prize in economics, an accolade he received in 2002. Afterwards, Romesh Vaitilingam interviewed him about how we should go about understanding happiness (or 'subjective well-being') as an indicator of social progress. Romesh Vaitilingam: You make a distinction between living and thinking about it – and between what you call our 'experiencing-self' and our 'remembering-self'. Could you explain these ideas and their significance for happiness research? Daniel Kahneman: We keep insisting that there is one notion of happiness or well-being. I argue that we need at least two. One measurement you obtain when you ask people how they feel right now – I call this experience happiness. Another you obtain when you ask people how they think about their life – this is life evaluation. It turns out that experience happiness and life evaluation have very different determinants and very different consequences. They are both legitimate parts of well-being but we need to look at them separately. RV: You also talk about the focusing illusion. Could you explain this idea? Nobel laureate Daniel Kahneman talks to CentrePiece editor Romesh Vaitilingam about how we should go about understanding happiness as an indicator of social progress. Understanding happiness: the distinction between living – and thinking about it"
ee0142a7f6a9ab17872affbc397a35e8964c6612,
f0a0ca78388a691fc9654fd4acb332e68570c2dc,
4579cf167e82498c3fbb5cc3979ad17e40443491,
607aa02d95827fbce9b24e0d7e7006affd8abaea,"Our goal is to design and build a tool for the creation of expressive character animation. Virtual puppetry, also known as performance animation, is a technique in which the user interactively controls a character's motion. In this paper we introduce local physical models for performance animation and describe how they can augment an existing kinematic method to achieve very effective animation control. These models approximate specific physically‐generated aspects of a character's motion. They automate certain behaviours, while still letting the user override such motion via a PD‐controller if he so desires. Furthermore, they can be tuned to ignore certain undesirable effects, such as the risk of having a character fall over, by ignoring corresponding components of the force. Although local physical models are a quite simple approximation to real physical behaviour, we show that they are extremely useful for interactive character control, and contribute positively to the expressiveness of the character's motion. In this paper, we develop such models at the knees and ankles of an interactively‐animated 3D anthropomorphic character, and demonstrate a resulting animation. This approach can be applied in a straight‐forward way to other joints."
9360e5ce9c98166bb179ad479a9d2919ff13d022,"It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
b7b5bea7b4d40003a6887794652ea07196a97134,
fa81f01142a848679f614a440f3e585f0ecad4c0,"Learning to act optimally in a complex, dynamic and noisy environment is a hard problem. Various threads of research from reinforcement learning, animal conditioning, operations research, machine learning, statistics and optimal control are beginning to come together to offer solutions to this problem. I present a thesis in which novel algorithms are presented for learning the dynamics, learning the value function, and selecting good actions for Markov decision processes. The problems considered have high-dimensional factored state and action spaces, and are either fully or partially observable. The approach I take is to recognize similarities between the problems being solved in the reinforcement learning and graphical models literature, and to use and combine techniques from the two fields in novel ways. 
In particular I present two new algorithms. First, the DBN algorithm learns a compact representation of the core process of a partially observable MDP. Because inference in the DBN is intractable, I use approximate inference to maintain the belief state. A belief state action-value function is learned using reinforcement learning. I show that this DBN algorithm can solve POMDPs with very large state spaces and useful hidden state. Second, the PoE algorithm learns an approximation to value functions over large factored state-action spaces. The algorithm approximates values as (negative) free energies in a product of experts model. The model parameters can be learned efficiently because inference is tractable in a product of experts. I show that good actions can be found even in large factored action spaces by the use of brief Gibbs sampling. 
These two new algorithms take techniques from the machine learning community and apply them in new ways to reinforcement learning problems. Simulation results show that these new methods can be used to solve very large problems. The DBN method is used to solve a POMDP with a hidden state space and an observation space of size greater than 2180. The DBN model of the core process has 232 states represented as 32 binary variables. The PoE method is used to find actions in action spaces of size 240 ."
088720694feed36064044545f99f618ac620ee99,"We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its final goal is to be able to generalize, i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures, such as trees and lists."
1cec83b461e780f5fa04aba579fe1c5669f6dc6e,"We present a novel class of learning algorithms for undirected graphical models, based on the contrastive free energy (CF). In particular we study the naive mean field, TAP and Bethe approximations to the contrastive free energy. The main advantage of CFlearning is the fact that it eliminates the need to infer equilibrium statistics for which mean field type approximations are particularly unsuitable. Instead, learning decreases the distance between the data distribution and the distribution with onestep reconstructions clamped on the visible nodes. We test the learning algorithm on the classification of digits."
fe7716fa80fcf7cea3ac71666754360c77197f42,"This article reports on an effort to explore the differences between two approaches to intuition and expertise that are often viewed as conflicting: heuristics and biases (HB) and naturalistic decision making (NDM). Starting from the obvious fact that professional intuition is sometimes marvelous and sometimes flawed, the authors attempt to map the boundary conditions that separate true intuitive skill from overconfident and biased impressions. They conclude that evaluating the likely quality of an intuitive judgment requires an assessment of the predictability of the environment in which the judgment is made and of the individual's opportunity to learn the regularities of that environment. Subjective experience is not a reliable indicator of judgment accuracy."
0010cf0981ad4c1176e101137f39d1d6a690c113,"The optimal moment to address the question of how to impr ove human decision making has arrived. Thanks to fifty years of research by judgment and decision making scholars, psychologists have developed a d tailed picture of the ways in which human judgment is bounded. This paper argues that the time has come to focus attention on the search for st rategies that will improve bounded judgment because decision making errors are costly an d are growing more costly, decision makers are receptive, and academic insights are sure to follow from research on improvement. In addition to ca lling for research on improvement strategies, this paper organizes the existing li terature pertaining to improvement strategies, highlighting promising directions f r future research. Acknowledgements: We thank Katie Shonk and Elizabeth Weiss for their assistance. How Can Decision Making Be Improved?   3 Daniel Kahneman, Amos Tversky, and others have clarifi ed the specific ways in which decision makers are likely to be biased. As a resu lt, we can now describe how people make decisions with astonishing detail and reliability. Furthermore, thanks to the normative models of economic theory, we have a clear vision of how much better decision making could be. If we all behaved optimally, c osts and benefits would always be accurately weighed, impatience would not exist, no rel evant information would ever be overlooked, and moral behavior would always be aligned w ith moral attitudes. Unfortunately, we have little understanding of how to hel p people overcome their many biases and behave optimally. The Big Question We propose that the time has come to move the study of biases in judgment and decision making beyond description and toward the developmen t of improvement strategies. While a few important insights about how t improve decision making have already been identified, we argue that many others awai t discovery. We hope judgment and decision-making scholars will focus their attention on the search for improvement strategies in the coming years, seeking to answer the ques tion: how can we improve decision making? Why the Question Is Important"
262d314d7bfab438bcb4dbbf8e858f630e279b5e,This paper describes research in progress on two quite different ways of training systems that are composed of many small Hidden Markov Models (HMM’s). The first is a purely discriminative method in which all of the parameters of all the HMM’s are adjusted to optimize classification performance. The second is an unsupervised method in which many little HMM’s are used to model the probability density of a single set of sequences.
643dbf7b8eafb3a5445fc0cd722c3136f29ac67e,"An efficient and useful representation for an object viewed from different positions is in terms of its instantiation parameters. We show how the Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to develop a population code for the instantiation parameters of an object in an image. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a standard shape (a bump) in this space, they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in a self-supervised network are trained to make the activities form a bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops separate population codes when presented with different objects."
66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b,"Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations."
7d49080de5eecf0dd756ed6b28743aa837fce881,"High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers, and the ""global coordination"" of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model's parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones."
822470d70758828217780563e9fbe86bf426b7d1,"ral networks in the 1980s was primarily fueled by supervised learning, exemplified by the backpropagation algorithm. In supervised learning, a desired output signal is provided to the learner together with an input signal, and the system adjusts parameters so that its response in the future will be closer to the desired signal. Although supervised learning has been dominant in machine learning, much of our intelligence, in particular, perception, is acquired without a teacher. Through mere exposure, humans and animals learn how to analyze their environments and recognize relevant objects and events. For example, consider our experience of sorting out apples from oranges by their appearances, an ability that can be gained before naming them. This analysis calls for unsupervised learning—learning without a teacher, also known as self-organization. Unsupervised learning has been studied in neural networks since the early days. However, in recent years, there has been a steady shift in the research focus from supervised learning to unsupervised learning, and the latter now becomes a predominant subject in neural networks. Unsupervised Learning: Foundations of Neural Computation is a collection of 21 papers published in the journal Neural Computation in the 10-year period since its founding in 1989 by Terrence Sejnowski. Neural Computation has become the leading journal of its kind. The editors of the book are Geoffrey Hinton and Terrence Sejnowski, two pioneers in neugiving external instruction? There is no simple answer to this critical question. In fact, many different objectives have been proposed, including to discover clusters in the input data, extract features that characterize the input data more compactly, and uncover nonaccidental coincidences within the input data. Beneath these objectives is the fundamental task of representation: Unsupervised learning attempts to derive hidden structure from the raw data. This endeavor is meaningful because input data are far from random; they are produced by physical processes. For example, a picture taken by a camera reflects the luminance of physical objects that constitute the visual scene, and an audio recording reflects acoustic events in the auditory scene. Physical processes tend to be coherent; an object occupies a connected region of the space, has a smooth surface, moves continuously, and so on. From the information theory standpoint, physical objects and events tend to have limited complexity and can be described in a small number of bits. This observation is, in my view, the foundation of unsupervised learning. Because perception is concerned with recovering the physical causes of the input data, a better representation should reveal more of the underlying physical causes. Physical causes are hidden in the data, and they could, in principle, be revealed by unsupervised learning. However, there is an enormous variety of physical causes; trees have different colors, have textures, leave patterns, and so on, and they all look very different from animals. Without external supervision, the best unsupervised learning can achieve is to uncover generic structure that exists in a variety of physical causes. Fortunately, guided by some general assumptions or principles, there are plenty of interesting problems to solve. One general principle for unsupervised learning is minimum entropy proposed in Barlow’s article. The idea is that the derived representation should minimize redundancy (correlation) contained in the input data. The goal is similar to that pursued in communication theory: to minimize the bandwidth needed for signal transmission. Closely associated is the minimum–description length principle advocated in the Zemel and Hinton article on learning population codes. Another principle, put forward in Field’s article, is sparse coding: The goal of the representation is to minimize the number of units in a distributed network that are activated by"
8ab1847c54db8e1de233a6a995f455065995d0d7,
8b27153da18537bd7ec7fd8205d24a34d1c64883,"We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization."
95d002022b5a9342fd254f09957a4deff2f621df,Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the Standard method of discriminatively training HMM's.
b348e98f869a5b656f98688cb9d77208b8475379,"We present products of hidden Markov models (PoHMM's), a way of combining HMM's to form a distributed state time series model. Inference in a PoHMM is tractable and eAEcient. Learning of the parameters, although intractable, can be e ectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of. We show some results on modelling character strings, a simple language task and the symbolic family trees problem, which highlight these advantages."
c958128419f40636645d3e2c7a8c88b1073b7c4c,"We present a new way of interpreting ICA as a probability density model and a new way of fitting this model to data. The advantage of our approach is that it suggests simple, novel extensions to overcomplete, undercomplete and multilayer non-linear versions of ICA. 1. ICA AS A CAUSAL GENERATIVE MODEL Factor analysis is based on a causal generative model in which an observation vector is generated in three stages. First, the activities of the factors (also known as latent or hidden variables) are chosen independently from one dimensional Gaussian priors. Next, these hidden activities are multiplied by a matrix of weights (the “factor loading” matrix) to produce a noise-free observation vector. Finally, independent Gaussian “sensor noise” is added to each component of the noise-free observation vector. Given an observation vector and a factor loading matrix, it is tractable to compute the posterior distribution of the hidden activities because this distribution is a Gaussian, though it generally has off-diagonal terms in the covariance matrix so it is not as simple as the prior distribution over hidden activities. ICA can also be viewed as a causal generative model [1, 2] that differs from factor analysis in two ways. First, the priors over the hidden activities remain independent but they are non-Gaussian. By itself, this modification would make it intractable to compute the posterior distribution over hidden activities. Tractability is restored by eliminating sensor noise and by using the same number of factors as input dimensions. This ensures that the posterior distribution over hidden activities collapses to a point. Interpreting ICA as a type of causal generative model suggests a number of ways in which it might be generalized, for instance to deal with more hidden units than input dimensions. Most of these generalizations retain marginal independence of the hidden activities and add sensor noise, but fail to preserve the property that the posterior distribution collapses to a point. As Funded by the Wellcome Trust and the Gatsby Charitable Foundation. a result inference is intractable and crude approximations are needed to model the posterior distribution, e.g., a MAP estimate in [3], a Laplace approximation in [4, 5] or more sophisticated variational approximations in [6]. 2. ICA AS AN ENERGY-BASED DENSITY MODEL We now describe a very different way of interpreting ICA as a probability density model. In the next section we describe how we can fit the model to data. The advantage of our energy-based view is that it suggests different generalizations of the basic ICA algorithm which preserve the computationally attractive property that the hidden activities are a simple deterministic function of the observed data. Instead of viewing the hidden factors as stochastic latent variables in a causal generative model, we view them as deterministic functions of the data with parameters . The hidden factors are then used for assigning an energy , to each possible observation vector :"
e741bdb25efd158d7020d34a09c27fd2e3138c08,
16bdd9337ebc0f575defbb5c2c14f46358fe3f26,"The term utility can be interpreted in terms of the hedonic experience of an outcome (experienced utility) or in terms of the preference or desire for that outcome (decision utility). It is this second interpretation that lies at the heart of the methods that economists have developed to value non-market goods, such as health. In this article, we argue that decision utility is unlikely to generate meaningful data on the utility associated with different experiences, and instead economists should look towards developing measures that focus more directly on experienced utility."
1d0d97b19b41ea1d44a13256ce9ae88cf4afab78,"I study how limited abilities to process information affect choice behavior. I model the decision making process by an automaton, and measure the complexity of a specific choice rule by the minimal number of categories an automaton implementing the rule uses to process information. I establish that any choice rule that is less complicated than utility maximization displays framing effects. I then prove that the unique choice rule that results from an optimal tradeoff between maximizing utility and minimizing complexity is a history-dependent satisficing procedure that displays primacy and recency effects, default tendency and choice overload. ∗I am indebted to Bob Wilson for his devoted guidance, constant support, and most valuable suggestions. I am grateful to Ariel Rubinstein and Jeremy Bulow for the encouragement, productive discussions, and important comments. I thank Gil Kalai, Ron Siegel, and Andy Skrzypacz for most insightful feedback in various stages of this project. I also thank Matt Jackson, Jon Levin, Michael Ostrovsky, Roy Radner, Ilya Segal, and seminar participants at UC Berkeley, Brown, Caltech, Harvard, Hebrew University, Iowa, LSE, MIT, Northwestern, NYU, Stanford, Tel-Aviv, UCL, Washington University in St. Louis, and Yale for helpful comments. This research is supported in part by the Leonard W. and Shirley R. Ely Fellowship of the Stanford Institute for Economic Policy Research."
2899d2516fb877da40c4a7b95fe0b084a103b9bf,
fb14f063b6d4bbd803132ef1a2cd7f69c195ec30,"We present products of hidden Markov models (PoHMM’s), a way of combining HMM’s to form a distributed state time series model. Inference in a PoHMM is tractable and efficient. Learning of the parameters, although intractable, can be effectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of. We show some results on modelling character strings, a simple language task and the symbolic family trees problem, which highlight these advantages. Products of Hidden Markov Models Andrew D. Brown andy@gatsby.ucl.ac.uk Geoffrey E. Hinton hinton@gatsby.ucl.ac.uk"
388b9ef14cfb67372a1d6e7b6f3f5eea19d7f1d6,"The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-learning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation."
3a70524a98cd0c8e1f9dd160cacf56d613c3832c,"Linear Relational Embedding is a method of learning a distributed representation of concepts from data consisting of binary relations between concepts. Concepts are represented as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization."
66689a4f519cc1deb147def41f15870b97665487,"We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series modelshidden Markov models and linear dynamical systemsand is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, & Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models."
6837be732ea83290a4061458b0f3e1ef921fc7dc,"We discsus Hinton’s (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximisation procedure of Dempster, Laird & Rubin (1976)."
89b5b3ce906fcc3e578c6f074329b80da18190f5,"Prediction markets are markets for contracts that yield payments based on the outcome of an uncertain future event, such as a presidential election. Using these markets as forecasting tools could substantially improve decision making in the private and public sectors. We argue that U.S. regulators should lower barriers to the creation and design of prediction markets by creating a safe harbor for certain types of small stakes markets. We believe our proposed change has the potential to stimulate innovation in the design and use of prediction markets throughout the economy, and in the process to provide information that will benefit the private sector and government alike."
92098c25d64f5c9e7aab513750cf3ad69aed750a,"BY DANIEL KAHNEMAN AND AMOS TVERSKY' This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory. Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory. In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty. This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses. In addition, people generally discard components that are shared by all prospects under consideration. This tendency, called the isolation effect, leads to inconsistent preferences when the same choice is presented in different forms. An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights. The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains. Decision weights are generally lower than the corresponding probabilities, except in the range of low probabilities. Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling."
a8744bea48d039f591e52b5053822f2231cd112d,
b85a14846e5b2d20ec7ca2018e941f81b866dbcd,"you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit organization founded in 1995 to build trusted digital archives for scholarship. We work with the scholarly community to preserve their work and the materials they rely upon, and to build a common research platform that promotes the discovery and use of these resources. For more information about JSTOR, please contact support@jstor.org."
73e93d0346e8eee6c2ab45e46c26eaafb66e12a8,"We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations."
a9b8eb922d7530373f1e5fa6d6d2eb98cb60eda9,"Linear relational embedding (LRE) was introduced previously by the authors (1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains, which show that learning leads to good generalization."
dd7caa1aef84bf55bf5d436c3a91992b440b96fc,"A persistent worry with computational models of unsupervised learning is that learning will become more difficult as the problem is scaled. We examine this issue in the context of a novel hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We first demonstrate that the model can extract a sparse, distributed, hierarchical representation of global disparity from simplified random-dot stereograms. We then investigate some of the scaling properties of the algorithm on this problem and find that: (1) Increasing the image size leads to faster and more reliable learning; (2) Increasing the depth of the network from one to two hidden layers leads to better representations at the first hidden layer, and (3) Once one part of the network has discovered how to represent disparity, it “supervises” other parts of the network, greatly speeding up their learning."
f6766fdc16fd2bdb868af1e311f4b0afef03800d,"It is possible to combine multiple non-linear probabilistic models of the same data by multiplying the probability distributions together and then renormalizing. A “productof experts”is a very efficient way to model data that simultaneously satisfies many different constraints. It is difficult to fit a product of experts to data using maximum likelihood because the gradient of the log likelihood is intractable, but there is an efficient way of optimizing a different objective function and this produces good models of high-dimensional data."
b989190439b130dd5ef973806b1da988e0b1e900,"On the occasion of the November 2006 annual meeting of the Israeli-Palestinian Science Organization (IPSO), we, the members of IPSO's International Scientific Council, noted with considerable satisfaction the receipt of 71 proposals for joint scientific research between Palestinian and Israeli"
cead4f2e8edd111ed73222bbba79fcfda82dd10b,
1a6d209dc0bb99be6d4d87f4b7abc2cde07e331e,"We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained."
40c2747fea2465efd25e07143df7db68ca029412,We first show how to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generative model.
43e65fcf6d6ae239ad506684339dcaf2b703b39b,"A persistent worry with computational modelsof unsupervised learningis thatlearningwill becomemoredifficult astheproblem is scaled.We examinethis issuein thecontext of a novel hierarchical,generati ve model that can be viewed as a nonlineargeneralizationof factoranalysisandcanbeimplemented in a neuralnetwork. Themodelperformsperceptual inference in a probabilistically consistentmannerby using top-down, bottom-upandlateralconnections.Theseconnectionscanbe learnedusingsimplerules that requireonly locally available information. We first demonstratethat the modelcanextract a sparse,distributed,hierarchicalrepresentationof globaldisparity from simplified random-dotstereograms.We then investigatesomeof the scalingpropertiesof the algorithm on this problemandfind that: (1) Increasingtheimagesizeleads to fasterandmorereliablelearning;(2) Increasingthedepthof thenetwork from oneto two hiddenlayersleadsto betterrepresentationsat thefirst hiddenlayer, and(3) Onceonepartof thenetwork hasdiscoveredhow to representdisparity, it “supervises”otherpartsof thenetwork, greatlyspeedingup their learning."
5bf65452ae566a052b00d919404f462470869600,"It is possible to combine multiple probabilistic models of the same data by multiplying the probabilities together and then renormalizing. This is a very efficient way to model high-dimensional data which simultaneously satisfies many different low dimensional constraints. Each individual expert model can focus on giving high probability to data vectors that satisfy just one of the constraints. Data vectors that satisfy this one constraint but violate other constraints will be ruled out by their low probability under the other expert models. Training a product of models appears difficult because, in addition to maximizing the probabilities that the individual models assign to the observed data, it is necessary to make the models disagree on unobserved regions of the data space. However, if the individual models are tractable there is a fairly efficient way to train a product of models. This training algorithm suggests a biologically plausible way of learning neural population codes."
6075b4d1e7adb57f60e025a3f98b592064b27403,
f18e8dd77c7fb1e39a2f8fe3d6a56d990321bec6,
65f726b5d03fff0f5dbda1adbd88ca9aac5dac1e,"A persistent worry with computational models of unsupervised learning is that learning will become more difficult as the problem is scaled. We examine this issue in the context of a novel hierarchical, generative model that can be viewed as a nonlinear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We first demonstrate that the model can extract a sparse, distributed, hierarchical representation of global disparity from simplified random-dot stereograms. We then investigate some of the scaling properties of the algorithm on this problem and find that: 1) increasing the image size leads to faster and more reliable learning; 2) increasing the depth of the network from one to two hidden layers leads to better representations at the first hidden layer; and 3) once one part of the network has discovered how to represent disparity, it ""supervises"" other parts of the network, greatly speeding up their learning."
763aa50583ed047528ba4ef471d72bfbe34471e6,"We view perceptual tasks such as vision and speech recognition as inference problems where the goal is to estimate the posterior distribution over latent variables (e.g., depth in stereo vision) given the sensory input. The recent flurry of research in independent component analysis exemplifies the importance of inferring the continuous-valued latent variables of input data. The latent variables found by this method are linearly related to the input, but perception requires nonlinear inferences such as classification and depth estimation. In this article, we present a unifying framework for stochastic neural networks with nonlinear latent variables. Nonlinear units are obtained by passing the outputs of linear gaussian units through various nonlinearities. We present a general variational method that maximizes a lower bound on the likelihood of a training set and give results on two visual feature extraction problems. We also show how the variational method can be used for pattern classification and compare the performance of these nonlinear networks with other methods on the problem of handwritten digit recognition."
8b7a634f347eb9f90170536ad1982ccc79aa20a1,"Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can be computationally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient ""NeuroAnimator"" that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physics-based models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. We demonstrate NeuroAnimators for a variety of physics-based models. By exploiting the network structure of the NeuroAnimator, we also introduce a remarkably fast algorithm for learning controllers that enables either complex physics-based models or their neural network emulators to synthesize motions satisfying prescribed animation goals."
ba7d43e388b66755da54c7afaadb41c97aa7cc31,"This paper describes a practical application of a mixture of factor analyzers (MFA) to pattern recognition. The MFA extracts locally linear manifolds underlying given high dimensional data. In this respect, the NFA-based approach is similar to the conventional subspace methods that approximate the data space with low dimensional linear subspaces. However, the MFA-based classifier, unlike the conventional subspace methods, can perform classification based on the Bayes decision rule due to its probabilistic formulation. Experimental results show that the MFA-based approach can obtain better classification performance than the conventional subspace methods."
d62bcde418144411068d5b09952090962fbc05f6,
f23b4404ce10cb10047becd296fd504f70957c6d,
0226787e45577a115d4999eab26fde43baebbe49,"We describe a method for incrementally constructing a hierarchical generative model of an ensemble of binary data vectors. The model is composed of stochastic, binary, logistic units. Hidden units are added to the model one at a time with the goal of minimizing the information required to describe the data vectors using the model. In addition to the top-down generative weights that define the model, there are bottom-up recognition weights that determine the binary states of the hidden units given a data vector. Even though the stochastic generative model can produce each data vector in many ways, the recognition model is forced to pick just one of these ways. The recognition model therefore underestimates the ability of the generative model to predict the data, but this underestimation greatly simplifies the process of searching for the generative and recognition weights of a new hidden unit."
0bca8bb2393eea5340d4d546b75761ad0e588995,
2925f8c37906a4c8f23be600afcffbf60bde605b,"The EM algorithm for Gaussian mixture models often gets caught in local maxima of the likelihood which involve having too many Gaussians in one part of the space and too few in another, widely separated part of the space. We present a new EM algorithm which performs split and merge operations on the Gaussians to escape from these configurations. This algorithm uses two novel criteria for efficiently selecting the split and merge candidates. Experimental results on synthetic and real data show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data."
6a45ae25f0c78560414b1374ca181fc3674621c7,"During the course of this investigation, many observations were made about the estimation of the in vivo power spectrum as it applies to estimating the characteristic size of the tissue microstructure. In this final chapter, the conclusions derived from all of the observations are summarized. Then, future goals for extending the work and applying the conclusions are discussed. 9.1 Conclusions from Current Investigation For the estimation of the characteristic size of the tissue microstructure, the in vivo power spectrum can be determined to sufficient accuracy by estimating the total attenuation along the propagation path over the frequency range used to estimate the scatterer size. Hence, most of the work in this investigation attempted to solve for the scatterer size and total attenuation simultaneously. However, before solving for these two parameters, the effects of focusing on the estimation of the scatterer size were considered while assuming that the attenuation was known so that the results of the investigation could later be applied to clinically relevant fields. By assuming that the velocity potential fields near the focus could be modeled as a three-dimensional Gaussian distribution, focusing could be corrected by using a new generalized attenuation-compensation function that accounted for focusing, windowing, and attenuation. The new generalized attenuation-compensation function was then compared to the traditional attenuation-compensation functions that neglected focusing along the beam axis using computer simulations and phantom experiments. The generalized attenuation-compensation function yielded improvements in size estimation accuracy as high as 100% over the traditional attenuation-compensation functions. However, errors from the 155"
77ec09a8760f4ee2ac85a891a4d5054541c7ae06,"We describe a method for incrementally constructing a hierarchical generative model of an ensemble of binary data vectors. The model is composed of stochastic, binary, logistic units. Hidden units are added to the model one at a time with the goal of minimizing the information required to describe the data vectors using the model. In addition to the top-down generative weights that define the model, there are bottom-up recognition weights that determine the binary states of the hidden units given a data vector. Even though the stochastic generative model can produce each data vector in many ways, the recognition model is forced to pick just one of these ways. The recognition model therefore underestimates the ability of the generative model to predict the data, but this underestimation greatly simplifies the process of searching for the generative and recognition weights of a new hidden unit."
7bfecbb87f447ef9706d359f8220ee7194ebbf0e,"Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can be computationally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient ""NeuroAnimator"" that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physics-based models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. We demonstrate NeuroAnimators for a variety of physics-based models."
9e0020d2cc7d8255661063ea5fb87f2000df1b21,"Animation through the numerical simulation of physicsbased graphics models offers unsurpassed realism, but it can be computationally demanding. Likewise, the search for controllers that enable physics-based models to produce desired animations usually entails formidable computational cost. This paper demonstrates the possibility of replacing the numerical simulation and control of dynamic models with a dramatically more efficient alternative. In particular, we propose the NeuroAnimator, a novel approach to creating physically realistic animation that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physicsbased models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. Furthermore, by exploiting the network structure of the NeuroAnimator, we introduce a fast algorithm for learning controllers that enables either physics-based models or their neural network emulators to synthesize motions satisfying prescribed animation goals. We demonstrate NeuroAnimators for a variety of physics-based models. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Animation; I.6.8 [Simulation and Modeling]: Types of Simulation—Animation"
9f87a11a523e4680e61966e36ea2eac516096f23,
a88f04ee3dd09c2a753685cb70f7e43d478aab82,"We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations, we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems."
cd9137285899edc9805dcc51e1941d0de6055a4b,
7c0ade464620ed604ff58d9ad64bcfa1bc37a86f,
d30bce17c136bd620095217647863718575ea0c9,"We apply a battery of modern, adaptive non-linear learning methods to a large real database of cardiac patient data. We use each method to predict 30 day mortality from a large number of potential risk factors, and we compare their performances. We find that none of the methods could outperform a relatively simple logistic regression model previously developed for this problem."
1cb7b19e8b4e90c4b1328f4487d31d8afdd53b89,"Glove-Talk II is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-Talk II uses several input devices, a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. With Glove-Talk II, the subject can speak slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer."
279bf08d1795c6d10c0232a809341f0da2fc06ed,"We describe the \wake-sleep"" algorithm that allows a multilayer, unsupervised, neural network to build a hierarchy of representations of sensory input. The network has bottom-up \recognition"" connections that are used to convert sensory input into underlying representations. Unlike most arti cial neural networks, it also has top-down \generative"" connections that can be used to reconstruct the sensory input from the representations. In the \wake"" phase of the learning algorithm, the network is driven by the bottom-up recognition connections and the top-down generative connections are trained to be better at reconstructing the sensory input from the representation chosen by the recognition process. In the \sleep"" phase, the network is driven top-down by the generative connections to produce a fantasized representation and a fantasized sensory input. The recognition connections are then trained to be better at recovering the fantasized representation from the fantasized sensory input. In both phases, the synaptic learning rule is simple and local. The combined e ect of the two phases is to create representations of the sensory input that are e cient in the following sense: On average, it takes more bits to describe each sensory input vector directly than to rst describe the representation of the sensory input chosen by the recognition process and then describe the di erence between the sensory input and its reconstruction from the chosen representation."
42ef72fe3707d67c7696435edafc035e54ac03aa,"Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer."
628b80ac7952a67155d62e10dc2854ac8c04a6e4,"We discuss Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster, Laird, and Rubin (1977)."
62f4d89a3c1441b47170c7e1380137fb388d0799,"This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed."
71c7de60ab2b28921dfe5e4bd11b0437cf75fab1,
775bddc3476a25cc7da5bfcf2083fca94983661a,"We show how a neural network can be used to allow a mobile robot to derive an accurate estimate of its location from noisy sonar sensors and noisy motion information. The robot's model of its location is in the form of a probability distribution across a grid of possible locations. This distribution is updated using both the motion information and the predictions of a neural network that maps locations into likelihood distributions across possible sonar readings. By predicting sonar readings from locations, rather than vice versa, the robot can handle the very nongaussian noise in the sonar sensors. By using the constraint provided by the noisy motion information, the robot can use previous readings to improve its estimate of its current location. By treating the resulting estimates as if they were correct, the robot can learn the relationship between location and sonar readings without requiring an external supervision signal that specifies the actual location of the robot. It can learn to locate itself in a new environment with almost no supervision, and it can maintain its location ability even when the environment is nonstationary."
7a64be9fb3f1bce1dada1c32bfac42164cf3cdec,"We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topographically organised local feature detectors."
8b994ced581a7c42e316cec2716a6876836bc755,"The protein structure prediction problem (PSP) is one of the central problems in molecular and structural biology. A computational method that could produce a correct detailed three-dimensional structural model for a protein, given its linear sequence of amino acids, would greatly accelerate progress in the biomedical sciences and industries. This thesis presents PSP as a combinatorial optimization problem, the most straightforward formulations of which require search of an exponentially-large conformation space and are known to be NP-Hard. This otherwise intractable search can in practice be reduced or eliminated through the discovery and use of motifs. Motifs are abstractions of observed patterns that encode structurally important relationships among constituent parts of a complex object like a protein tertiary structure. Motif discovery is accomplished by particular combinatorial search and statistical estimation methods. 
This thesis explores in detail two particular motif discovery subproblems, and discusses how their solutions can be applied to the overall structure prediction problem: (1) For a complex multi-stage prediction task, what makes a good intermediate representation language? We address this question by presenting and analyzing methods for the discovery of protein secondary structure classes that are more predictable from amino acid sequence than the standard classes of $\alpha$-helix, $\beta$-sheet, and ""random coil"". (2) Given a database of M objects, each characterized by values $a\sb{ij}\in {\cal A}\sb{j}$ for each of N discrete variables $\{c\sb{j}\}\sbsp{j=1}{N},$ return the list of ""most interesting"" higher-order features $\gamma\sb{l},$ i.e., sets of $k\sb{l}$ variables with highest estimated correlation, for any $2 \le k\sb{l} \le N$. In the PSP context, the problem is the detection of correlations between amino acid residues in an aligned set of evolutionarily-related protein sequences. We present and analyze a fast procedure, based on multinomial sampling and a novel coding scheme, that avoids the exhaustive search, prior limits on the order k, and exponentially large parameter space of other methods. 
The focus of this thesis is PSP, but the techniques and analysis are also aimed at wider application to other hard, multi-stage prediction problems."
a3dde51dbd39dda5bc22df0d8163608d3898823b,"In this paper, we introduce a new algorithm calledbits-back coding' that makes stochastic source codes ef""cient. For a given one-to-many source code, we show that this algorithm can actually be more ef""cient than the algorithm that always picks the shortest codeword. Optimal ef""ciency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parameters— maximum-likelihood estimation—actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum-likelihood estimation—the generalized expectation-maximization algorithm—minimizes the bits-back coding cost. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding. We illustrate the performance of bits-back coding using non-synthetic data with a binary Bayesian network source model that produces 2 60 possible codewords for each input symbol. The rate for bits-back coding is nearly one half of that obtained by picking the shortest"
a4a34496869a2ef6dfd2ddb880ae5b5dc9cdf60f,"Pattern classification, data compression, and channel coding are tasks that usually must deal with complex but structured natural or artificial systems. Patterns that we wish to classify are a consequence of a causal physical process. Images that we wish to compress are also a consequence of a causal physical process. Noisy outputs from a telephone line are corrupted versions of a signal produced by a structured man-made telephone modem. Not only are these tasks characterized by complex structure, but they also contain random elements. Graphical models such as Bayesian networks provide a way to describe the relationships between random variables in a stochastic system. 
In this thesis, I use Bayesian networks as an overarching framework to describe and solve problems in the areas of pattern classification, data compression, and channel coding. Results on the classification of handwritten digits show that Bayesian network pattern classifiers outperform other standard methods, such as the k-nearest neighbor method. When Bayesian networks are used as source models for data compression, an exponentially large number of codewords are associated with each input pattern. It turns out that the code can still be used efficiently, if a new technique called ""bits-back coding"" is used. Several new error-correcting decoding algorithms are instances of ""probability propagation"" in various Bayesian networks. These new schemes are rapidly closing the gap between the performances of practical channel coding systems and Shannon's 50-year-old channel coding limit. The Bayesian network framework exposes the similarities between these codes and leads the way to a new class of ""trellis-constraint codes"" which also operate close to Shannon's limit."
b3961e74b20cdc9fac3e36a0213ad22d89b90aea,"Table 2. Results of the best runs for the Geneeproblem. CENNBPROP yields both the lowest training and the lowest testing error. Training set Testing set Algorithm SSE Class. rate SSE Class. achieved the lowest SSE although RPROP normally outperforms BPROP. F or the Thyroiddproblem CENNBPROP proved to be comparable with standard RPROP. CENNRPROP always yields the best results SSE=0.0 and beats the standard RPROP which still left some residual errors. In the beginning of training, CENNBPROP has worked best for both benchmark problems. This can be explained by the fact that RPROP needs some time to adapt its step widths. The CENNoptimization improves BPROP training with respect to convergence speed and the nal quality of the trained networks. Especially, t h i s is exploited in the case of the Thyroiddproblem. Since CENNBPROP updates the weights after each pattern presentation online training, it should be well suited for applications with feedback, e.g. neural controllers. In summary, CENNoptimization has proven to outperform all the investigated BP algorithms and we guess that it is the fastest training algorithm at the time being. Of course, this claim needs to be veriied by using much more benchmarks and comparisons to other training algorithms. A direct adaptive method for faster backpropagation learning: The rprop algorithm. Table 1. Results of the best runs for the Thyroiddproblem. CENNRPROP yields both the lowest training error and the lowest testing error. training set testing set Algorithmus SSE Class. rate SSE Class. rate BPROP 246.0 For each problem 10 networks were created which are then used for the four different training algorithms. Before training, the networks were initialized by random numbers drawn from a normal distribution with zero mean and a standard deviation of 0.01. Thus, all algorithms had equal initial conditions. BPROP and CENNBPROP used a learning rate of 0.01 and a momentum of 0.9. RPROP and CENNRPROP used an initial step width of 0.001, uppdown factors of 0.551.2 and a minimallmaximal step width of 10 ,6 50.0, repectively. The results are displayed in gure 1 for the Thyroiddproblem and in gure 2 for the Geneeproblem. All the 10 runs were plotted simultaneously. Note that the standard and the CENNoptimized versions can only be compared if the SSE is plotted over the numberofepochs. In table 1 and in table 2, the results of the best runs are summarized. The generalization performance is estimated by the error with respect to …"
badb0c63bb8ac4a764230fc8aed69d8353cac254,"To read a hand-written digit string, it is helpful to segment the image into separate digits. Bottom-up segmentation heuristics often fail when neighboring digits overlap substantially. We describe a system that has a stochastic generative model of each digit class and we show that this is the only knowledge required for segmentation. The system uses Gibbs sampling to construct a perceptual interpretation of a digit string and segmentation arises naturally from the \explaining away"" e ects that occur during Bayesian inference. By using conditional mixtures of factor analyzers, it is possible to extract an explicit, compact representation of the instantiation parameters that describe the pose of each digit. These instantiation parameters can then be used as the inputs to a higher level system that models the relationships between digits. The same technique could be used to model individual digits as redundancies between the instantiation parameters of their parts."
f4af8f352e1ca608db4d7de5b124785219d62745,"The standard GTM (generative topographic mapping) algorithm assumes that the data on which it is trained consists of independent, identically distributed (i.i.d.) vectors. For time series, however, the i.i.d. assumption is a poor approximation. In this paper we show how the GTM algorithm can be extended to model time series by incorporating it as the emission density in a hidden Markov model. Since GTM has discrete hidden states we are able to find a tractable EM algorithm, based on the forward-backward algorithm, to train the model. We illustrate the performance of GTM through time using flight recorder data from a helicopter."
09ef86868035bbfd4803a9e1c98640804bf8f4a4,"Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing di erent local factor models in di erent regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation{Maximization algorithm for tting the parameters of this mixture of factor analyzers."
2e3170f91e1d8037f8ba03286fa5ddd347a0b88e,"Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems. In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Sto er, 1982). We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models. Introduction The goal of this note is to introduce the EM algorithm for estimating the parameters of linear dynamical systems (LDS). Such linear systems can be used both for supervised and unsupervised modeling of time series. We rst describe the model and then brie y point out its relation to factor analysis and other data modeling techniques. The Model Linear time-invariant dynamical systems, also known as linear Gaussian state-space models, can be described by the following two equations: xt+1 = Axt +wt (1) yt = Cxt+ vt: (2) Time is indexed by the discrete index t. The output yt is a linear function of the state, xt, and the state at one time step depends linearly on the previous state. Both state and output noise, wt and vt, are zero-mean normally distributed random variables with covariance matrices Q and R, respectively. Only the output of the system is observed, the state and all the noise variables are hidden. Rather than regarding the state as a deterministic value corrupted by random noise, we combine the state variable and the state noise variable into a single Gaussian random"
4c678012d28757921853ef98dbbbe36a1a804e69,"This manual describes the preliminary release of the DELVE environment. Some features described here have not yet implemented, as noted. Support for regression tasks is presently somewhat more developed than that for classiication tasks. We recommend that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software. We hope that you will send us reports of any problems you encounter, as well as any other comments you may have on the software or manual, at the e-mail address below. Please mention the version number of the manual and/or the software with any comments you send. All Rights Reserved Permission to use, copy, modify, and distribute this software and its documentation for non-commercial purposes only is hereby granted without fee, provided that the above copyright notice appears in all copies and that both the copyright notice and this permission notice appear in supporting documentation, and that the name of The University of Toronto not be used in advertising or publicity pertaining to distribution of the software without speciic, written prior permission. The University of Toronto makes no representations about the suitability of this software for any purpose. It is provided \as is"" without express or implied warranty. The University of Toronto disclaims all warranties with regard to this software, including all implied warranties of merchantability and tness. In no event shall the University of Toronto be liable for any special, indirect or consequential damages or any damages whatsoever resulting from loss of use, data or proots, whether in an action of contract, negligence or other tortious action, arising out of or in connection with the use or performance of this software. If you publish results obtained using DELVE, please cite this manual, and mention the version number of the software that you used."
56efc84e0858f1e0a7cf052e5c4275d4c46c21c2,"We describe a method of recognizing handwritten digits by fitting generative models that are built from deformable B-splines with Gaussian ""ink generators"" spaced along the length of the spline. The splines are adjusted using a novel elastic matching procedure based on the expectation maximization algorithm that maximizes the likelihood of the model generating the data. This approach has many advantages: 1) the system not only produces a classification of the digit but also a rich description of the instantiation parameters which can yield information such as the writing style; 2) the generative models can perform recognition driven segmentation; 3) the method involves a relatively small number of parameters and hence training is relatively easy and fast; and 4) unlike many other recognition schemes, it does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations and a limited degree of image rotation. We have demonstrated that our method of fitting models to images does not get trapped in poor local minima. The main disadvantage of the method is that it requires much more computation than more standard OCR techniques."
6dad26916ac88188dae82f94f2995c80c19a9589,
76df8ca4f6fd7eee7879fe1b01dc6b5953c88b10,"We describe a method of recognizing handwritten digits by fitting generative models that are built from deformable B- splines with Gaussian ""ink generators"" spaced along the length of the spline. The splines are adjusted using a novel elastic matching procedure based on the Expectation Maximization (EM) algorithm that maximizes the likelihood of the model generating the data. This approach has many advantages. 1) After identifying the model most likely to have generated the data, the system not only produces a classification of the digit but also a rich description of the instantiation parameters which can yield information such as the writing style. 2) During the process of explaining the image, generative models can perform recognition driven segmentation. 3) The method involves a relatively small number of parameters and hence training is relatively easy and fast. 4) Unlike many other recognition schemes, it does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations and a limited degree of image rotation. We have demonstrated our method of fitting models to images does not get trapped in poor local minima. The main disadvantage of the method is it requires much more computation than more standard OCR techniques. Index Terms-Deformable model, elastic net, optical character recognition, generative model, probabilistic model, mixture model"
7ea37a9b06cca8d4a667aeb894942c27ecdc2aca,"We introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol. It may seem that the most sensible codeword to use in this case is the shortest one. However, in the proposed free energy approach, random codeword selection yields an effective codeword length that can be less than the shortest codeword length. If the random choices are Boltzmann distributed, the effective length is optimal for the given source code. The expectation-maximization parameter estimation algorithms minimize this effective codeword length. We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method."
ad4963fdb01b7e55fc08fe8914a6dab15205ba7c,
f9197ff9fdabd2b78bfe0602365011c6699b0d66,"The assumption that acquired character istics are not in­ herited is ofte n taken to imply t hat t he adaptations t hat an organism learns dur ing its lifeti me cannot guide t he course of evolut ion . This infere nce is incor rec t (2). Learni ng alt ers the shape of t he search space in which evolu tio n operates and thereby pro vides good evolut ion ar y paths towa rds sets of co-adapted alleles. We demonst r at e t hat th is effect allows learning organisms to evolve much faster than their 000 ­ learning equivalents, even though the characteris tics acquired by t he phenotype are not communicated to the genotype."
280ccfcfec38b3c38372466fb9e34333d921715a,"Glove-TaikII is a system which translates hand gestures-· to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary, multiple languages in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TaikII uses several input devices (including a Cyberglove, a ContactGlove, a polhemus sensor, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gestureto-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained for about 100 hours to speak intelligibly with Glove-TalkII. He passed through eight distinct stages while learning to speak. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations."
605402e235bd62437baf3c9ebefe77fb4d92ee95,"Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
7f1c69f4dcd7ceea6363367822ed845015e5836a,"Supervised learning procedures for neural networks have recently met with considerable success in learning di cult mappings. So far, however, they have been limited by their poor scaling behaviour, particularly for networks with many hidden layers. A promising alternative is to develop unsupervised learning algorithms by de ning objective functions that characterize the quality of an internal representation without requiring knowledge of the desired outputs of the system. Our major goal is to build self-organizing network modules which capture important regularities in the environment in a simple form. A layered hierarchy of such modules should be able to learn in a time roughly linear in the number of layers. We propose that a good objective for perceptual learning is to extract higher-order features that exhibit simple coherence across time or space. This can be done by transforming the input representation into an underlying representation in which the mutual information between adjacent patches of the input can be expressed in a simple way. We have applied this basic idea to develop several interesting learning algorithms for discovering spatially coherent features in images. Our simulations show that a network can discover depth of surfaces when trained on binary random dot stereograms with discrete global shifts, as well as on real-valued stereograms of surfaces with continuously varying disparities. Once a module of depth-tuned units has developed, we show that units in a higher layer can discover a simple form of surface interpolation of curved surfaces, by learning to predict the depth of one image region based on depth measurements in surrounding regions."
bbb7229bfe2b9995ee54272217156ed91494b91d,"Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a computationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points, there is one hyperplane that is orthogonal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods, particularly when the training sets were small."
cbb0362cbfef094dbed0907329e6057dc5d09714,"The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connections in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit."
0067ef94cbf90021ad27784ce97d8a2a38f643e0,"Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a CyberGlove, a ContactGlove, a 3- space tracker, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations."
1335b3997c95c7543691770651564c91e2517673,"In this thesis I develop a method for recognizing isolated handprinted digits using trainable deformable models. Each digit is modelled by a cubic B-spline whose basic shape is defined by the ""home"" positions of the control points. A Gaussian distribution over displacements of the control points away from their home locations defines a probability distribution over shapes. The quality of the match of a spline model to an image is calculated as the likelihood of the data under a mixture of Gaussian ""ink generators"" placed along the length of the spline. Each spline model is adjusted to minimize an energy function that includes both the deformation energy of the model and the likelihood of the data, using a elastic matching procedure which is a generalization of the Expectation Maximization (EM) algorithm. I show that the matching procedure can be significantly speeded up by using a neural net to provide better starting points for the search. 
The use of deformable models has a number of advantages. (1) After identifying the model most likely to have generated the data, the system not only produces a classification of the digit but also a rich description of the instantiation parameters. I have shown that these can be used to detect writing style consistency within a string of digits. (2) During the process of explaining the image, generative models can perform recognition-driven segmentation. (3) Unlike many other recognition schemes the method does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations and a limited degree of image rotation. The main disadvantage of the method is it requires much more computation than more standard optical character recognition techniques."
18abbcafeda842a4d72e482771cb27f266894f14,"Deformable models are an attractive approach to recognizing nonrigid objects which have considerable within class variability. However, there are severe search problems associated with fitting the models to data. We show that by using neural networks to provide better starting points, the search time can be significantly reduced. The method is demonstrated on a character recognition task."
9dea20c1e5bbb1f543ff08113ffde5380c679f1f,"We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance."
aad5b4340e4b3b23ad8d292bf9ed8e7f60c53159,"Distributed representations are attractive for a number of reasons. They offer the possibility of representing concepts in a continuous space, they degrade gracefully with noise, and they can be processed in a parallel network of simple processing elements. However, the problem of representing nested structure in distributed representations has been for some time a prominent concern of both proponents and critics of connectionism (Fodor and Pylyshyn 1988; Smolensky 1990; Hinton 1990). The lack of connectionist representations for complex structure has held back progress in tackling higher-level cognitive tasks such as language understanding and reasoning. 
In this thesis I review connectionist representations and propose a method for the distributed representation of nested structure, which I call ""Holographic Reduced Representations"" (HRRs). HRRs provide an implementation of Hinton's (1990) ""reduced descriptions"". HRRs use circular convolution to associate atomic items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, and predicates can be represented in a fixed-width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties. 
Circular convolution, which is the basic associative operator for HRRs, can be built into a recurrent neural network. The network can store and produce sequences. I show that neural network learning techniques can be used with circular convolution in order to learn representations for items and sequences. 
One of the attractions of connectionist representations of compositional structures is the possibility of computing without decomposing structures. I show that it is possible to use dot-product comparisons of HRRs for nested structures to estimate the analogical similarity of the structures. This demonstrates how the surface form of connectionist representations can reflect underlying structural similarity and alignment."
bbf4924123e798f32c028d184dd8ee051bef55a3,"Deformable models are an attractive way for characterizing handwritten digits since they have relatively few parameters, are able to capture many topological variations, and incorporate much prior knowledge. We have described a system [8] that uses learned digit models consisting of splines whose shape is governed by a small number of control points. Images can be classi ed by separately tting each digit model to the image, and using a simple neural network to decide which model ts best. We use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The use of multiple models for each digit can characterize the population of handwritten digits better. We show how multiple models may be used without increasing the time required for elastic matching."
c8eb7f54bd9cec4e2d8c866d98c76a875374b594,We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models--trained by either EM or gradient ascent--there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers.
24049fe69136657a5e9dfa05bb079b8e29df8582,
ad08906089b4a49fb3d1604512cdae6abb1f538e,
25c9f33aceac6dcff357727cbe2faf145b01d13c,"Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights."
2993bed6663f53a12584ea293bf8d487a91cd25a,"The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center ofthis bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs. Most existing unsupervised learning algorithms can be understood using the Minimum Description Length (MDL) principle (Rissanen, 1989). Given an ensemble of input vectors, the aim of the learning algorithm is to find a method of coding each input vector that minimizes the total cost, in bits, of communicating the input vectors to a receiver. There are three terms in the total description length: • The code-cost is the number of bits required to communicate the code that the algorithm assigns to each input vector."
3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f,"An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes."
53ad5627f0bacb4c1de4aa374adbaecf40336b07,"The minimum description length (MDL) principle can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes."
703b3e9dc9ad014cb1baa840f5a03b33c67021d8,"To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single ;speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1% of the time, and no word is produced about 5% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user."
da11a50037b266fe6b1929b76322395716ec2029,"An adaptation algorithm for equalizers operating on very distorted channels is presented. The algorithm is based on the idea of adjusting the equalizer tap gains to maximize the likelihood that the equalizer outputs would be generated by a mixture of two Gaussians with known means. The decision-directed least-mean-square algorithm is shown to be an approximation to maximizing the likelihood that the equalizer outputs come from such an independently and identically distributed source. The algorithm is developed in the context of a binary pulse-amplitude-modulation channel, and simulations demonstrate that the algorithm converges in channels for which the decision-directed LMS algorithms does not converge. >"
f065b631b2df7964d308c8a649fb05e9c6c91f67,"We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton 1992b). In this paper, we propose two new models that handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities."
0f8d16c6c1ee2bc20c9530a4d9ce33283bb2fedf,
1678bd32846b1aded5b1e80a617170812e80f562,"One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. 
 
We illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map."
e08d090d1e586610d636a46004876e9f3ded8209,
2c85b7fe70dda0adbbd7630e2a341a904c74fbd2,
88b6feed9cb70dcca854ff50c4eba478f732d59a,"The computer metaphor for the mind or brain has long outlived its usefulness, being based on Cartesian ideas. Connectionism has not broken free from this metaphor, and this has stunted the directions connectionist research has taken. The subordinate role of timing in computations has resulted in networks with real-value timelags on signals passing between nodes being ignored. The notion of representation in connectionism is generally confused; this can be clariied when at all times it is made explicit who or what Q and S are in the formula \P is used by Q to represent R to S"". Frequently they may be layers or modules within a network, but the typical confusion is symptomatic of the computer metaphor which in practice favours feedforward and militates against arbitrarily connected networks. Rejecting this metaphor, an alternative paradigm is suggested of a brain as a complex dynamical system; investigating the dynamics of arbitrarily connected networks with real-valued timelags, speciied so as to produce appropriate behaviour when they act as a nervous system for an organism or machine in continuous longterm interaction with its environment. The practical diierences a change of metaphor makes are pointed out, and possible techniques for pursuing this line are indicated."
c6d749df7813bf16d11374b8ddcd3e640210dd36,
de75e4e15e22d4376300e5c968e2db44be29ac9e,"One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms."
0a41ca65a80b5644d23649043f2f625b4002a225,"Geoffrey E. Hinton Department of Computer Science . U ni versi ty of Toran to Toronto, Canada M5S lA4 One way of simplifying neural networks so they generalize better is to add an extra t.erm 10 the error fUll c tion that will penalize complexit.y. \Ve propose a new penalt.y t.erm in which the dist rihution of weight values is modelled as a mixture of multiple gaussians . C nder this model, a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values . We allow the parameters of the mixture model to adapt at t.he same time as t.he network learns. Simulations demonstrate that this complexity term is more effective than previous complexity terms."
2ea4a33a468958de14303daaaba2349d0ed07b73,"The Boltzmann machine learning procedure has been successfully applied in deterministic networks of analog units that use a mean field approximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of deterministic Boltzmann machine (DBM) learns much faster than the equivalent stochastic Boltzmann machine (SBM), but since the learning procedure for DBM's is only based on an analogy with SBM's, there is no existing proof that it performs gradient descent in any function, and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described, and this makes the DBM more biologically plausible than back-propagation (Werbos 1974; Parker 1985; Rumelhart et al. 1986)."
15034107f195f625922881ef197515a7997b4c0d,"Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit, the control points have preferred ""home"" locations, and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points."
4012505f97e9e14235fef519201cb9a282863109,"In the unsupervised learning paradigm, a network of neuron-like units is presented with an ensemble of input patterns from a structured environment, such as the visual world, and learns to represent the regularities in that input. The major goal in developing unsupervised learning algorithms is to find objective functions that characterize the quality of the network's representation without explicitly specifying the desired outputs of any of the units. The sort of objective functions considered cause a unit to become tuned to spatially coherent features of visual images (such as texture, depth, shading, and surface orientation), by learning to predict the outputs of other units which have spatially adjacent receptive fields. Simulations show that using an information-theoretic algorithm called IMAX, a network can be trained to represent depth by observing random dot stereograms of surfaces with continuously varying disparities. Once a layer of depth-tuned units has developed, subsequent layers are trained to perform surface interpolation of curved surfaces, by learning to predict the depth of one image region based on depth measurements in surrounding regions. An extension of the basic model allows a population of competing neurons to learn a distributed code for disparity, which naturally gives rise to a representation of discontinuities."
4725bea4175f9a3912188aca3c4f25c66469fea1,
4a3650f84b355a6e820174081c4d6d5274af46e4,"faster than a non-decomposed network when the number of weights in the two networks is approximately equal remains to be tested. Second, we performed a more extensive search in and space than [Kamm and Singhal, 1990], resulting in generally higher learning rates and training times of only a few epochs. The range of learning rates explored in the current study included higher rates than those explored in [Kamm and Singhal, 1990], so it is dicult to determine whether the learning speed-up (in terms of arithmetic operations) is attributable to problem decomposition or learning rate. To address this issue, we also trained monolithic 125ms networks for approximately the same number of operations as the decomposed net, using a) the learning parameters used for the 125ms subnetwork and b) the original parameters for the monolithic network. On the 200-sentence-test set, these networks had normalized AHAs of 47.3% and 46.3% and normalized AFAs of 5.9% and 5.1%, respectively. This performance is markedly poorer than performance of the decomposed network, supporting the hypothesis that the problem decomposition strategy was a primary contributor to the learning speed-up. The problem decomposition strategy facilitated the more thorough search for learning parameters, since it allowed the use of dierent parameters for training the dierent subnetworks. In many empirical studies utilizing the back-propagation algorithm, it is dicult to control for the and parameters. Often, they are held constant over all experimental conditions. However, since optimal parameter values are often problem-dependent, this constraint can result in misleading comparisons if and are chosen optimally for one network but not another. Our experience in this project supports the fact that the careful selection of network training parameters is critical for successful training (and performance). 6 Summary Problem decomposition was applied to AP-net, a neural network for mapping acoustic spectra to phoneme classes. AP-net was decomposed into subnetworks diering in both output units (assigning phoneme classes to subnetworks based on their durations) and input units (providing dierent input spans to the subnetworks). Subnetworks were trained individually and then combined into a larger network using \glue"" units and ne-tuning of the entire network. This problem decomposition strategy achieved comparable performance to that of a monolithic network, but required an order-of-magnitude fewer arithmetic operations during training. This result provides evidence for the general utility of this problem decomposition methodology for addressing the learning speed problem in back-propagation. Table 1 summarizes the conditions used to train …"
64ca2be80e62981c1ed1ee81efcbb9fb748a70be,"BoltzCONS - dynamic symbol structures in a connectionist network, D.S. Touretzky mapping part-whole hierarchies into connectionist networks, G. E. Hinton recursive distributed representations, J.B. Pollack mundane reasoning by settling on a plausible model, M. Derthick tensor product variable binding and the representation of symbolic structures in connectionist systems, P. Smolensky learning and applying contextual constraints in sentence comprehension, M.F. St. John and J.L. McClelland."
7720ba204e8c3df0399b0861d671a1ec2d865a30,"6 TRACE has been simulated with a variety of input equations with negligible difference in results. Activity in Cognitive Elements 47 sequence (and likewise for the relationship between the phase sequence and the phase cycle). Since Hebb considered the cell assembly to be the smallest unit of conscious thought, our simulation, which is oriented to perception, cognition, and memory for objects, would be more properly considered as at the phase sequence level. For the purposes of our model, however, these distinctions may not be of great importance. We assume that the basic functions characteristic of these networks are likely to be the same across the three different levels. 2 The embedding fields theory of Grossberg (1969,1971), in which the unit activation function has positive feedback and decay, provides a unique exception to this generalization. 3 We present here a system of difference equations. In fact, solutions to the corresponding system of differential equations were obtained by several approximation methods. These solutions were quite close to those obtained by employing Euler's method with a step size of 1. So, to simplify the exposition and reduce computational requirements, we present and simulate the difference equations. 4 In the remainder of the exposition we replace the functional notation X(t) with the simpler notation X. It should be understood that a variable X which appears in the right hand side of an equation is evaluated at time t. 5 I is an instantaneous function, not a state variable, so it is evaluated at time t+1 rather than at time t as are the state variables in these equations. Memory storage as a function of arousal and time with homogeneous and heterogeneous lists. Footnotes 1 We have adopted the expression ""cell assembly"" because of its common usage in descriptions of Hebb's work. Hebb's analysis of cognitive processes actually included three levels of organization: cell assemblies, phase sequences, and phase cycles. Cell assemblies served as elements in the higher level network that constituted the phase Pillsbury, W. B. (1913). ""Fluctuations of attention"" and the refractory period. (1956). Tests on a cell assembly theory of the action of the brain, using a large digital computer. Differential recall of paired associates as a function of arousal and concreteness-imagery levels. (1989). Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties."
7e5e08fc8cf395cdc313977b599386acb45917db,"This chapter contains sections titled: 1. Introduction, 2. Reasoning with Inconsistent Information, 3. μKLONE Language, 4. μKLONE Architecture, 5. Conclusion"
4ade4934db522fe6d634ff6f48887da46eedb4d1,
9438172bfbb74a6a4ea4242b180d4335bb1f18b7,"This chapter contains sections titled: 1. Introduction, 2. Connectionist Representation and Tensor Product Binding: Definition and Examples, 3. Tensor Product Representation: Properties, 4. Conclusion"
c07f22297f783475d799b1ad3d69f86f89cca1d3,"We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton, 1992). In this paper, we propose two new models which handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities."
c8d90974c3f3b40fa05e322df2905fc16204aa56,"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network."
ceb5324414c72957ed4a7f9c857f765b456a1e42,
e608ed5f23cfa03fc772752ce54c00a125720b8a,
fb31c68da5bb2e4d069858c2f9531dd236f8e046,"This chapter contains sections titled: 1. Introduction, 2. Direct and Indirect Representations, 3. Representing Linked Lists on an Associative Retrieval Machine, 4. Associative Stacks, 5. Associative Trees, 6. Connectionist Implementation, 7. Managing a Distributed Memory, 8. Discussion, 9. Conclusions"
1c3c5c56955b658a108731f5b7f65b76cd95f84e,"We compare the performance of the modular architecture, composed of competing expert networks, suggested by Jacobs, Jordan, Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex, but low-dimensional, vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task."
21f97b5742376a63f94f0844ca71e486ee5d8222,
3aaa051450b3db30f95918e7901db6f1aba62d41,
42f2dc77a6b3f2aa5909780682d02b4b511cda0e,"This thesis describes a frame system similar to KL-ONE, called micro-KLONE, for representing and reasoning about knowledge which may be incomplete or inconsistent. An unusual semantics appropriate to familiar situations is proposed. It is based on probabilistic sampling to find a single plausible model of the domain in order to answer a query. Correct answering of queries is intractable, so the implementation make two approximations in order to run quickly: (1) The underlying connectionist architecture is only large enough to represent partial models of the domain, and (2) the system is only allowed to search for a limited time, so it may not even find the best partial intepretation. Lacking a provably correct implementation, the usefulness of the system becomes an empirical question. The ""Ted Turner"" problem is presented as an example in which the system draws an interesting common sense conclusion to a counterfactual query."
4886fa9820ada07ca905cfd6a9ee973ce070b10b,
4f58283d9f5611759a9cdb00a927817ee195335c,
59fe3dc927489303e235d4baaecf5367a26c46a9,"Abstract : The simplicity and locality of the contrastive Hebb synapse (CHS) used in Boltzmann machine learning makes it an attractive model for real biological synapses. The slow learning exhibited by the stochastic Boltzmann machine can be greatly improved by using a mean field approximation and it has been shown (Hinton, 1989) that the CHS also performs steepest descent in these deterministic mean field networks. A major weakness of the learning procedure, from a biological perspective, is that the derivation assumes detailed symmetry of the connectivity. Using networks with purely asymmetric connectivity, we show that the CHS still works in practice provided the connectivity is grossly symmetrical so that if unit i sends a connection to unit j, there are numerous indirect feedback paths from j to i."
706ae842fb2107f5940cfa39682d52235d269eb7,"A number of researchers have begun exploring the use of massively parallel architectures in an attempt to get around the limitations of conventional symbol processing. Many of these parallel architectures are connectionist: The system's collection of permanent knowledge is stored as a pattern of connections or connection strengths among the processing elements, so the knowledge directly determines how the processing elements interact rather that sitting passively in a memory, waiting to be looked at by the CPU. Some connectionist schemes use formal, symbolic representations, while others use more analog approaches. Some even develop their own internal representations after seeing examples of the patterns they are to recognize or the relationships they are to store. Connectionism is somewhat controversial in the AI community. It is new, still unproven in large-scale practical applications, and very different in style from the traditional AI approach. The authors have only begun to explore the behavior and potential of connectionist networks. In this article, the authors describe some of the central issues and ideas of connectionism, and also some of the unsolved problems facing this approach. Part of the motivation for connectionist research is the possible similarity in function between connectionist networks and the neutral networksmore » of the human cortex, but they concentrate here on connectionism's potential as a practical technology for building intelligent systems.« less"
71dd4d477ca17b4db3b270d25225822ff3a41fac,
8209e1cd0ee97873345ce49584a825f8d603bcda,
852d3055ce81ef75b51dd9a406c0f5d056d270da,"Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network ""sees"" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size."
a99c9ce3cd25076edd3076729b3bce0696ec728e,"Abstract : Currently, one of the most powerful connectionist learning procedures is back-propagation which repeatedly adjusts the weights in a network so as to minimize a measure of the difference between the actual output vector of the network and a desired output vector given the current input vector. The simple weight adjusting rule is derived by propagating partial derivatives of the error backwards through the net using the chain rule. Experiments have shown that back-propagation has most of the properties desired by connectionists. As with any worthwhile learning rule, it can learn non-linear black box functions and make fine distinctions between input patterns in the presence of noise. Moreover, starting from random initial states, back-propagation networks can learn to use their hidden (intermediate layer) units to efficiently represent the structure that is inherent in their input data, often discovering intuitively pleasing features. The fact that back-propagation can discover features and distinguish between similar patterns in the presence of noise makes it a natural candidate as a speech recognition method. Another reason for expecting back-propagation to be good at speech is the success that hidden Markov models have enjoyed in speech can be useful when there is a rigorous automatic method for tuning its parameters."
af6759ecd0f6c8ba1eb7030894eff4c91a55778d,An algorithm that is widely used for adaptive equalization in current modems is the bootstrap or decision-directed version of the Widrow-Hoff rule. We show that this algorithm can be viewed as an unsupervised clustering algorithm in which the data points are transformed so that they form two clusters that are as tight as possible. The standard algorithm performs gradient ascent in a crude model of the log likelihood of generating the transformed data points from two gaussian distributions with fixed centers. Better convergence is achieved by using the exact gradient of the log likelihood.
5b00ab8cc5a56ee934618bd7555965185a7f83ec,"We describe a model that can recognize two-dimensional shapes in an unsegmented image, independent of their orientation, position, and scale. The model, called TRAFFIC, efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations, with increasing complexity of features at each successive layer, the network can recognize multiple objects in parallel. An implementation of TRAFFIC is described, along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner."
a275138be4d3133ba2e9d1a7a014e0afcbb5d546,"Contents: G.E. Hinton, J.A. Anderson, Introduction to the Updated Edition. D.E. Rumelhart, D.A. Norman, Introduction. J.A. Anderson, G.E. Hinton, Models of Information Processing in the Brain. J.A. Feldman, A Connectionist Model of Visual Memory. D. Willshaw, Holography, Associative Memory, and Inductive Generalization. T. Kohonen, E. Oja, P. Lehtio, Storage and Processing of Information in Distributed Associative Memory Systems. S.E. Fahlman, Representing Implicit Knowledge. G.E. Hinton, Implementing Semantic Networks in Parallel Hardware. T.J. Sejnowski, Skeleton Filters in the Brain. J.A. Anderson, M.C. Mozer, Categorization and Selective Neurons. S. Geman, Notes on a Self-Organizing Machine. R. Ratcliff, Parallel-Processing Mechanisms and Processing of Organized Information in Human Memory."
a57c6d627ffc667ae3547073876c35d6420accff,
b5affc896bcb291bca6e3ba60d34eeac28214e2e,"It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task."
cd62c9976534a6a2096a38244f6cbb03635a127e,"The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
e1d592e789bbec99188b12af20e58a75ec2987a0,"A new form of the deterministic Boltzmann machine (DBM) learning procedure is presented which can efficiently train network modules to discriminate between input vectors according to some criterion. The new technique directly utilizes the free energy of these ""mean field modules"" to represent the probability that the criterion is met, the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learning fails to extract the higher order feature of shift at a network bottleneck, combining the new mean field modules with the mutual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision."
19ec5c7be1fe5e7088f3a042d3160ede757f5902,"Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities affect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing unknown non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI."
834b3738673dacc767563c2714239852a8a6d4b4,"A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.<<ETX>>"
86f737b5d36933fcff42111c965344885afa1833,
a2641de9a59d4f176a6e088d79846576e5ff9513,"DCPS is a connectionist production system interpreter that uses distributed representations. As a connectionist model it consists of many simple, richly interconnected neuron-like computing units that cooperate to solve problems in parallel. One motivation for constructing DCPS was to demonstrate that connectionist models are capable of representing and using explicit rules. A second motivation was to show how “coarse coding” or “distributed representations” can be used to construct a working memory that requires far fewer units than the number of different facts that can potentially be stored. The simulation we present is intended as a detailed demonstration of the feasibility of certain ideas and should not be viewed as a full implementation of production systems. Our current model only has a few of the many interesting emergent properties that we eventually hope to demonstrate: It is damage-resistant, it performs matching and variable binding by massively parallel constraint satisfaction, and the capacity of its working memory is dependent on the similarity of the items being stored."
a5b6a187151bc2dd526f17ba296a18bbb679662e,
044496bf20b0e316a21c2ae721ee670fae0d432c,
da029646be2ad6447c1f5eca58a475894f176c3a,
104f0e3e005f8823f519467fbba0913271267d41,"I would like to acknowledge the help of Gul Agha, Jonathon Amsterdam, Peter de Jong, Carl Manning, Richard Waldinger, and Fanya Montalvo in improving the presentation. I owe a tremendous intellectual debt to my colleagues in the Message Passing Semantics Group, the Tremont Research Institute, and the MIT Artificial Intelligence Laboratory. Ken Kahn, Ueda, Keith Clark, and Takeuchi helped me greatly to understand (Flat) Concurrent Prolog, (Flat) Parlog, and (Flat) Guarded Horn Clauses. This paper describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Major support for the research reported in this paper was provided by the System Development Foundation. Major support for other related work in the Artificial Intelligence Laboratory is provided, in part, by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N0014-80-C-0505. I would like to thank Carl York, Charles Smith, and Patrick Winston for their support and encouragement."
17808fdaa56ca0227cea4990ecf36f5a1bc9bb58,"Hill climbing is used to maximize an information theoretic measure of the difference 
between the actual behavior of a unit and the behavior that would be predicted by a 
statistician who knew the first order statistics of the inputs but believed them to be 
independent. This causes the unit to detect higher order correlations among its inputs. 
Initial simulations are presented, and seem encouraging. We describe an extension of the 
basic idea which makes it resemble competitive learning and which causes members of a 
population of these units to differentiate, each extracting different structure from the input."
3e6bea2649298c68d17b9421fc7dd19eeacc935e,
52aa1d989cd00c96d37fec8617f85cfc4764a286,
c97be26d2d0eac3511c750a9cea8ee9a28e45fc2,
7e39621a532a9834245bc645e9fafee5b1aa47b9,"1. A detailed theory of cerebellar cortex is proposed whose consequence is that the cerebellum learns to perform motor skills. Two forms of inputoutput relation are described, both consistent with the cortical theory. One is suitable for learning movements (actions), and the other for learning to maintain posture and balance (maintenance reflexes). 2. It is known that the cells of the inferior olive and the cerebellar Purkinje cells have a special one-to-one relationship induced by the climbing fibre input. For learning actions, it is assumed that: (a) each olivary cell responds to a cerebral instruction for an elemental movement. Any action has a defining representation in ferms of elemental movements, and this representation has a neural expression as a sequence of firing patterns in the inferior olive; and (b) in the correct state of the nervous system, a Purkinje cell can initiate the elemental movement to which its corresponding olivary cell responds. 3. Whenever an olivary cell fires, it sends an impulse (via the climbing fibre input) to its corresponding Purkinje cell. This Purkinje cell is also exposed (via the mossy fibre input) to information about the context in which its olivary cell fired; and it is shown how, during rehearsal of an action, each Purkinje cell can learn to recognize such contexts. Later, when the action has been learnt, occurrence of the context alone is enough to fire the Purkinje cell, which then causes the next elemental movement. The action thus progresses as it did during rehearsal. 4. It is shown that an interpretation of cerebellar cortex as a structure which allows each Purkinje cell to learn a number of cQntexts is consistent both with the distributions of the various types of cell, and with their known excitatory or inhibitory natures. It is demonstrated that the mossy fibre-granule cell arrangement provides the required pattern discrimination capability. 5. The following predictions are made. (a) The synapses from parallel fibres to Purkinje cells are facilitated by the conjunction ofpresynaptic and climbing fibre (orpost-synaptic) activity."
6b59ebc8abee160f1cb34a992a6b07d1c9f7bafb,
7257eacd80458e70c74494eb1b6759b52ff21399,"Connectionist models usually have a single weight on each connection. Some interesting new properties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associations are ""blurred"" by subsequent learning, all the original associations can be ""deblurred"" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning."
97f7d20e1e82347d78cef335218692207b29d23f,"We describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a ""visible"" group to be represented by activity vectors in a ""hidden"" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the reconstruction error, and the learning procedure aims to minimize this error. The learning procedure has two passes. On the first pass, the original visible vector is passed around the loop, and on the second pass an average of the original vector and the reconstructed vector is passed around the loop. The learning procedure changes each weight by an amount proportional to the product of the ""presynaptic"" activity and the difference in the post-synaptic activity on the two passes. This procedure is much simpler to implement than methods like back-propagation. Simulations in simple networks show that it usually converges rapidly on a good set of codes, and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error."
03ec7af2824c331577cb73a2a92f4e7b9d728856,
e90273c4806b4d3352c1fda1fcb7cbce01594290,"Quantitative Biology, LV:899{910, 1990. [20] T. Poggio, S. Edelman, and M. Fahle. Learning of visual modules from examples: a framework for understanding adaptive visual performance. Computer Vision, Graphics, and Image Processing: Image Understanding, 56:22{30, 1992. [21] H. Putnam. Representation and reality. MIT Press, Cambridge, MA, 1988. [22] Z. Pylyshyn. What the mind's eye tells the mind's brain: a critique of mental imagery. Psychological Bulletin, 80:1{24, 1973. [23] E. T. Rolls. Neural organization of higher visual functions. Current Opinion in Neurobiology, 1:274{278, 1991. [24] C. D. Salzman, K. H. Britten, and W. T. Newsome. Cortical microstimulation in uences perceptual judgements of motion direction. Nature, 346:174{177, 1990. [25] H. P. Snippe and J. J. Koenderink. Discrimination thresholds for channel-coded systems. Biological Cybernetics, 66:543{551, 1992. [26] G. R. Stoner, T. D. Albright, and V. S. Ramachandran. Transparency and coherence in human motion perception. Nature, 344:153{155, 1990. [27] K. Tanaka. Inferotemporal cortex and higher visual functions. Current Opinion in Neurobiology, 2:502{505, 1992. [28] S. Treue and R. A. Andersen. 3-D structure from motion: rigidity and surface interpolation. Invest. Ophthalm. Vis. Sci., 31 (4):172, 1992. [29] E. K. Warrington and A. M. Taylor. The contribution of the right parietal lobe to object recognition. Cortex, 9:152{164, 1973. [30] G. Westheimer. Visual hyperacuity. Prog. Sensory Physiol., 1:1{37, 1981. [31] M. P. Young and S. Yamane. Sparse population coding of faces in the inferotemporal cortex. Science, 256:1327{1331, 1992. [32] A. L. Yuille and N. M. Grzywacz. A computational theory for the perception of coherent visual motion. Nature, 333:71{74, 1988."
052b1d8ce63b07fec3de9dbb583772d860b7c769,
111fd833a4ae576cfdbb27d87d2f8fc0640af355,
1bd9ac8414a9fbe08c5322381bc3a67041184b95,
3106e66537a0c8f53278e553bcb38f0b0992ec0e,
bd2fbbd4228f4e6c79a182876afa7fd972f79dde,
4a42b2104ca8ff891ae77c40a915d4c94c8f8428,"Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in weight space . We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the analog networks described by Hopefield and Tank (Hopfield 85). The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search."
8592e46a5435d18bba70557846f47290b34c1aa5,"This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
99f43ab78c280cbc1ec44145a015ee78ac2f21b0,"A cnW computatbnd problem in ~ercepUon bmarch. Ohrm 8 ~ # ( d q n d k l a t e h y p o l h . s c w ; r b o u t h o w t o k ~ p r m o r m p e c w d m ~ , n d r r t d p l a w i b k c o M t r a i n l e ~ t l ~ r m , m . v r k r s r m u d b s l r i g n r d t O ~ ~ ~ m t o minim&@ lha total vidaUon d the daudbk csc\atninla Thk kdo~byrHowlngrndworkdcompuUnq~reLolmt le into a rtrM. atate. Each element rcpnrentS r Mpsthsd& urb the intaractionrbetween thedmrnlmP#(NMlt ttn~WtmkW"
b300e1bbb6ad0d513db2eeb64a2508b4fafb9da6,
e03d550b23e45d1c5da474574914d91bee77101f,"This chapter contains sections titled: Classes of PDP Models, Specific Versions of the General Parallel Activation Model, Sigma-Pi Units, Conclusion, Acknowledgments"
f77867d5468d42dcefeb41222657725c8dea442e,"The human brain is very different from a conventional digital computer. It relies on massive parallelism rather than raw speed and it stores long-term knowledge by modifying the way its processing elements interact rather than by setting bits in a passive, general purpose memory. It is robust against minor physical damage and it learns from experience instead of being explicitly programmed. We do not yet know how the brain uses the activities of neurons to represent complex, articulated structures, or how the perceptual system turns the raw input into useful internal representations so rapidly. Nor do we know how the brain learns new representational schemes. But over the past few years there have been a lot of new and interesting theories about these issues. Much of the theorizing has been motivated by the belief that the brain is using computational principles which could also be applied to massively parallel artificial systems, if only we knew what the principles were. 
 
In the talk, I shall focus on the issue of learning. Early research on perceptrons and associative nets (or matrix memories) showed how to set the weights of the connections between input units and output units so that a pattern of activity on the input units would cause the desired pattern of activity on the output units. A variant, called the auto-associative net, did not distinguish between input and output units. It modified the weights of pairwise inter-connections among the units to ensure that any sufficiently large part of a stored pattern could recreate the rest. Recently, Hopfield has developed an interesting way of analyzing the behavior of iterative, auto-associative nets, but research on simple associative networks is generally of limited interest because most interesting tasks are too complex to be performed by auto-association or by direct connections from the input units to the output units. Many intervening layers of ""hidden"" units are generally required and the tough learning problem is to decide how to use these hidden units. The reason this is so difficult is that we are requiring the network to invent its own representational scheme, and the space of possible schemes is immense, even if we restrict ourselves to those that can be implemented conveniently in networks of neuron-like units."
fb0a052d29920bbb90e33641466d0e9e57e44f55,
6b88f41738085c1a2bffe6123541755b1118e5e2,"One way to achieve viewpoint-invariant shape recognition is to impose a canonical, object-based frame of reference on a shape and to describe the positions, sizes and orientations of the shape's features relative to the imposed frame. This compulation can be implemented in a parallel network of neuron-like processors, but the network has a tendency to make errors of a peculiar kind: When presented with several shapes it sometimes perceives one shape in the position of another. The parameters can be carefully tuned to avoid these ""illusory conjunctions"" in normal circumstances, but they reappear if the visual input is replaced by a random mask before the network has settled down. Treisman and Schmidt (1982) have shown that people make similar errors."
7b5056ab6f5c8edf9186e1eec9fa1166aac9ead8,
8942e97ad0fd716170777e452759f2dd94189d0f,
05485a404228b27318ef881a68aec3365df8292a,
a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657,
a896cec84fb454495a5c8f89c6f8f8eee06201d6,"A cnW computatbnd problem in ~ercepUon bmarch. Ohrm 8 ~ # ( d q n d k l a t e h y p o l h . s c w ; r b o u t h o w t o k ~ p r m o r m p e c w d m ~ , n d r r t d p l a w i b k c o M t r a i n l e ~ t l ~ r m , m . v r k r s r m u d b s l r i g n r d t O ~ ~ ~ m t o minim&@ lha total vidaUon d the daudbk csc\atninla Thk kdo~byrHowlngrndworkdcompuUnq~reLolmt le into a rtrM. atate. Each element rcpnrentS r Mpsthsd& urb the intaractionrbetween thedmrnlmP#(NMlt ttn~WtmkW"
b0cb3be87b7f4f50d62d8dbba5a2e8d78c7d90a9,"Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning."
b2175312a8a6197829f2913b6a1b264bc42a328d,
8540cc34a283ee7924d73d11585778afa241e601,"Visual recognition (e.g., object, scene and action recognition) is an active area of research in computer vision due to its increasing number of real-world applications such as video (image) indexing and search, intelligent surveillance, human-machine interaction, robot navigation, etc. Effective modeling of the objects, scenes and actions is critical for visual recognition. Recently, bag of visual words (BoVW) representation, in which the image patches or video cuboids are quantized into visual words (i.e., mid-level features) based on their appearance similarity using clustering, has been widely and successfully explored. The advantages of this representation are: no explicit detection of objects or object parts and their tracking are required; the representation is somewhat tolerant to within-class deformations, and it is efficient for matching. However, the performance of the BoVW is sensitive to the size of the visual vocabulary. Therefore, computationally expensive cross-validation is needed to find the appropriate quantization granularity. This limitation is partially due to the fact that the visual words are not semantically meaningful. This limits the effectiveness and compactness of the representation. To overcome these shortcomings, in this thesis we present principled approach to learn a semantic vocabulary (i.e. high-level features) from a large amount of visual words (mid-level features). In this context, the thesis makes two major contributions. First, we have developed an algorithm to discover a compact yet discriminative semantic vocabulary. This vocabulary is obtained by grouping the visual-words based on their distribution in videos (images) into visual-word clusters. The mutual information (MI) between the clusters and the videos (images) depicts the discriminative power of the semantic vocabulary, while the MI between visual-words and visual-word clusters measures the compactness of the vocabulary. We apply the information bottleneck (IB) algorithm to find the optimal number of visual-word clusters by finding the good tradeoff between compactness and discriminative power. We tested our proposed approach on the state-of-the-art KTH"
a047ac0875b3cb2ac3e15b31bc9cd042bf14aef9,"In order to control a reaching movement of the arm and body, several different computational problems must be solved. Some parallel methods that could be implemented in networks of neuron-like processors are described. Each method solves a different part of the overall task. First, a method is described for finding the torques necessary to follow a desired trajectory. The methods is more economical and more versatile than table look-up and requires very few sequential steps. Then a way of generating an internal representation of a desired trajectory is described. This method shows the trajectory one piece at a time by applying a large set of heuristic rules to a ""motion blackboard"" that represents the static and dynamic parameters of the state of the body at the current point in the trajectory. The computations are simplified by expressing the positions, orientations, and motions of parts of the body in terms of a single, non-accelerating, world-based frame of reference, rather than in terms of the joint-angles or an egocentric frame based on the body itself."
1718965f492d4e9fe1d98a3fb83efe671a4aed2c,"When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with realnumbers, we usc a more dircct encoding in which thc probability \ associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular nondeterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences."
a1e33c9f79f993cc2f7d917d8cb6baee095c570d,"It is becoming increasingly apparent that some aspects of intelligent behavior require enormous computational power and that some sort of massively parallel computing architecture is the most plausible way to deliver such power. Parallelism, rather than raw speed of the computing elements, seems to be the way that the brain gets such jobs done. But even if die need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various AI tasks. 
 
In this paper we will attempt to isolate a number of basic computational tasks that an intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of diese computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number of tasks that are inefficient or impossible on the other architectures."
badb0516cf0ca095c4370a41795323e40e43372c,
79784521ab4a4e19db4ad762ad0fab660a3c8c00,
8cb9a7ae894f43e1ce14186e00298db9a5b5bbe7,
272cb3db246145e13f8db4acbe1d2eae088c7677,"There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units [1 2 3 4]. At the early stages of visual processing, individual units can represent hypotheses about how small local fragments of the visual input should be interpreted, and interactions between units can encode knowledge about the constraints between local interpretations. Higher up in the visual system, the representational issues are more complex. This paper considers the difficulties involved in representing shapes in parallel systems, and suggests ways of overcoming them. In doing so, it provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts."
5b87030ad4ef55fa7c563938217d87b840c2a158,"A viewpoint-independent description of the shape of an object can be generated by imposing a canonical frame of reference on the object and describing the spatial dispositions of the parts relative to this object-based frame. When a familiar object is in an unusual orientation, the deciding factor in the choice of the canonical object-based frame may be the fact that relative to this frame the object has a familiar shape description. This may suggest that we first hypothesise an object-based frame and then test the resultant shape description for familiarity. However, it is possible to organise the interactions between units in a parallel network so that the pattern of activity in the network simultaneously converges on a representation of the shape and a representation of the object-based frame of reference. The connections in the network are determined by the constraints inherent in the image formation process."
921fbe6310285777780dc2df14b29e8a5490933b,
487a36ae299563f9aaea846cf669abfa4906b8fe,"A visual imagery task is presented which is beyond the limits of normal human ability, and some of the factors contributing to its difficulty are isolated by comparing the difficulty of related tasks. It is argued that complex objects are assigned hierarchical structural descriptions by being parsed into parts, each of which has its own local system of significant directions. Two quite different schemas for a wire-frame cube are used to illustrate this theory, and some striking perceptual differences to which they give rise are described. The difficulty of certain mental imagery tasks is shown to depend on which of the alternative structural descriptions of an object is used, and this is interpreted as evidence that structural descriptions are an important component of mental images. Finally, it is argued that analog transformations like mental folding involve changing the values of continuous variables in a structural description."
4a27fe1eef2180d53d3c93814e07999d970b1dce,
f6afa2a788e79f180a0591f441caf34a355732cb,"The problem of finding a puppet in a configuration of overlapping, transparent rectangles is used to show how a relaxation algorithm can extract the globally best figure from a network of conflicting local interpretations."
a7a2461551325aa39ccfa42101f2ba408447c13a,
2b114f4d05494fceb22473fcd29d940e9aa52bf4,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have pro-gressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be ap-proximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through mul-tiple layers of feature detectors."
3e5d838aef4aea3ff22a744f292cfb4094909e35,". Making a perceptual interpretation can be viewed as . a computational process in which a plausible combination is chosen from anlong a large set of interdependent hypotheses. In a cooperative computation the hypotheses are irnplen~ented by units that interact non-linearly and in pa.ralle1 via exciktory and inhibitory Sinks (Julesz, 1971; Man. & Poggio, 1976; Sejnowski, 1976). A particular : perceptual task iS specified by external inputs to some of the units and the whole system must then discover a stable state of activity in which the active units represent the hypothges that are tAen as true. We describe a search procedure based on statistical mechanics that finds near optimal combinations of hypotheses with high probability, and we show that the .. hardware units required for its efficient irnpkmentation are sin~ilar to neurons. Even though the individual units are . non-linear, there is a linear relationship between the synaptic wights and t ie logariLhms of the of global states into which the system settles. This makes it possible to iitiplement a'conyergent learning procedure which specifies just how the synaptic weights need to be changed in order to learn the constrain& in a given domain."
88d214e9fb00e58a7b504c88be0af4b4d07c624f,"Introduction Consider the problem of getting a neural network to associate an appropriate response with an image sequence. The obvious approach is to use supervised training. If the network has around 1014 parameters and bnly lives for around lo9 seconds, the supervision signal had better contain at least lo5 bits per second to make use of the capacity of the synapses. It is not immediately obvious where such a rich supervision signal could come from. A more promising approach depends on the observation that images are not random but are generated by physical processes of limited complexity and that the appropriate response to an image nearly always depends on the physical causes of the image rather than the pixel intensities. This suggests that an unsupervised learning process should be used to solve the difficult problem of extracting the underlying causes, and decisions about responses can be left to a separate learning algorithm that takes the underlying causes rather than the saw sensory data as its inputs. Unsupervised learning can usually be viewed as a method of modeling the probability density of the inputs, so the rich sensory input itself can provide the lo5 bits per second of constraint that is required to amke use of the capacity of the synapses. The papers in this collection provide a sample of research on unsuper-vised learning. Some areas and important contributions are not represented either because an appropriate paper did not appear in Neural Computation or because of the limited space that was available. One entire area of research in unsupervised learning, self-organizing map formation, will appear as a separate volume in this series. Despite these limitations, the wide range of approaches that is included here serves as a guide to the development of the field of unsupervised learning. Redundancy Reduction One of the earliest formulations of unsupervised learning in the context of vision was the concept of redundancy reduction (Attneave 1954; Barlow 1959; Barlow 1989). The goal was to find ways to compress the information contained in images, a goal that was also pursued in the commercial arena to reduce the bandwidth needed to transmit images. In the case of the human visual system, information in the array of photoreceptors in the retina, which number around 100 million, is compressed and represented by spike trains in around 1 million ganglion cells whose axons form the optic nerve. Atick and Redlich (1993) used …"
f1fa5a8addd84be5a0e0382772c807d1ebcf0476,"recognize 10 isolated letters and used artificial markers on the lips. No visual feature extraction was integrated into their model. Also of interest are some psychological studies about human speechreading and their approach to describe the human performance. This measurements could also be applied to the performance analysis of automated speechreading systems. Dodd and Campbell [3], and Demorest and Bernstein [2] did some valuable work in this area. We have shown how a state-of-the-art speech recognition system can be improved by considering additional visual information for the recognition process. This is true for optimal recording conditions but even more for non-optimal recording conditions as they usually exist in real world applications. Experiments were performed on the connected letter recognition task, but similar results can be expected for continuous speech recognition as well. Work is in progress to integrate not only the time independent weight sharing but also position independent weight sharing for the visual TDNN, in order to locate and track the lips. We are also on the way to largely increase our database in order to achieve better recognition rates and to train speaker independently. Investigations of different approaches are still in progress in order to combine visual and acoustic features and to apply different prepro-cessing to the visual data. ACKNOWLEDGEMENTS We appreciate the help from the DEC on campus research center (CEC) for the initial data acquisition. classify /b/ and /p/ based only on visual information would lead to recognition rates not better than guessing, or the net perhaps would get sensitive for features which are uncorelated to the produced speech. This leads to the design of a smaller set of visual distinguishable units in speech, so called "" visemes "". We investigate a new set of 42 visemes and a 1-ton mapping from the viseme set to the phoneme set. The mapping is necessary for the combined layer, in order to calculate the combined acoustic and visual hyphotheses for the DTW layer. For example the hypotheses for /b/ and /p/ are built out of the same viseme /b_or_p/ but the different phonemes /b/ and /p/ respectly. Our database consists of 114 and 350 letter sequences spelled by two male speakers. They consist of names and random sequences. The first data set was split into 75 training and 39 test sequences (speaker msm). The second data set was split into 200 training and 150 test sequences (speaker mcb). …"
4941b79d28f610f6b60928a8f68143da7f70e4ed,"The MIT AI Laboratory has a long tradition of research in most aspects of Artificial Intelligence. Currently, the major foci include computer vision, manipulation, learning, English-language understanding, VLSI design, expert engineering problem solving, common-sense reasoning, computer architecture, distributed problem solving, models of human memory, programmer apprentices, and human education."
24967619930d270ec4595eb95e18f0c198d64ecb,
e9f00e9cf60a768e7495cd46cb1586362f12dd7c,
73855bef1e2131291cfc12d3f98b19e63e17f2f0,
c3264d55336a734b7bfea7125ed822843c25d7f7,
52df763d69ed5fe609a4a88f07e97a6cfaadf59f,"An additive composition which can be adapted into conventional automatic dishwasher detergents for reducing foam during use thereof, comprising:"
f15d659d956c15d1186e24b42e8b00c3b27c11e9,"The problems posed by the representation and recognition of the movements of 3-D shapes are analysed. A representation is proposed for the movements of shapes that lie within the scope of the Marr & Nishihara (1978) 3-D model representation of static shapes. The basic problem is how to segment a stream of movement into pieces, each of which can be described separately. The representation proposed here is based upon segmenting a movement at moments when a component axis, e. g. an arm, starts to move relative to its local coordinate frame (here the torso). For example, walking is divided into a segment of the stationary states between each swing of the arms and legs, and the actual motions between the stationary points (relative to the torso, not the ground). This representation is called the state─motion─state (SMS) moving shape representation, and several examples of its application are given."
2dcffe8606e9fd3fb611be6f1c9571f49664b263,"The construction of directionally selective units, and their use in the processing of visual motion, are considered. The zero crossings of ∇2G(x, y) ∗ I(x, y) are located, as in Marr & Hildreth (1980). That is, the image is filtered through centre-surround receptive fields, and the zero values in the output are found. In addition, the time derivative ∂[∇2G(x, y) ∗ l(x, y)]/∂t is measured at the zero crossings, and serves to constrain the local direction of motion to within 180°. The direction of motion can be determined in a second stage, for example by combining the local constraints. The second part of the paper suggests a specific model of the information processing by the X and Y cells of the retina and lateral geniculate nucleus, and certain classes of cortical simple cells. A number of psychophysical and neurophysiological predictions are derived from the theory."
2b71f43ae6360b82fb6a76bacfff3438aae0a832,
3460b763881e1389be844edc979b4502daefc5a4,
52d761994496abd791996727208c281a65523cc4,
a35cb763da6077d40fa9e1331ef2533c98cf6f1b,
d83ecd7a588461dc13f579d723801a46b90144cb,"For human vision to be explained by a computational theory, the first question is plain: What are the problems that the brain solves when we see? It is argued that vision is the construction of efficient symbolic descriptions from images of the world. An important aspect of vision is therefore the choice of representations for the different kinds of information in a visual scene. An overall framework is suggested for extracting shape information from images, in which the analysis proceeds through three representations: (1) the primal sketch, which makes explicit the intensity changes and local two-dimensional geometry of an image; (2) the 2 1/2-D sketch, which is a viewer-centred representation of the depth, orientation and discontinuities of the visible surfaces; and (3) the 3-D model representation, which allows an object-centred description of the three-dimensional structure and organization of a viewed shape. The critical act in formulating computational theories for processes capable of constructing these representations is the discovery of valid constraints on the way the world behaves, that provide sufficient additional information to allow recovery of the desired characteristic. Finally, once a computational theory for a process has been formulated, algorithms for implementing it may be designed, and their performance compared with that of the human visual processor."
e736f8c5c2008aa8c3733ffacc75fc906b9925dc,"We suggest from the psychophysical data on two-point and line acuity that the smallest foveal channel in human vision must have an excitatory center with a diameter of around 1' 20"". Taking into account the optics of the eye and the finite size of the receptors, we show that this would correspond well to the properties of a retinal ganglion cell receiving excitatory input from a single cone."
1b357b72b725c4a49e115ec3b23ece34141f24ba,
9009c9685754346deb93f316144a9da1f70ffcd8,"A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of ∇2G(x, y)* I(x, y) for image I, where G(x, y) is a two-dimensional Gaussian distribution and ∇2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially localized. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround ∇2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr & Ullman 1979)."
2f84af465b0c8061d18bc014750bfcddf7d8d305,
94645777d9ace911a1be34c7147c52bce1a17534,
bd32c8c89e52f5b4142b848d9b4dae2ececc2892,"Under appropriate conditions zero-crossings of a bandpass signal are very rich in information. The authors examine here the relevance of this result to the early stages of visual information processing, where zero-crossings in the output of independent spatial-frequency-tuned channels may contain sufficient information for much of the subsequent processing."
4bea3b05c503528c993f72a0a64c6e267e54f02e,
bdf9b8f4de001f5f37bc844efbce1210d581d599,"The human visual process can be studied by examining the computational problems associated with deriving useful information from retinal images. In this paper, we apply this approach to the problem of representing three-dimensional shapes for the purpose of recognition. 1. Three criteria, accessibility, scope and uniqueness, and stability and sensitivity, are presented for judging the usefulness of a representation for shape recognition. 2. Three aspects of a representation’s design are considered, (i) the representation’s coordinate system, (ii) its primitives, which are the primary units of shape information used in the representation, and (iii) the organization the representation imposes on the information in its descriptions. 3. In terms of these design issues and the criteria presented, a shape representation for recognition should: (i) use an object-centred coordinate system, (ii) include volumetric primitives of varied sizes, and (iii) have a modular organization. A representation based on a shape’s natural axes (for example the axes identified by a stick figure) follows directly from these choices. 4. The basic process for deriving a shape description in this representation must involve: (i) a means for identifying the natural axes of a shape in its image and (ii) a mechanism for transforming viewer-centred axis specifications to specifications in an object-centred coordinate system. 5. Shape recognition involves: (i) a collection of stored shape descriptions, and (ii) various indexes into the collection that allow a newly derived description to be associated with an appropriate stored description. The most important of these indexes allows shape recognition to proceed conservatively from the general to the specific based on the specificity of the information available from the image. 6. New constraints supplied by a conservative recognition process can be used to extract more information from the image. A relaxation process for carrying out this constraint analysis is described."
1af25c587e3d7f664c9f14ed8494315418cae7f1,"Almost nothing can be deduced about a general three-dimensional surface given only its occluding contours in an image, yet contour information is easily and effectively used by us to infer the shape of a surface. Therefore, implicit in the perceptual analysis of occluding contour must lie various assumptions about the viewed surfaces. The assumptions that seem most natural are (a) that the distinction between convex and concave segments reflects real properties of the viewed surface; and (b) that contiguous portions of contour arise from contiguous parts of the viewed surface – i. e. there are no invisible obscuring edges. It is proved that, for smooth surfaces, these assumptions are essentially equivalent to assuming that the viewed surface is a generalized cone. Methods are defined for finding the axis of such a cone, and for segmenting a surface constructed of several cones into its components, whose axes can then be found separately. These methods provide one link between an uninterpreted figure extracted from an image, and the 3-D representation theory of Marr & Nishihara (1977)."
09526c3ff288be85e51508f4270c920899389181,"A time‐delay neural network (TDNN) approach is presented to speech recognition that is characterized by two important properties: (1) Using multilayer arrangements of simple computing units, a TDNN can represent arbitrary nonlinear classification decision surfaces that are learned automatically using error back propagation. (2) The time‐delay arrangement enables the network to discover acoustic‐phonetic features and the temporal relationships between them independent of position in time and, hence, not blurred by temporal shifts in the input. The TDNNs are compared with the currently most popular technique in speech recognition, hidden Markov models (HMM). Extensive performance evaluation shows that the TDNN recognizes voiced stops extracted from varying phonetic contexts at an error rate four times lower (1.5% vs 6.3%) than the best of our HMMs. To perform this task, the TDNN “invented” well‐known acoustic‐phonetic features (e.g., F2 rise, F2 fall, vowel onset) as useful abstractions. It also developed a..."
32f8034014fa218a365e70b6b3f82ec407af9b2e,
75d5b6239679642070a29859ec009874f3be9a4a,"An Algorithm is proposed for solving the stereoscopic matching problem. The algorithm consists of five steps: (1) Each image is filtered with bar masks of four sizes that vary with eccentricity; the equivalent filters are about one octave wide. (2) Zero-crossings of the mask values are localized, and positions that correspond to terminations are found; (3) For each mask size, matching takes place between pairs of zero- crossings or terminations of the same sign in the two images, for a range of disparities up to about the vergence movements, thus causing small masks to come into correspondence; (5) When a correspondence is achieved, it is written into a dynamic buffer, called the 2 1/2 D sketch. It is shown that this proposal provides a theoretical framework for most existing phychophysical and neurophysiological data about stereopsis. Several critical experimental predictions are also made, for instance, about the wsize of Panum?s area under various conditions. The results of such experiments would tell us whether, for example, cooperativity is necessary for the fusion process."
c4acae182d7282dcef9973231f06b1fca55090e7,"Abstract : Vision is the construction of efficient symbolic descriptions from images of the world. An important aspect of vision is the choice of representations for the different kinds of information in a visual scene. In the early stages of the analysis of an image, the representations used depend more on what it is possible to compute from an image than on what is ultimately desirable, but later representations can be more sensitive to the specific needs of recognition. This essay surveys recent work in vision at M.I.T. from a perspective in which the representational problems assume a primary importance. An overall framework is suggested for visual information processing, in which the analysis proceeds through three representations; (1) the primal sketch, which makes explicit the intensity changes and local two-dimensional geometry of an image, (2) the 2 1/2-D sketch, which is a viewer-centered representation of the depth, orientation and discontinuities of the visible surfaces, and (3) the 3-D model representation, which allows an object-centered description of the three-dimensional structure and organization of a viewed shape. Recent results concerning processes for constructing and maintaining these representations are summarized and discussed. (Author)"
608e7b3105a732660b0b94c861f607b303ddf756,
663cf1997c0995cb20d88a734366fb24f09ef06f,"SUMMARY: The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of a computation is expressed; (2) that at which the algorithms that implement a computation are characterlsed; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realised in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level from current research in vision, and a brief review of the immediate prospects for the field is given. Working papers are informal papers intended for internal use. Introduction Modern neurophysiology has learned much about the operation of the individual neuron, but deceivingly little about the meaning of the circuits they compose. The reason for this can be attributed, at least in part, to a failure to recognise what it means to understand a complex information-processing system. Complex systems cannot be understood as a simple extrapolation of the properties of their elementary components.. One does not formulate a description of thermodynamic al effects using a large set of wave equations, one for each of the particles involved. One describes such effects at their own level, and tries to show that in principle, the microscopic and. macroscopic descriptions are consistent with one another. The core of the problem is that a system as complex as a nervous system or a developing embryo must be analyzed and understood at several different levels. For a system that solves an information processing problem, we may distinguish four important levels of description. At the lowest, there is basic component and circuit analysis-how do transistors, neurons, diodes and synapses work? The second level is the study of particular mechanisms; adders, multipliers, and memories accessed by address or by content. The third level is that of the algorithm, and the top level contains the theory of the overall. computation. For example, take the case of Fourier analysis. The computational theory of the Fourier transform is well understood, and is expressed independently of the particular way in which it is computed. One level down, there are several algorithms for implementing a Fourier transform-the Fast Fourier transform (Cooley & Tukey 1965) which is a …"
6bb6eba248b6ef8d228dc33e6113c3e12dd48fee,"The extraction of stereo-disparity information from two images depends upon establishing a correspondence between them. In this article we analyze the nature of the correspondence computation and derive a cooperative algorithm that implements it. We show that this algorithm successfully extracts information from random-dot stereograms, and its implications for the psychophysics and neurophysiology of the visual system are briefly discussed."
6a8092b98771b8157437e71e351ae1231bdd8259,"Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ''Hopfield Networks is All You Need'' paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class."
872cbaf470315b2ed4832388a317a344a684bce5,
9f3f1fdc930e2f1058c55c445875ad8a416914ba,"Abstract : The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of a computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level. (Author)"
aa4b60b5847999c2f778e3e67ca1f2201e396abb,"An introduction is given to a theory of early visual information processing. The theory has been implemented, and examples are given of images at various stages of analysis. It is argued that the first step of consequence is to compute a primitive but rich description of the grey-level changes present in an image. The description is expressed in a vocabulary of kinds of intensity change (EDGE, SHADING-EDGE, EXTENDED-EDGE, LINE, BLOB etc.). Modifying parameters are bound to the elements in the description, specifying their POSITION, ORIENTATION, TERMINATION points, CONTRAST, SIZE and FUZZINESS. This description is obtained from the intensity array by fixed techniques, and it is called the primal sketch. For most images, the primal sketch is large and unwieldy. The second important step in visual information processing is to group its contents in a way that is appropriate for later recognition. From our ability to interpret drawings with little semantic content, one may infer the presence in our perceptual equipment of symbolic processes that can define ""place-tokens"" in an image in various ways, and can group them according to certain rules. Homomorphic techniques fail to account for many of these grouping phenomena, whose explanations require mechanisms of construction rather than mechanisms of detection. The necessary grouping of elements in the primal sketch may be achieved by a mechanism that has available the processes inferred from above, together with the ability to select items by first order discriminations acting on the elements' parameters. Only occasionally do these mechanisms use downward-flowing information about the contents of the particular image being processed. It is argued that ""non-attentive"" vision is in practice implemented by these grouping operations and first order discriminations acting on the primal sketch. The class of computations so obtained differs slightly from the class of second order operations on the intensity array. The extraction of a form from the primal sketch using these techniques amounts to the separation of figure from ground. It is concluded that most of the separation can be carried out by using techniques that do not depend upon the particular image in question. Therefore, figure-ground separation can normally precede the description of the shape of the extracted form. Up to this point, higher-level knowledge and purpose are brought to bear on only a few of the decisions taken during the processing. This relegates the widespread use of downward-flowing information to a later stage than is found in current machine-vision programs, and implies that such knowledge should influence the control of, rather than interfering with, the actual data-processing that is taking place lower down."
e68cbf62ba53026b788bc068f276588eec349169,
acee550c3ea5610f183059710dd357bea4076014,
f5e796dc458978f1a01d1b1698636df9c5181934,"Abstract : It is proposed that the 3-D representation of an object is represented primarily by a stick-figure configuration, where each stick represents one or more axes in the object's generalized cylinder representation. The loosely hierarchical description of a stick figure is interpreted by a special-purpose processor, able to maintain two vector and the gravitational vertical relative to a Cartesian space-frame. It delivers information about the appearance of these vectors."
593f2d4025cca7a940f6eeabbd4cce5f5101e3b7,"Abstract : A family of symbols is defined by which much of the useful information in an image may be represented, and its choice is justified. The family includes symbols for the various commonly occurring intensity profiles that are associated with the edges of objects, and symbols for the gradual luminance changes that provide clues about a surface's shape. It is shown that these descriptors may readily be computed from measurements similar to those made by simple cells in the visual cortex of the cat. (Author)"
5d84d620de3a1993917396a03f03b81a362f81b5,
8d5b71ca2efff2a16e18165e09822b3403afd32d,
a4722f3b4331e6d119b0848ec08e973105bc091f,"Abstract : The article advances the thesis that the purpose of low-level vision is to encode symbolically all of the useful information contained in an intensity array, using a vocabulary of very low-level symbols. Subsequent processes should have access only to this symbolic description. The reason is one of computational expediency. It allows the low-level processes to run almost autonomously; and it greatly simplifies the application of criteria to an image, whose representation in terms of conditions on the initial intensities, or on simple measurements made from them, is very cumbersome."
218f14f52e549b20cb7cd75761e27645c4a4c3bf,
c47bc9f6f9bac8eb609afa4deb48300d89c61017,"Abstract : A review of the known physiology, anatomy, histology, and psychophysics of the retina is given. Land's retinex theory of colour vision is summarized. It is proposed that one function of the retina is to compute lightness using a two-dimensional parallel algorithm. There are three stages: (1) a centre-surround difference operation; (2) a threshold applied to the difference signal; (3) the inverse of (1), whose output is lightness. The operation of the midget bipolar - midget ganglion channel is analysed in detail, and a functional interpretation of various retinal structures is given. Requirements of the theory are stated concerning the arrangement and connexions of cells, and the signs of the synapses, in the inner plexiform layer."
2c0c20b272df7c2e3752b93e99dafa19f5d43664,
8c01d56e3e34ed6ff2b30e0721e2e0a318df2d27,A quantitative summary is given by visual cortex in the cat. Part of using a sample-and-average technique; technique are briefly set out. of the computation that is performed this computation seems to be achieved some quantitative features of this Working Papers are informal papers intended for internal use.
03854785c693f07eeb1cc64c15c21e6b92699e0c,"It is proposed that the most important characteristic of archicortex is its ability to perform a simple kind of memorizing task. It is shown that rather general numerical constraints roughly determine the dimensions of memorizing models for the mammalian brain, and from these is derived a general model for archicortex. The addition of further constraints leads to the notion of a simple representation, which is a way of translating a great deal of information into the firing of about 200 out of a population of 10 $^5$ cells. It is shown that if about 10 $^5$ simple representations are stored in such a population of cells, very little information about a single learnt event is necessary to provoke its recall. A detailed numerical examination is made of a particular example of this kind of memory, and various general conclusions are drawn from the analysis. The insight gained from these models is used to derive theories for various archicortical areas. A functional interpretation is given of the cells and synapses of the area entorhinalis, the prcsubiculum, the prosubiculum, the cornu ammonis and the fascia dentata. Many predictions are made, a substantial number of which must be true if the theory is correct. A general functional classification of typical archicortical cells is proposed."
ffdbf3958ca2cf325c78ab314034b0543226cd1f,"It is proposed that the learning of many tasks by the cerebrum is based on using a very few fundamental techniques for organizing information. It is argued that this is made possible by the prevalence in the world of a particular kind of redundancy, which is characterized by a ‘Fundamental Hypothesis’. This hypothesisis used to found a theory of the basic operations which, it is proposed, are carried out by the cerebral neocortex. They involve the use of past experience to form so-called ‘classificatory units’ with which to interpret subsequent experience. Such classificatory units are imagined to be created whenever either something occurs frequently in the brain’s experience, or enough redundancy appears in the form of clusters of slightly differing inputs. A(non-Bayesian) information theoretic account is given of the diagnosis of an input as an instance of an existing classificatory unit, and of the interpretation as such of an incompletely specified input. Neural models are devised to implement the two operations of diagnosis and interpretation, and it is found that the performance of the second is an automatic consequence of the model’s ability to perform the first. The discovery and formation of new classificatory units is discussed within the context of these neural models. It is shown how a climbing fibre input (of the kind described by Cajal) to the correct cell can cause that cell to perform a mountain-climbing operation in an underlying probability space, that will lead it to respond to a class of events for which it is appropriate to code. This is called the ‘spatial recognizer effect’. The structure of the cerebral neocortex is reviewed in the light of the model which the theory establishes. It is found that many elements in the cortex have a natural identification with elements in the model. This enables many predictions, with specified degrees of firmness, to be made concerning the connexions and synapses of the following cortical cells and fibres: Martinotti cells; cerebral granule cells; pyramidal cells of layers III, V and II; short axon cells of all layers, especially I, IV and VI; cerebral climbing fibres and those cells of the cortex which give rise to them; cerebral basket cells; fusiform cells of layers VI and VII. It is shown that if rather little information about the classificatory units to be formed has been coded genetically, it may be necessary to use a technique called codon formation to organize structure in a suitable way to represent a new unit. It is shown that under certain conditions, it is necessary to carry out a part of this organization during sleep. A prediction is made about the effect of sleep on learning of a certain kind."
3c6f34131ad83fda26a3d8ca9892a6705fd40d11,"1. A detailed theory of cerebellar cortex is proposed whose consequence is that the cerebellum learns to perform motor skills. Two forms of input—output relation are described, both consistent with the cortical theory. One is suitable for learning movements (actions), and the other for learning to maintain posture and balance (maintenance reflexes)."
674d9e8bfdef3e19938397674ca3a6be676b8278,"Modern neurophysiology has learned much about the operation of the individual neurons but little about the meaning of the circuits they compose. The reason' for this can be attributed, at least in part, to a failure to recognize what it means to understand a complex information-processing system. Complex systems cannot be understood as simple extrapola-tions of the properties of their elementary components. One does not formulate a description of thermodynamic effects using a large set of wave equations, one for each of the particles involved. One describes such effects at their own level and tries to show that, in principle, the microscopic and macroscopic descriptions are consistent with one another. The core of the problem is that a system as complex as a nervous system or a developing embryo must be analyzed and understood at several different levels. For a system that solves an information-processing problem, we may distinguish four important levels of description. At the lowest, there is basic component and circuit analysis-how do transistors, neurons, diodes, and synapses work? The second level is that of particular mechanisms: adders, multipliers, and memories accessed by address or by content. The third level is that of the algorithm , and the top level contains the theory of the overall computation. For example, take the case of Fourier analysis. The computational theory of the Fourier transform is well understood and is expressed independently of the particular way in which it is computed. One level down, there are several algorithms for implementing a Fourier transform-the Fast Fourier Transform (FFT) (Cooley and Tukey, 1965), which is a serial algorithm, and the paTane] ""spatial"" algorithm, which is based on the mechanisms of laser optics. All these algoritruns carry out the same computation, and the choice of which one to use depends upon the particular mechanisms that are available. If one has fast digital memory, adders, and multipliers, one will use the FFT; if one has a laser and photographic plates, one will use an ""optical"" algorithm. In general, mechanisms are strongly determined by hardware, the nature of the computation is determined by the problem, and the algorithms are determined by the computation and the available mechanisms. Each of these four levels of description has its place in the"
dfa732acc875a39b2ae4b8b1de0acda94288a69a,"Local Hebbian learning is believed to be inferior in performance to end-to-end training using a backpropagation algorithm. We question this popular belief by designing a local algorithm that can learn convolutional filters at scale on large image datasets. These filters combined with patch normalization and very steep non-linearities result in a good classification accuracy for shallow networks trained locally, as opposed to end-to-end. The filters learned by our algorithm contain both orientation selective units and unoriented color units, resembling the responses of pyramidal neurons located in the cytochrome oxidase 'interblob' and 'blob' regions in the primary visual cortex of primates. It is shown that convolutional networks with patch normalization significantly outperform standard convolutional networks on the task of recovering the original classes when shadows are superimposed on top of standard CIFAR-10 images. Patch normalization approximates the retinal adaptation to the mean light intensity, important for human vision. We also demonstrate a successful transfer of learned representations between CIFAR-10 and ImageNet 32x32 datasets. All these results taken together hint at the possibility that local unsupervised training might be a powerful tool for learning general representations (without specifying the task) directly from unlabeled data."
99ba331e4679c4e81de9c858540f04ef7165d9d6,"Significance Despite great success of deep learning a question remains to what extent the computational properties of deep neural networks are similar to those of the human brain. The particularly nonbiological aspect of deep learning is the supervised training process with the backpropagation algorithm, which requires massive amounts of labeled data, and a nonlocal learning rule for changing the synapse strengths. This paper describes a learning algorithm that does not suffer from these two problems. It learns the weights of the lower layer of neural networks in a completely unsupervised fashion. The entire algorithm utilizes local learning rules which have conceptual biological plausibility. It is widely believed that end-to-end training with the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility and which is motivated by Hebb’s idea that change of the synapse strength should be local—i.e., should depend only on the activities of the pre- and postsynaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer and is capable of learning early feature detectors in a completely unsupervised way. These learned lower-layer feature detectors can be used to train higher-layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm on simple tasks."
aa52f05176380ac84294d7e941bc036d1665aaac,"Deep neural networks (DNNs) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNNs and humans classify patterns and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. Our article examines these questions within the framework of dense associative memory (DAM) models. These models are defined by the energy function, with higher-order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units, fail to transfer to and fool the models with higher-order interactions. This opens up the possibility of using higher-order models for detecting and stopping malicious adversarial attacks. The results we present suggest that DAMs with higher-order energy functions are more robust to adversarial and rubbish inputs than DNNs with rectified linear units."
a6039921628ced6f90f3f2ae620f8071372db7ae,"An approach to vision research is described that combines ideas about low level processing with more abstract notions about the representation of knowledge in intelligent systems. A particular problem, of the representation of knowledge about the three-dimensional world, is di.sCussed: the outline of a solution is given, and an experimental world of simple mechanical assemblies is described, in which the solution may be implemented and tested. A tentative summary is given of the knowledge that is required for operating in this world, and a research project is proposed. Working Papers are informal papers intended for internal use."
c7385be9489108f6d9c4b0070390b5ff11594e4c,
6840bf5f0f52138d72f9ebf484a47c4037a45a03,"Any solid object can be decomposed into a collection of convex polytopes (in short, convexes). When a small number of convexes are used, such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental to real-time physics simulation in computer graphics, where it creates a unified representation of dynamic geometry for collision detection. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull, or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training, as they abstract away from the topology of the geometry they need to represent. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an autoencoding process. We investigate the applications of this architecture including automatic convex decomposition, image to 3D reconstruction, and part-based shape retrieval."
e93a2b3b4dbbf53bae23b45cc3a39e1eb1ee9c1e,"After a large ""teacher"" neural network has been trained on labeled data, the probabilities that the teacher assigns to incorrect classes reveal a lot of information about the way in which the teacher generalizes. By training a small ""student"" model to match these probabilities, it is possible to transfer most of the generalization ability of the teacher to the student, often producing a much better small model than directly training the student on the training data. The transfer works best when there are many possible classes because more is then revealed about the function learned by the teacher, but in cases where there are only a few possible classes we show that we can improve the transfer by forcing the teacher to divide each class into many subclasses that it invents during the supervised training. The student is then trained to match the subclass probabilities. For datasets where there are known, natural subclasses we demonstrate that the teacher learns similar subclasses and these improve distillation. For clickthrough datasets where the subclasses are unknown we demonstrate that subclass distillation allows the student to learn faster and better."
b89e9f0cef5ace08946a7c07bf7284854c418445,"When a vision system creates an interpretation of some input data, it assigns truth values or probabilities to internal hypotheses about the world. \Ve present a non-deterministic method for assigning truth values that avoids many of the problems encountered by existing relaxation methods. Instead of representing probabilities with realnumbers. we use a more direct encoding in which the probability \ associated with a hypothesis is represented by the probability that it is in one of two states. true or false. We give a particular nondeterministic operator. based on statistlcal mechanics. for updating the truth values of hypotheses. The operator ensures that the probability of discovering a particular combination of hypotheses is a simple function of how good that combination is. We show that there is a simple relationship between this operato.r and Bayesian inference. and we describe a learning rule which allows a parallel system to converge on a set""of weights that optimizes its perceptual inferences."
6a3696419fa4ce169e10f25027a951767f44544f,
fc48accc709c7aa22a9e0b66bf4966f5ea162999,"An autoencoder network uses a set of recognition weights to convert an input vector into a representation vector. It then uses a set of generative weights to convert the representation vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the representation vector and the reconstruction error. This information is minimized by choosing representation vectors stochastically according to a Boltzmann distribution. Unfortunately, if the representation vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible representation vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution. This approximation corresponds to using a suboptimal encoding scheme and therefore gives an upper bound on the minimal description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn distributed representations in which many di erent hidden causes combine to produce each observed data vector. Such representations can be exponentially more e cient in their use of hardware than standard vector quantization or mixture models."
31b7d44b971eee9baf62974cbdb94a6b5d62a7af,
191e7b2aeaf08ef86bec0c0392333ade640bfa1e,"Security over the years remains a major concern of all especially the law enforcement agencies. One way of arresting this concern is to be able to reliably detecting deception. Detecting deception remains a difficult task as no perfect method has been found for the detection. Past researches made use of a single cue (verbal or nonverbal), it was found that examining combinations of cues will detect deception better than examining a single cue. Since no single verbal or nonverbal cue is able to successfully detect deception the research proposes to use both the verbal and nonverbal cues to detect deception. Therefore, this research aims to develop a KNN model for classifying the extracted verbal, nonverbal and VerbNon features as deceptive or truthful. The system extracted desired features from the dataset of Perez-Rosas. The verbal cues capture the speech of the suspect while the nonverbal cues capture the facial expressions of the suspect. The verbal cues include the voice pitch (in terms of variations), frequency perturbation also known as jitters, pauses (voice or silent), and speechrate (is defined as the rate at which the suspect is speaking). The Praat (a tool for speech analysis) was used in"
7e08b47eadfac97fab508485ed5fbef9dbbbd9a3,"We describe a hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model uses bottom-up, top-down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demonstrate that the network learns to extract sparse, distributed, hierarchical representations."
4c38005ebf73eb339fe65540d23c91ae7c203fab,
6dd01cd9c17d1491ead8c9f97597fbc61dead8ea,"An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up ""recognition"" connections convert the input into representations in successive hidden layers, and top-down ""generative"" connections reconstruct the representation in one layer from the representation in the layer above. In the ""wake"" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the ""sleep"" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
4f1f81478848f96c732b2b09fa892711ba54dfe8,
387b79fbb2d8a379d86b272189e3fa8665110e18,"An efficient and useful representation for an object viewed from different positions is in terms of its instantiation parameters. We show how the Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to develop a population code for the instantiation parameters of an object in an image. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a standard shape (a bump) in this space, they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in a self-supervised network are trained to make the activities form a bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops separate population codes when presented with different objects."
bba79178c8f632a1dc519d6b64e02f07f0ab2401,
7b62ab1607ff003300b8819e1b8a76035406e7b6,"A recurrent connectionist network was trained to output semantic feature vectors when presented with letter strings. When damaged, the network exhibited characteristics that resembled several of the phenomena found in deep dyslexia and semantic-access dyslexia. Damaged networks sometimes settled to the semantic vectors for semantically similar but visually dissimilar words. With severe damage, a forced-choice decision between categories was possible even when the choice of the particular semantic vector within the category was not possible. The damaged networks typically exhibited many mixed visual and semantic errors in which the output corresponded to a word that was both visually and semantically similar. Surprisingly, damage near the output sometimes caused pure visual errors. Indeed, the characteristic error pattern of deep dyslexia occurred with damage to virtually any part of the network."
7f45b5560224b821c6a9231fced18873d1a0f5ec,
e4250448b8b2511246a9eae881ca7397747d4750,"A recurrent connectionist network was trained to output semantic feature vectors when presented with letter strings. When damaged, the network exhibited characteristics that resembled several of the phenomena found in deep dyslexia and semantic-access dyslexia. Damaged networks sometimes settled to the semantic vectors for semantically similar but visually dissimilar words. With severe damage, a forced-choice decision between categories was possible even when the choice of the particular semantic vector within the category was not possible. The damaged networks typically exhibited many mixed visual and semantic errors in which the output corresponded to a word that was both visually and semantically similar. Surprisingly, damage near the output sometimes caused pure visual errors. Indeed, the characteristic error pattern of deep dyslexia occurred with damage to virtually any part of the network."
a87511a0b85d7892c01c3a020943d3f61c53b7e3,Abstract : Contents: Back-Propagation Learning; Sequential and Recurrent Networks; New Learning Architectures; Analysis of Networks; Language and Cognition; Speech Recognition; Vision; Part 8 Hardware.
f0d4ee24cc267a41e8991336491afa8cf490ceb6,
266ce7e9380be04990d2c603a42465bbe8e9c7be,"Most people can correctly apply the concepts of horizontal and vertical in describing objects, but a simple demonstration shows that they are confused about how these concepts work. The nature of the confusion and its possible causes are briefly discussed."
749ce8ccd9453d1b34901143cddf5f9bee2977cf,
874599e658bb6ec05c44f900e98a30d1c8e66412,"The differentiation of figure from ground plays an important role in the perceptual organization of visual stimuli. The rapidity with which we can discriminate the inside from the outside of a figure suggests that at least this step in the process may be performed in visual cortex by a large number of neurons in several different areas working together in parallel. We have attempted to simulate this collective computation by designing a network of simple processing units that receives two types of information: bottom-up input from the image containing the outlines of a figure, which may be incomplete, and a top-down attentional input that biases one part of the image to be the inside of the figure. No presegmentation of the image was assumed. Two methods for performing the computation were explored: gradient descent, which seeks locally optimal states, and simulated annealing, which attempts to find globally optimal states by introducing noise into the computation. For complete outlines, gradient descent was faster, but the range of input parameters leading to successful performance was very narrow. In contrast, simulated annealing was more robust: it worked over a wider range of attention parameters and a wider range of outlines, including incomplete ones. Our network model is too simplified to serve as a model of human performance, but it does demonstrate that one global property of outlines can be computed through local interactions in a parallel network. Some features of the model, such as the role of noise in escaping from nonglobal optima, may generalize to more realistic models."
c0900df8d62877a68f052b41cadb3ded6e142785,
3d98e58d31dc0cd28fd711cc50d2c2344bb5e80d,"Micronesian navigators routinely make voyages across large expanses of open ocean. To do this, a navigator must judge both the direction in which he is sailing and the distance he has travelled. The rising and setting points of the stars (and other cues) provide instantaneous information about direction, but distance can only be judged by integrating velocity-related information over time. Micronesian navigators judge distance in a way that seems odd. When they are out of sight of land, they imagine that the canoe is stationary and that the islands move back past them. For each voyage, they ‘attend’ to an island off to the side of the course which is out of sight over the horizon. As they sail, they imagine the island moving back along the horizon changing in bearing until it is imagined to be under the bearing it is known to have from the destination island. Then they know they are near their destination. There is good reason for using a frame of reference whose origin is defined by the boat. We show how it finesses a perceptual paradox–the rising and setting points of the stars do not exhibit motion parallax."
cd5158389fdb229879c89d5f66b2f84b3f87352c,
07ea608b82d9741ee585f5f5dcfd334b360c2b89,"The fruit fly Drosophila's olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule, our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science, BioHash and BioConvHash are fast, scalable and yield compressed binary representations that are useful for similarity search."
42b3b888a0526bde853337ac96076c51ef7be88c,
f23ce18b70f62c3b92692d6c118c1810bc33aac5,"‘I didn’t really think of this as moving into biology, but rather as exploring another venue in which to do physics.’ John Hopfield provides a personal perspective on working on the border between physical and biological sciences."
37cfd9d4f551af3a0e6e8173b249108a715feeb3,"Efficient path planning and navigation is critical for animals, robotics, logistics and transportation. We study a model in which spatial navigation problems can rapidly be solved in the brain by parallel mental exploration of alternative routes using propagating waves of neural activity. A wave of spiking activity propagates through a hippocampuslike network, altering the synaptic connectivity. The resulting vector field of synaptic change then guides a simulated animal to the appropriate selected target locations. We demonstrate that the navigation problem can be solved using realistic, local synaptic plasticity rules during a single passage of a wavefront. Our model can find optimal solutions for competing possible targets or learn and navigate in multiple environments. The model provides a hypothesis on the possible computational mechanisms for optimal path planning in the brain, at the same time it is useful for neuromorphic implementations, where the parallelism of information processing proposed here can fully be harnessed in hardware."
690498843827924f87745059199f3ebf526b5432,"A of us who have watched as a friend or relative has disappeared into the fog of Alzheimer’s arrive at the same truth. Although we recognize people by their visual appearance, what we really are as individual humans is determined by how our brains operate. The brain is certainly the least understood organ in the human body. If you ask a cardiologist how the heart works, she will give an engineering description of a pump based on muscle contraction and valves between chambers. If you ask a neurologist how the brain works, how thinking takes place, well . . . Do you remember Rudyard Kipling’s Just So Stories, full of fantastical evolutionary explanations, such as the one about how the elephant got its trunk? They are remarkably similar to a medical description of how the brain works. The annual meeting of the Society for Neuroscience attracts over thirty thousand registrants. It is not for lack of effort that we understand so little of how the brain functions. The problem is one of the size, complexity, and individuality of the human brain. Size: the human brain has approximately one hundred billion nerve cells, each connecting to one thousand others. Complexity: there are one hundred different types of nerve cells, each with its own IAS The Institute Letter"
a1d5a3751e6d00bca2916a6e2f1d12d064ad80a3,"Efficient path planning and navigation is critical for animals, robotics, logistics and transportation. We study a model in which spatial navigation problems can rapidly be solved in the brain by parallel mental exploration of alternative routes using propagating waves of neural activity. A wave of spiking activity propagates through a hippocampus-like network, altering the synaptic connectivity. The resulting vector field of synaptic change then guides a simulated animal to the appropriate selected target locations. We demonstrate that the navigation problem can be solved using realistic, local synaptic plasticity rules during a single passage of a wavefront. Our model can find optimal solutions for competing possible targets or learn and navigate in multiple environments. The model provides a hypothesis on the possible computational mechanisms for optimal path planning in the brain, at the same time it is useful for neuromorphic implementations, where the parallelism of information processing proposed here can fully be harnessed in hardware."
6bdf523ada308b2cbdb8cf9eedb9e60a65b937d0,"The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non–linear models. This framework quantitatively embodies ‘Occam’s razor’. Over–complex and under– regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. When applied to ‘neural networks’, the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on–line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well–determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier’s predictions yields improved performance. Comparisons of the inferences of the Bayesian framework with more traditional cross– validation methods help detect poor underlying assumptions in learning models. The relationship of the Bayesian learning framework to ‘active learning’ is examined. Objective functions are discussed which measure the expected informativeness of candidate data measurements, in the context of both interpolation and classification problems. The concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation."
08fa30e547ebb7f7810ebae121de5a4003cdbabc,"Thinking allows an animal to take an effective action in a novel situation based on a mental exploration of possibilities and previous knowledge. We describe a model animal, with a neural system based loosely on the rodent hippocampus, which performs mental exploration to find a useful route in a spatial world it has previously learned. It then mentally recapitulates the chosen route, and this intent is converted to motor acts that move the animal physically along the route. The modeling is based on spiking neurons with spike-frequency adaptation. Adaptation causes the continuing evolution in the pattern of neural activity that is essential to mental exploration. A successful mental exploration is remembered through spike-timing-dependent synaptic plasticity. The system is also an episodic memory for an animal chiefly concerned with locations."
f2c8bbae0a1bd80783235ca021268b69c95a06f3,"Animals learn tasks requiring a sequence of actions over time. Waiting a given time before taking an action is a simple example. Mimicry is a complex example, e.g. in humans, humming a brief tune you have just heard. Re-experiencing a sensory pattern mentally must involve reproducing a sequence of neural activities over time. In mammals, neurons in prefrontal cortex have time-dependent firing rates that vary smoothly and slowly in a stereotyped fashion. We show through modeling that a Many are Equal computation can use such slowly-varying activities to identify each timepoint in a sequence by the population pattern of activity at the timepoint. The MAE operation implemented here is facilitated by a common inhibitory conductivity due to a theta rhythm. Sequences of analog values of discrete events, exemplified by a brief tune having notes of different durations and intensities, can be learned in a single trial through STDP. An action sequence can be played back sped up, slowed down, or reversed by modulating the system that generates the slowly changing stereotyped activities. Synaptic adaptation and cellular post-hyperpolarization rebound contribute to robustness. An ability to mimic a sequence only seconds after observing it requires the STDP to be effective within seconds."
87d6d181334434d7c9a5df411ec92e09f6fc14cc,"Although gamma frequency oscillations are common in the brain, their functional contributions to neural computation are not understood. Here we report in vitro electrophysiological recordings to evaluate how noisy gamma frequency oscillatory input interacts with the overall activation level of a neuron to determine the precise timing of its action potentials. The experiments were designed to evaluate spike synchrony in a neural circuit architecture in which a population of neurons receives a common noisy gamma oscillatory synaptic drive while the firing rate of each individual neuron is determined by a slowly varying independent input. We demonstrate that similarity of firing rate is a major determinant of synchrony under common noisy oscillatory input: Near coincidence of spikes at similar rates gives way to substantial desynchronization at larger firing rate differences. Analysis of this rate-specific synchrony phenomenon reveals distinct spike timing “fingerprints” at different firing rates that emerge through a combination of phase shifting and abrupt changes in spike patterns. We further demonstrate that rate-specific synchrony permits robust detection of rate similarity in a population of neurons through synchronous activation of a postsynaptic neuron, supporting the biological plausibility of a Many Are Equal computation. Our results reveal that spatially coherent noisy oscillations, which are common throughout the brain, can generate previously unknown relationships among neural rate codes, noisy interspike intervals, and precise spike synchrony codes. All of these can coexist in a self-consistent manner because of rate-specific synchrony."
782aaae1c6565763de13160754845e41ddd9f704,
9309441fca9fe93ab3962af953b756f40ea07fcc,
866951afa8b949a180756510516d816a686c2ed8,"The algorithms that simple feedback neural circuits representing a brain area can rapidly carry out are often adequate to solve easy problems but for more difficult problems can return incorrect answers. A new excitatory-inhibitory circuit model of associative memory displays the common human problem of failing to rapidly find a memory when only a small clue is present. The memory model and a related computational network for solving Sudoku puzzles produce answers that contain implicit check bits in the representation of information across neurons, allowing a rapid evaluation of whether the putative answer is correct or incorrect through a computation related to visual pop-out. This fact may account for our strong psychological feeling of right or wrong when we retrieve a nominal memory from a minimal clue. This information allows more difficult computations or memory retrievals to be done in a serial fashion by using the fast but limited capabilities of a computational module multiple times. The mathematics of the excitatory-inhibitory circuits for associative memory and for Sudoku, both of which are understood in terms of energy or Lyapunov functions, is described in detail."
389c85f6ea2e0c07d2a1dec6c0165b5dad204105,"A new dynamical system and computational circuit is described and analyzed. The dynamics permits the construction of a Lyapunov function that ensures global convergence to a unique stable equilibrium. The analog circuit realization is of the neural network type, with N cells represented by high-gain amplifiers, global feedback, and at most 2N interconnections, where N is the number of inputs. A specific application (called ""the K-selector"") which signals the ranks of the K largest elements of input list and, in parallel the rank of the (K+1)th element, is designed and numerically tested. For a given density of the input elements, one obtains feasible separation intervals of output signals, i.e., good processing performances. The circuit requires an appropriate control source and suitable scaling of the input data."
a210b6319f838aea250d49ffd5c0db3d81dba390,"This thesis presents two novel concepts; the first, in the field of Machine Learning, where a probabilistic generative model, the Nonnegative Boltzmann Machine, is developed for the specific case of modeling continuous nonnegative data; and the second in the area of Global Optimization, where a novel algorithm, the Tunneling Salesman Algorithm, is presented that exploits the mathematics of quantum mechanics (QM). 
The Nonnegative Boltzmann machine (NNBM) is a recurrent neural network model that can describe multimodal nonnegative data. Application of maximum likelihood estimation to this model gives a learning rule that is analogous to the binary Boltzmann machine. We examine the utility of the mean field approximation for the NNBM, and describe the use of Monte Carlo sampling techniques to learn the model parameters. Reflective slice sampling is particularly well-suited for this distribution, and can efficiently be implemented to sample the distribution. A second-order mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a “high-temperature” expansion is also presented. The two approaches to learning the model are tested on learning a bimodal 2-dimensional model, a high-dimensional translationally-invariant distribution, a generative model for handwritten digits, and a generative model for face images. 
Machine learning problems frequently reduce to search for an optimum of some cost-function on the parameters of the model in question. In the novel optimization algorithm we propose, the groundstate of a quantum system is approximated using a method based on perturbation theory. By choosing the mass of the particle in the system to be sufficiently large, the ground state wave function of the system becomes localized to the global minimum of its potential. 
Hence, a given optimization problem involving the minimisation of a cost function can be restated as the problem of finding the ground state of the QM system with a potential specified by that cost function. In particular, computationally ‘hard’ integer problems, including factoring and the travelling salesman problem, can be cast as QM problems involving a quartic polynomial potential. In this thesis an optimization algorithm based on these ideas is demonstrated in 1 and 2-dimensions, for visualization, and then applied to the problem of factoring biprimes of up to 12 bits, and 4- and 5-City Travelling Salesman problems."
f9a29d49db004d16a2c216ad4ef74d81d1a23e53,"Submitted for the MAR05 Meeting of The American Physical Society Computation through the development of synchrony in neural models JAN ENGELBRECHT, Boston College, JOHN HOPFIELD, Princeton University — We investigate integrate-and-fire models with an emphases on temporal order. Our goal is to demonstrate that simple models can perform nontrivial computations based on the development of synchrony. While the computational strategies are general, we realize our model in a sensory processing context, particularly olfaction, where we consider recognizing an odor in the presence of a background, using multiple sniffs. This introduces a time-dependent aspect in the stimuli where the advantages of temporal coding may become more apparent. Our synchrony-based computational strategy emphasizes the role of adaptation and also employs the general principle: “What moves together is an object.” Jan Engelbrecht Boston College Date submitted: 07 Dec 2004 Electronic form version 1.4"
3c6625bfbb58bb4b61003757cdca8066555e02a6,"The structural similarity of neural networks and genetic regulatory networks to digital circuits, and hence to each other, was noted from the very beginning of their study [1, 2]. In this work, we propose a simple biochemical system whose architecture mimics that of genetic regulation and whose components allow for in vitro implementation of arbitrary circuits. We use only two enzymes in addition to DNA and RNA molecules: RNA polymerase (RNAP) and ribonuclease (RNase). We develop a rate equation for in vitro transcriptional networks, and derive a correspondence with general neural network rate equations [3]. As proof-of-principle demonstrations, an associative memory task and a feedforward network computation are shown by simulation. A difference between the neural network and biochemical models is also highlighted: global coupling of rate equations through enzyme saturation can lead to global feedback regulation, thus allowing a simple network without explicit mutual inhibition to perform the winner-take-all computation. Thus, the full complexity of the cell is not necessary for biochemical computation: a wide range of functional behaviors can be achieved with a small set of biochemical components."
a3d022a9b70a2cbe63fc3936dddad4e03f85d06a,"Many stimuli have meaning only as patterns over time. Most auditory and many visual stimuli are of this nature and can be described as multidimensional, time-dependent vectors. A simple neuron can encode a single component of the vector in a firing rate. The addition of a small subthreshold oscillatory current perturbs the action-potential timing, encoding the signal also in a timing relationship, with little effect on the coexisting firing rate representation. When the subthreshold signal is common to a group of neurons, the timing-based information is significant to neurons receiving inputs from the group. This information encoding allows simple implementation of computations not readily done with rate coding. These ideas are examined by using speech to provide a realistic input signal to a biologically inspired model network of spiking neurons. The output neurons of the two-layer system are shown to specifically encode short linguistic elements of speech."
edf6188089886aa03e43e00b3c9b76ee3142d3de,
02637cb8a44879c49ae809de6ad8bfd1f5e897a7,A clamping unit for clamping the mold of an injection molding machine has at least one removable protective cover for covering the clamping unit in the area of the mold. The cover has at least three protective cover plates arranged to be moved independently of one another in the opening direction of the clamping unit between an open position and a protective position. Switches and a safety valve are associated with the cover for interrupting the main current and fluid flows of the injection molding machine when all of the cover plates are in their open position.
1b8fcb53438f1535cc8a24a6aa38089b2d317b28,"We present evidence that resonance properties of rat vibrissae differentially amplify high-frequency and complex tactile signals. Consistent with a model of vibrissa mechanics, optical measurements of vibrissae revealed that their first mechanical resonance frequencies systematically varied from low (60-100 Hz) in longer, posterior vibrissae to high (∼750 Hz) in shorter, anterior vibrissae. Resonance amplification of tactile input was observed in vivo and ex vivo, and in a variety of boundary conditions that are likely to occur during perception, including stimulation of the vibrissa with moving complex natural stimuli such as sandpaper. Vibrissae were underdamped, allowing for sharp tuning to resonance frequencies. Vibrissa resonance constitutes a potentially useful mechanism for perception of high-frequency and complex tactile signals. Amplification of small amplitude signals by resonance could facilitate detection of stimuli that would otherwise fail to drive neural activity. The systematic map of frequency sensitivity across the face could facilitate texture discrimination through somatotopic encoding of frequency content. These findings suggest strong parallels between vibrissa tactile processing and auditory encoding, in which the cochlea also uses resonance to amplify low-amplitude signals and to generate a spatial map of frequency sensitivity."
7543ffd34a120f9c5b4b9bb65a1672e7cc4742dd,"Plasticity in connections between neurons allows learning and adaptation, but it also allows noise to degrade the function of a network. Ongoing network self-repair is thus necessary. We describe a method to derive spike-timing-dependent plasticity rules for self-repair, based on the firing patterns of a functioning network. These plasticity rules for self-repair also provide the basis for unsupervised learning of new tasks. The particular plasticity rule derived for a network depends on the network and task. Here, self-repair is illustrated for a model of the mammalian olfactory system in which the computational task is that of odor recognition. In this olfactory example, the derived rule has qualitative similarity with experimental results seen in spike-timing-dependent plasticity. Unsupervised learning of new tasks by using the derived self-repair rule is demonstrated by learning to recognize new odors."
c512379abe510a7b28b5e526a02aafead56019d4,
c8f8e130fff5f764d9ecfa16a5d95ceb078b8c18,
de5d5aa9a3c1d7148f391e1d16f3656dbf21d98a,"Most academic departments in computer science and electrical engineering provide some sort of intellectual umbrella to integrate research in machine learning, computer vision, robotics, and artificial intelligence (AI). For largely historical and outmoded reasons, however, research in auditory computation and machine listening has not been included in these efforts. We hope to rectify this situation."
40fbc95482351775a28f2a7bbd8ebeb500ee9ac1,
af1453a9c6bdb005b11cca0cdb21f886bae6a8c7,
406d968077fdb21f7d44e5b0c095ff1d66bcd329,"A previous paper described a network of simple integrate-and-fire neurons that contained output neurons selective for specific spatiotemporal patterns of inputs; only experimental results were described. We now present the principles behind the operation of this network and discuss how these principles point to a general class of computational operations that can be carried out easily and naturally by networks of spiking neurons. Transient synchrony of the action potentials of a group of neurons is used to signal ""recognition"" of a space-time pattern across the inputs of those neurons. Appropriate synaptic coupling produces synchrony when the inputs to these neurons are nearly equal, leaving the neurons unsynchronized or only weakly synchronized for other input circumstances. When the input to this system comes from timed past events represented by decaying delay activity, the pattern of synaptic connections can be set such that synchronization occurs only for selected spatiotemporal patterns. We show how the recognition is invariant to uniform time warp and uniform intensity change of the input events. The fundamental recognition event is a transient collective synchronization, representing ""many neurons now agree,"" an event that is then detected easily by a cell with a small time constant. If such synchronization is used in neurobiological computation, its hallmark will be a brief burst of gamma-band electroencephalogram noise when and where such a recognition event or decision occurs."
50c110c375ce072ca2d470dfd29d0ac956ea60ee,"In contrast to networks of neurons where behavior is governed by average firing rate, what computations are implemented most easily, efficiently, and robustly by networks of neurons that spike? Spiking neurons synchronize much more readily when their firing rates are similar than when they are different. This property can be used to very simply and robustly implement, in a network of appropriately connected spiking neurons, a many are equals operation: synchronization indicates that many of the neurons' firing rates are similar. Such an operation is computationally very powerful. The computation is robust to outliers, and contains a natural invariance: over a broad range of firing rates, the synchronization phenomenon depends only on rate similarity, and not on the precise firing rate level. We demonstrate the computational power of this operation by constructing a simple network of spiking neurons with output neurons that respond selectively to a complex spectrotemporal pattern, the spoken word one. The response is invariant to uniform time-warp. Time is encoded by slowly decaying firing rates, and the selectivity is largely speaker-independent. We posit many are equals synchronization is a simple yet powerful computational building block for spiking neural networks."
42fd26e1ec3c485d5fd3db8d78a8ce1228efac7d,"Recognition of complex temporal sequences is a general sensory problem that requires integration of information over time. We describe a very simple ""organism"" that performs this task, exemplified here by recognition of spoken monosyllables. The network's computation can be understood through the application of simple but generally unexploited principles describing neural activity. The organism is a network of very simple neurons and synapses; the experiments are simulations. The network's recognition capabilities are robust to variations across speakers, simple masking noises, and large variations in system parameters. The network principles underlying recognition of short temporal sequences are applied here to speech, but similar ideas can be applied to aspects of vision, touch, and olfaction. In this article, we describe only properties of the system that could be measured if it were a real biological organism. We delay publication of the principles behind the network's operation as an intellectual challenge: the essential principles of operation can be deduced based on the experimental results presented here alone. An interactive web site (http://neuron.princeton.edu/ approximately moment) is available to allow readers to design and carry out their own experiments on the organism."
81ae2502783ed2614ceb81a4eaf005959e470d9c,
510db90e58c064a64a6225365cf04ed672bbeb2d,
537d4ceb7272fc4a78774b8e3899aeb14b88a8ff,
998db038a77d8a92acdf1308ded0f9292a304b5a,"Several basic olfactory tasks must be solved by highly olfactory animals, including background suppression, multiple object separation, mixture separation, and source identification. The large number N of classes of olfactory receptor cells-hundreds or thousands-permits the use of computational strategies and algorithms that would not be effective in a stimulus space of low dimension. A model of the patterns of olfactory receptor responses, based on the broad distribution of olfactory thresholds, is constructed. Representing one odor from the viewpoint of another then allows a common description of the most important basic problems and shows how to solve them when N is large. One possible biological implementation of these algorithms uses action potential timing and adaptation as the ""hardware"" features that are responsible for effective neural computation."
ec94b60c8e678e4fd7473d743babc23cba45a280,
2fb88fbc029f8bb70c02736a6263abfb84f2bee1,"The molecular mechanisms underlying long-term potentiation in the hippocampus have received much attention because of the likely functional importance of synaptic plasticity for information storage and the development of neuronal connectivity. Surprisingly, it remains unclear whether activity modifies the strength of individual synapses in a digital (all-or-none) or analog (graded) manner. Here we characterize step-like all-or-none transitions from baseline synaptic transmission to potentiated states following protocols for inducing potentiation at putative single CA3-CA1 synaptic connections. Individual synapses appear to have all-or-none potentiation indicative of highly cooperative processes but different thresholds for undergoing potentiation. These results raise the possibility that some forms of synaptic memory may be stored in a digital manner in the brain."
1d1ed4f0270df721ebeae5567cf2cf6bab17f09c,"A dynamic neural network is developed to detect soft failures of sensors and actuators in automobile engines. The network, currently implemented off-line in software, can process multi-dimensional input data in real time. The network is trained to predict one of the variables using others. It learns to use redundant information in the variables such as higher order statistics and temporal relations. The difference between the prediction and the measurement is used to distinguish a normal engine from a faulty one. Using the network, we are able to detect errors in the manifold air pressure sensor and the exhaust gas recirculation valve with a high degree of accuracy."
393106ef97287799ba26129dd928f35545ea7dbe,"Research in many fields has become immensely complex. It often requires a combination of knowledge, technique, skills, and inventions sufficiently diverse that only the cooperation of many scientists can result in an important new result and its publication. How then should the authorship of such a paper be described? Does it even matter how the authorship is described?"
626c9b86a15a963f1c6dc6a6cc36e36a99b399da,"A Lyapunov function for excitatory-inhibitory networks is constructed. The construction assumes symmetric interactions within excitatory and inhibitory populations of neurons, and antisymmetric interactions between populations. The Lyapunov function yields sufficient conditions for the global asymptotic stability of fixed points. If these conditions are violated, limit cycles may be stable. The relations of the Lyapunov function to optimization theory and classical mechanics are revealed by minimax and dissipative Hamiltonian forms of the network dynamics."
d39f9875babf29f9d3b1fc33fd277325d29df5db,"Most computational engineering based loosely on biology uses continuous variables to represent neural activity. Yet most neurons communicate with action potentials. The engineering view is equivalent to using a rate-code for representing information and for computing. An increasing number of examples are being discovered in which biology may not be using rate codes. Information can be represented using the timing of action potentials, and efficiently computed with in this representation. The ""analog match"" problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an underlying rhythm. By using adapting units to effect a fundamental change of representation of a problem, we map the recognition of words (having uniform time-warp) in connected speech into the same analog match problem. We describe the architecture and preliminary results of such a recognition system. Using the fast events of biology in conjunction with an underlying rhythm is one way to overcome the limits of an event-driven view of computation. When the intrinsic hardware is much faster than the time scale of change of inputs, this approach can greatly increase the effective computation per unit time on a given quantity of hardware."
a97b37761a21668687ad99329b1a60ce4be1151f,"The response of an equilibrium molecular system to perturbations depends on its environmental constraints. For example, the response of an equilibrium P, V, T system to a small temperature perturbation (specific heat) depends on whether the environmental constraint on the system is constant pressure or constant volume. In general, there are two classes of thermodynamic quantities of a system; one is completely determined by its equilibrium distribution, and the other also depends on how the distribution responds to macroscopic changes. The former class is independent of the environment of the thermodynamic system, while the latter class is a function of environmental constraints. In response to a small perturbation, the free energy change of an equilibrium system belongs to the first class but the entropy and enthalpy changes belong to the second. Therefore the thermodynamics of perturbation exhibit compensation between entropy and enthalpy of systems with different environments. The thermodynamic analysi..."
accd1c47d22a0d939c4fd5865f129a13c9379b2b,"Motifs of neural circuitry seem surprisingly conserved over different areas of neocortex or of paleocortex, while performing quite different sensory processing tasks. This apparent paradox may be resolved by the fact that seemingly different problems in sensory information processing are related by transformations (changes of variables) that convert one problem into another. The same basic algorithm that is appropriate to the recognition of a known odor quality, independent of the strength of the odor, can be used to recognize a vocalization (e.g., a spoken syllable), independent of whether it is spoken quickly or slowly. To convert one problem into the other, a new representation of time sequences is needed. The time that has elapsed since a recent event must be represented in neural activity. The electrophysiological hallmarks of cells that are involved in generating such a representation of time are discussed. The anatomical relationships between olfactory and auditory pathways suggest relevant experiments. The neurophysiological mechanism for the psychophysical logarithmic encoding of time duration would be of direct use for interconverting olfactory and auditory processing problems. Such reuse of old algorithms in new settings and representations is related to the way that evolution develops new biochemistry."
f15e4abbe5fd9a3caad000ab6a69fa88d612b5e5,
f28214ce11ce371c6cb2e36c9969be659485ebd6,"Motifs of neural circuitry seem surprisingly conserved over different areas of neocortex or of paleocortex, while performing quite different sensory processing tasks. This apparent paradox may be resolved by the fact that seeminglydifferentproblemsinsensoryinformationprocess- ingarerelatedbytransformations(changesofvariables)that convert one problem into another. The same basic algorithm thatisappropriatetotherecognitionofaknownodorquality, independent of the strength of the odor, can be used to recognize a vocalization (e.g., a spoken syllable), independent of whether it is spoken quickly or slowly. To convert one problemintotheother,anewrepresentationoftimesequences isneeded.Thetimethathaselapsedsincearecenteventmust be represented in neural activity. The electrophysiological hallmarks of cells that are involved in generating such a representation of time are discussed. The anatomical rela- tionships between olfactory and auditory pathways suggest relevant experiments. The neurophysiological mechanism for the psychophysical logarithmic encoding of time duration would be of direct use for interconverting olfactory and auditoryprocessingproblems.Suchreuseofoldalgorithmsin new settings and representations is related to the way that evolution develops new biochemistry."
0016aeabb79d3266a4baf16b8fb73a4cf6fc6907,"Driven systems of interconnected blocks with stick-slip friction capture main features of earthquake processes. The microscopic dynamics closely resemble those of spiking nerve cells. We analyze the differences in the collective behavior and introduce a class of solvable models. We prove that the models exhibit rapid phase locking, a phenomenon of particular interest to both geophysics and neurobiology. We study the dependence upon initial conditions and system parameters, and discuss implications for earthquake modeling and neural computation."
279c2770c16eb01ce0e404f37a3fa263d623446e,
383186f6a27c2e56ed5900601e4f50c51b717e8f,"The collective behavior of interconnected spiking nerve cells is investigated. It is shown that a variety of model systems exhibit the same short-time behavior and rapidly converge to (approximately) periodic firing patterns with locally synchronized action potentials. The dynamics of one model can be described by a downhill motion on an abstract energy landscape. Since an energy landscape makes it possible to understand and program computation done by an attractor network, the results will extend our understanding of collective computation from models based on a firing-rate description to biologically more realistic systems with integrate-and-fire neurons."
49b49d10cf5ab6a27423e20f4e0c8c364591719a,
95483f491f7d328a43c82c476b19a94ea1595179,
4cb6a4b35b9bb4e24efad3dbb014fd59e5af29a3,
5e0700d27ce3cbbf0ab2300fcac41cc22342ad17,
f5e65973dcf21f1ac5f54b1c9d703a4661b5036a,
754be070f7413bdf31e0ba9b03ae80b85b1683be,"Abstract The biological world is a physical system whose properties and behaviors seem entirely foreign to physics. The origins of this discrepancy lie in the very high information content in biological systems (the large amount of dynamically broken symmetry) and the evolutionary value placed on predicting the future (computation) in an environment which is inhomogeneous in time and in space. Within this context, ""free will"" can be described as a useful predictive myth."
9c5b34010eb3bc80bde689969fbe588f7d63ed07,
ccec4b2d668482f7ab6f1aa6b9a5de3979e844bd,"The question “How does it work?” is the motivation of many physicists. Condensed matter physics, chemical physics and nuclear physics can all be thought of as descriptions of the relation between structure and properties. The components of a biological system have functional properties that are particularly relevant to the operation of the system. Thus it is especially important in biology to understand the relation between structure and function. Such understanding can be sought at the level of the molecule, the cell, the organ, the organism or the social group."
d073ec8c638c8ed36b16d0c681c6f11f02e759dc,"We describe models for the olfactory bulb which perform separation and decomposition of mixed odor inputs from different sources. The odors are unknown to the system; hence this is an analog and extension of the engineering problem of blind separation of signals. The separation process makes use of the different temporal fluctuations of the input odors which occur under natural conditions. We discuss two possibilities, one relying on a specific architecture connecting modules with the same sensory inputs and the other assuming that the modules (e.g., glomeruli) have different receptive fields in odor space. We compare the implications of these models for the testing of mixed odors from a single source."
bde06244ec2ab2e2961be84a4d03ce34a48bebc3,
c00023d0293c34ec5e7073d1deb5888c406993e4,
cf356bc5aa8c35ffaa83140962efc668b6954234,"The design of a shift register memory at the molecular level is described in detail. The memory elements are based on a chain of electron-transfer molecules incorporated on a very large scale integrated (VLSI) substrate, and the information is shifted by photoinduced electron-transfer reactions. The design requirements for such a system are discussed, and several realistic strategies for synthesizing these systems are presented. The immediate advantage of such a hybrid molecular/VLSI device would arise from the possible information storage density. The prospect of considerable savings of energy per bit processed also exists. This molecular shift register memory element design solves the conceptual problems associated with integrating molecular size components with larger (micron) size features on a chip."
98ee20611ae89303c1d58a3bb05053f48dc452eb,
1d5ef06cbcd1063ffe9b39a344dd2c2ec6d3a967,"The capability of a small neural network to perform speaker-independent recognition of spoken digits in connected speech has been investigated. The network uses time delays to organize rapidly changing outputs of symbol detectors over the time scale of a word. The network is data driven and unclocked. To achieve useful accuracy in a speaker-independent setting, many new ideas and procedures were developed. These include improving the feature detectors, self-recognition of word ends, reduction in network size, and dividing speakers into natural classes. Quantitative experiments based on Texas Instruments (TI) digit databases are described."
86f8834cd814b45119c12fa8486800d15b6935fd,
c8d3c811ae6c9f0f81d225d4cf4b023b0ffbeca6,"Two kinds of dynamic processes take place in neural networks. One involves the change with time of the activity of each neuron. The other involves the change in strength of the connections (synapses) between neurons. When a neural network is learning or developing, both processes simultaneously take place, and their dynamics interact. This interaction is particularly important in feedback networks. A Lyapunov function is developed to help understand the combined activity and synapse dynamics for a class of such adaptive networks. The methods and viewpoint are illustrated by using them to describe the development of columnar structure of orientation-selective cells in primary visual cortex. Within this model, the columnar structure originates from symmetry breaking in feedback pathways within an area of cortex, rather than feedforward pathways between areas."
ff1a8f5fdbe5da8ee37738e04c41ec36f75f1b65,"Two kinds of dynamic processes take place in neural networks. One involves the change with time of the activity of each neuron. The other involves the change in the strength of the connections (synapses) between neurons. When a neural network is learning or developing, both processes take place, and their dynamics interact. A theoretical framework is developed to help understand the combined activity and synapse dynamics for a class of adaptive networks. The methods are illustrated by using them to describe the development of orientation-selective cells in the cat primary visual cortex. Within this model, the column structure of different orientation-selective neurons originates from feedback pathways within an area of the cortex, rather than feedforward pathways between areas.<<ETX>>"
0dba9c0e0f0055d54379fdd0b7df66c9bc44f7f2,"An analog neural network that can be taught to recognize stimulus sequences is used to recognize the digits in connected speech. The circuit computes in the analog domain, using linear circuits for signal filtering and nonlinear circuits for simple decisions, feature extraction, and noise suppression. An analog perceptron learning rule is used to organize the subset of connections used in the circuit that are specific to the chosen vocabulary. Computer simulations of the learning algorithm and circuit demonstrate recognition scores >99 % for a single-speaker connected-digit data base. There is no clock. The circuit is data driven, and there is no necessity for endpoint detection or segmentation of the speech signal during recognition. Training in the presence of noise provides noise immunity up to the trained level. For the speech problem studied, the circuit connections need only be accurate to about 3-b digitization depth for optimum performance. The algorithm used maps efficiently onto analog neutral network hardware. >"
3f19a131970681eebbb003ac6e9b50c7a4db352b,"Animals that are primarily dependent on olfaction must obtain a description of the spatial location and the individual odor quality of environmental odor sources through olfaction alone. The variable nature of turbulent air flow makes such a remote sensing problem solvable if the animal can make use of the information conveyed by the fluctuation with time of the mixture of odor sources. Behavioral evidence suggests that such analysis takes place. An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources. The network structure is an idealization of olfactory bulb circuitry. The dynamics of synapse change is essential to the computation. The synaptic variables themselves contain information needed by higher processing centers. The use of the same axons to convey intensity information and quality information requires time-coding of information. Covariation defines an individual odor source (object), and this may have a parallel in vision."
6dceb33b2ce28d9669a7fb03a953a7b7a3dd35a3,
59b3e85ed383ce3763ecb7247f39963d08a5dbc2,"Computation is carried out by the motion of the state of a dynamical system. This description unites digital and “neural network” computation. The use of an energy function to understand and to program batch mode computation on feedback networks is reviewed. Recognizing words in continuous speech illustrates the use of a compound network to solve a real-world problem where data arrive continuously over time. A new tool, a Lagrangian formulation, is introduced for studying networks which oscillate, and as a means for formulating problems which need to optimize functional integrals."
6a6e8eba22d82cd71f29324b5bfe8b4f0b960b3c,"Artificial neural network algorithms give adequate or even excellent results on many computational problems. Such algorithms can be embedded in special-purpose hardware for efficient implementation. Within a particular hardware class, the algorithms can be implemented either as analogue neural networks or as a digital representation of the same problem. The speed, area and required precision of the two forms of hardware for representing the same problem are discussed for a hardware model which lies between VLSI hardware and biological neurons. It is usually true that the digital representation computes faster, requires more devices and resources, and requires less precision of manufacture. An exception to this rule occurs when the device physics generates a function which is explicitly needed in the algorithm. Major advances in analogue neural net hardware will require the exploitation of device physics available at the level of materials and transistors."
9f494d374fcbbc99bfcd787feec9c9d596824fcd,
5101fca38aa29f22ae5c26a11f565a717857c226,
62222ec3886e1c9b7d58464d00d5b2eb207587f0,
8879e718ab012e530608a72cddab3ac0edc30743,"We describe how several optimization problems can be rapidly solved by highly interconnected networks of simple analog processors. Analog-to-digital (A/D) conversion was considered as a simple optimization problem, and an A/D converter of novel architecture was designed. A/D conversion is a simple example of a more general class of signal-decision problems which we show could also be solved by appropriately constructed networks. Circuits to solve these problems were designed using general principles which result from an understanding of the basic collective computational properties of a specific class of analog-processor networks. We also show that a network which solves linear programming problems can be understood from the same concepts."
a16b15fbb45e41ad23521523e8899273e73b6e75,
13950a9c18c0ec85b0e77578e53be9ca454851de,"Resonance Raman, optical absorption, and circular dichroism spectroscopic techniques have been used to examine the effect of the addition of inositol hexaphosphate (IHP) to a series of carp and human methemoglobin derivatives. Markers of spin equilibrium in the high-frequency region (1450-1650 cm-1) of the resonance Raman spectrum yield high/low-spin ratios consistent with direct magnetic susceptibility measurements. Changes in the low-frequency region (100-600 cm-1) of the resonance Raman spectrum appear to correlate with the quaternary structure transition. Changes in the ultraviolet absorption spectra and the circular dichroism spectra also appear to be related to the quaternary structure change. By using the resonance Raman spin markers, we find that those derivatives of carp methemoglobin which are in spin equilibrium have a larger ratio of high-spin to low-spin populations than the corresponding derivatives of human methemoglobin. Upon the addition of IHP to the methemoglobins the spin equilibrium is shifted toward a larger high-spin population. This change in equilibrium is larger for the carp protein than for the human protein. We obtain an IHP-induced change in the free energy difference between the high-spin and low-spin states of 300 cal/mol for those human methemoglobins in which a quaternary structure change occurs and 600 cal/mol for carp methemoglobins. Our data are consistent with a quaternary structure change induced by IHP in all the carp methemoglobins studied (F-, H2O, SCN-, NO2-, N3-, and CN-) and in the F-, H2O, and SCN- derivatives of the human protein but not in the NO2-, N3-, and CN- derivatives. The Fe-CN stretching mode has been identified by isotopic substitution and found to be unchanged in frequency in carp CN- metHb when the quaternary structure is changed. On the basis of our results we conclude that the protein forces at the heme due to the addition of IHP do not significantly affect the position of the iron atom with respect to the heme plane. Rather, the changes in spin equilibrium may be caused by protein-induced changes in the orientation of the proximal histidine or tertiary structure changes in the heme pocket which affect the porphyrin macrocycle. Either of these changes, or a combination thereof, leads to changes in the iron d orbital energies and concomitant changes in the spin equilibrium."
58cc1683ec449657442b510dbdf05712d97c9657,
7a23da2c14f9355dd63a434b62cf5b28aeebc305,
dbab56744e8b6cc7f4643f8414976ab1f22d41b1,
ff3383b663f1004c30d604bf8ca670645ff45984,
1f08533a5a731eb92983980c81538a649c1fcb73,
2c6dca055e07dcd73a5eba91637fcf91e140bf09,
3d5861684ef1c00b67f6607c0d8c2ad844d603eb,"Examines the following questions associated with artificial neural networks: why people are interested in artificial neural networks; what artificial neural networks are, from the point of view of electronic circuits, and how they work; how they can be programmed and made to solve particular problems; and whether interesting problems can actually be put on such networks. The author then describes the current state of artificial neural network technology and the resulting challenges to people working on electronic devices.<<ETX>>"
3e98f79458a2a9e7c9e91d238178593fd1aef561,"An electronic shift-register memory at the molecular level is described. The memory elements are based on a chain of electron-transfer molecules and the information is shifted by photoinduced electron-transfer reactions. This device integrates designed electronic molecules onto a very large scale integrated (silicon microelectronic) substrate, providing an example of a ""molecular electronic device"" that could actually be made. The design requirements for such a device and possible synthetic strategies are discussed. Devices along these lines should have lower energy usage and enhanced storage density."
50b5ee536e625beffb2c1c04b7d45e29d78e8c70,
f25938983409b1563bc33ab934e249c6b0c65efc,"The olfactory bulb of mammals aids in the discrimination of odors. A mathematical model based on the bulbar anatomy and electrophysiology is described. Simulations produce a 35-60 Hz modulated activity coherent across the bulb, mimicing the observed field potentials. The decision states (for the odor information) here can be thought of as stable cycles, rather than point stable states typical of simpler neuro-computing models. Analysis and simulations show that a group of coupled non-linear oscillators are responsible for the oscillatory activities determined by the odor input, and that the bulb, with appropriate inputs from higher centers, can enhance or suppress the sensitivity to particular odors. The model provides a framework in which to understand the transform between odor input and the bulbar output to olfactory cortex."
123e6005fbd4cedb4028c7c61d772cc1622bff2d,
1e570de6e8f260804a673dcb53f13b33c13a3014,
33fdc91c520b54e097f5e09fae1cfc94793fbfcf,"Since an tiquity, man has dreamed of building a de vice that would ""learn from examples"" 1 ""form generalizations"", and ""discover t he rules"" behind patt ern s in t he data. Recent work has shown that a high ly connected , layered networ k of simple an alog processing element s can be astonishingly successful at this, in some cases . In ord er to be precise about what has been observed, we give defini t ions of memorization, generalization , and rule ex traction. T he most im portant part of this paper proposes a way to measure th e ent ropy or information content of a learni ng task a nd the effi ciency wit h which a network ext racts informat ion from the dat a. We also a rgue that the way in which the ne tworks ca n compactly represent a wid e class of Boolean (an d othe r) functi ons is analogous to t he way in which polynomials or other famili es of functions can be ""curve fit"" to gene ral data; specifically, they ex tend the domain, a nd average noisy data. Alas , findi ng a suitable rep rese ntation is generall y an ill-posed and ill-cond itio ned problem. E ven whe n the problem has bee n "" regularized"", what rem ain s is a difficult combinatoria l opt imizatio n problem. Whe n a network is given mo re resou rces than the mi nimu m needed to solve a given t ask , the symmetric, low-order , local solut ions that hum an s see m to pre fer are not the ones that the network chooses from th e vast number of solut ions avai la ble; ind eed , th e generalized delt a method a nd similar learning procedures do not usually hold t he ""human "" solut ions stable against perturbations. Fortuna tely, the re are © 1987 Comp lex Systems Publications, Inc. 878 Denker, Schwart z, Wittner, Solla, Howard , J ackel, and Hopfield ways of ""program ming"" into t he networ k a preference for appropriately chosen symmetries . 1. Overview of the contents Section 2 gives seve ral examples that illustra te t he import ance of automatic learning from examples . Section 3 poses a tes t -case problem (""c l umps"") which will be used t hroughout the paper to illustrate the issues of interest. Section 4 describes the class of networks we are considering and introdu ces t he notation. Section 5 presents a proof by construction t hat a two-layer network can rep resent any Boolean function, and section 6 shows t hat there is an elegant representation for the c lumps tas k, using very few weights and processing units. Sections 7 an d 8 argue that the ob jective function E(W ) has a complicated st ruct ure: good solutions are generally not points in W space, bu t rat her parameteri zed fam ilies of points. Furt hermore, in all but the simplest sit uations, the E su rface is riddled with local minim a, and any automatic lear ning procedure must take firm measures to deal with t his. Section 9 shows that our c l umps tas k is a very simple prob lem, accordin g to the various schemes that have been proposed to quantify the complexity of network tasks and solut ions. Section 10 shows that a general network does no t prefer t he simple solut ions t hat hum ans seem to prefer. Sect ion 11 discusses the crucial effect of changes of representation on the feasibility of aut oma t ic learni ng. We prove that ""automat ic learn ing will always succeed, given t he right preprocessor,"" but we also show t hat t his statement is grossly misleading since there is no automati c procedure for const ruct ing the requ ired preprocessor. Sections 12 and 13 propose definit ions of rule ext ract ion and genera liza t ion and emphas ize th e disti nction between th e two. Sect ion 14 calculates th e entropy budget for ru le ext ract ion and est imates the informat ion available from the t rain ing data and from the ""programming"" or ""architecture"" of t he network. This leads to an ap proximate express ion for t he efficiency with which the learni ng procedu re ext rac ts infor mat ion from t he t ra ining data. Sect ion 16 presents a simple model which allows us to calculate the erro r rate duri ng t he learn ing process. Sect ion 17 discusses the rela t ionship bet ween rule ext ract ion in general and assoc iat ive memo ry in particular . In sect ion 18, we arg ue that when special informat ion is availabl e, such as infor mation about the symmetry, geomet ry, or topology of the task at hand, the netwo rk must be provided this information. We also discuss various ways in which this informat ion can be ""programmed"" into t he net wor k. Section 19 dr aws the analogy between th e family of functions t hat can be implemented by networks with limited amounts of resour ces and other families of funct ions such as polynomials of limited degree. App endix A contains detai ls of th e condit ions under which our data was taken. Large Automa.tic Learning) Rule Extraction, and Generaliza.tion 879 2. Why lea r n from examples? Automa t ic learning from exa mples is a top ic of enormo us importan ce. There are many application s where there is no ot her way to approach the task. For example, consider th e problem of recognizing hand-wri t ten characters. The raw image can be fed to a preprocessor that will detect salient fea tures such as straight line segments, arcs, terminations, et c., in various parts of the field. But what then? Th ere is no mathematical expression t hat will tell you what features correspo nd to a ""7"" or a ""Q"" . The task is defined purely by th e statist ics of what features convent iona lly go with what meaningt here is no ot her definition. T here is no way to prog ram it ; the solut ion must be learned by examp les [6,11]. Another example is the task of producing the correct pronunciation of a segment of written English . There are pattern s and rules of pron unciation , but th ey are so complex that a network th at could ""discover t he rules"" on its own would save an enormous amount of labor [37J. Another example concerns clinical medicine: t he task of mapping a set of symptoms onto a diagnosis. Here t he inputs have physical meaningth ey are not purely convent iona l as in the previous exa mplesbut we are st ill a long way from writing down an equat ion or a computer program that will perform the task a priori. We must learn from the statist ics of past exa mp les (41). Other examples include classifying sonar returns [10], recogni zing speech [5,16,30,23], and predi cting the secondary st ruct ure of proteins from the primary sequence [42]. In th e foregoing examples, t here was rea lly no alte rnat ive to learni ng from exa mples. However, in order to learn more about the power and limit ations of var ious learnin g methods and evaluate new methods as they are prop osed , people have st udied a number of ""test cases"" where t here was an alternativeth at is, where the ""correct"" solut ion was well understood. T hese includ e classifying input pattern s accord ing to th eir parity [33], geometric shape [33,35], or spatial symmetry [36J. 3. Example : tvo-or-more clumps Th e tes t case that we will use throughout t his pap er is a simple geometric task which an adaptive network ought to be able to handle. Th e network's inp ut pattern s will be Nbit binary st rings. Somet imes we will tr ea t the pattern s as numbers, so we can speak of numerical order ; somet imes we will also treat them as one-dimensional images, in which false bits (Fs) repr esent white pixels and true bits (Ts) rep resent black pixels. A cont iguous clump of T s represents a solid black bar . We th en choose the following rule to determine th e desired output of the network, as shown in table 1: if the inpu t pattern is such that all the T s appear in one cont iguous clump , th en th e output should be F , and if there are two or more dumps, th en t he 880 Denker, Schwartz, Wittner, Solla, Howard, Jackel, and Hopfield Input pattern Outpu t Interpretation ffft ttffff F 1 clump fffttftfff T 2 clumps ftt ttttttt F 1 clump tttffttff t T 3 clumps ffffffffff F no clumps Tabl e 1: Exa mples of the t woor-more clumps predicate. output should be T . We call this t he two-or-more clumps predicate.1 We will consider numerous variat ions of t his problem, such as three-versus -two clumps and so for t h. The one-versus-two clumps version is a lso known as t he contiguity predi cate [25]. Questions of connectedness have played an importan t role in the history of network s and automatic learning: Minsky a nd P ap er t devoted a sizable por t ion of t hei r book [27] to this sort of qu est ion. There a re a host of important questions that immedi a tely a rise, some of whi ch are list ed below. In some cases , we give summary answe rs ; the details of t he an swers will be given in following sections . Ca n any network of t he type we are con sid ering actua lly rep resen t such a fu nct ion? (Yes.) This is not a t rivial resu lt , since Minsky and Paper t [27J showed that a Perce ptron (with one layer of adjustable weight s) absolutely could not perform a wide class of functions, and our fun ction is in th is class. Can it perform the funct ion efficient ly? (Yes .) This is in cont ras t, say, to a solut ion of the par ity function usin g a standard programmable logic array (PLA) [26], which is possibl e but requires enormo us numbers of hardware components (O(2N ) gates). Can the net work learn to perform this function , by learn ing from examples? (Yes.) How qui ckly can it learn it ? (It de pen ds; see below.) How many layers are required , an d how many hidden units in eac h layer? How do t he answers to t he prev ious ques t ions de pen d on t he architecture (i.e. size an d shape) of th e network? How sensit ive a re the resul t s to t he num erical me t hods and other details of the implementation , such as t he an alog represe ntation of T and F, ""moment um term s"" , ""weight decay te rms"" , step size, et c.? Does t he solut ion (i. e. the configuration of weights) t hat the net work find s make sense? Is it s imilar to the solut ions t hat human s would choose , given t he task of designing"
7c8293e7054230cc6cc6e3172f761d89d267f7a7,"Learning algorithms have been used both on feed-forward deterministic networks and on feed-back statistical networks to capture input-output relations and do pattern classification. These learning algorithms are examined for a class of problems characterized by noisy or statistical data, in which the networks learn the relation between input data and probability distributions of answers. In simple but nontrivial networks the two learning rules are closely related. Under some circumstances the learning problem for the statistical networks can be solved without Monte Carlo procedures. The usual arbitrary learning goals of feed-forward networks can be given useful probabilistic meaning."
ac11d05f49fb9eaaf3a66468f0892d8963120770,"This paper develops a particular error correction code which can be effectively decoded by a relatively simple neural network. In high noise situations, this code is comparable to that used at present in deep space communications. The neural decoder has N! stable states with only N2 neurons, and can quickly extract information from analog noise. This example illustrates the effectiveness of neural networks in solving real problems when the problem can be cast in such a fashion that it fits gracefully on the network."
ba3f04e497764864e905d986956093e15e75a530,
c1cac007b95b49a69696907216ff93930b8adec2,
cb6ada182746d2f96a0eadaf5437763bb7eeb7c9,
d2783ea26d5ac28ed4929e3205b96c0f2b7f2703,
e5e1f0a0d1c3d4252d1ee70bb09a976f8319c43f,"Learning algorithms havebeenusedbothon feed-forward deterministic networks andonfeed-back statisti- calnetworks tocapture input-output relations anddopattern classification. Theselearning algorithms areexamined fora class ofproblems characterized bynoisy orstatistical data, in which thenetworks learn therelation between input dataand probability distributions ofanswers. Insimple butnontrivial networks thetwolearning rules areclosely related. Under somecircumstances thelearning problem forthestatistical networks canbesolved without MonteCarlo procedures. The usual arbitrary learning goals offeed-forward networks can begiven useful probabilistic meaning. Learning algorithms enable model""neural networks"" toac- quire capabilities intasks suchaspattern recognition orcon- tinuous input-output control. Feed-forward networks ofan- alog units having sigmoid input-output response havebeen studied extensively (1-4). Thesenetworks aremultilayer perceptrons withthetwo-state threshold units oftheoriginal perceptron (5-8) replaced byanalog units having asigmoid response. Another kindofnetwork (9,10)isbased onsym- metrical connections, anenergy function (11), two-state units, andarandom process togenerate astatistical equilib- riumprobability ofbeing invarious states. Itsconnection to thephysics ofacoupled setoftwo-level units inequilibrium withathermal bath(like amagnetic system ofIsing spins withabitrary exchange) ledittobetermed aBoltzmann net- work. These networks appear rather different. Oneisdetermip- istic, theother statistical; oneisdiscrete, theother continu- ous; onehasaone-way flowofinformation (feed-forward) in operation, theother atwo-way flowofinformation (symmet- rical connections). Thelearning algorithms therefore appear quite different, somuchsothat comparisons ofthecomputa- tional effort needed tolearn agiven task forthese twokinds ofnetworks havesometimes beenmade.Thispapershows that variants ofeachofthese twoclasses ofnetworks, adapt- edtoemphasize themeaning oftheactual procedure em- ployed, often haveveryclosely related learning algorithms andproperties. Thisviewfinds useful meaning, intermsof probabilities, foraparameter that hasappeared arbitrary in analog perceptron learning algorithms. Somethree-layer sta- tistical networks canbesolved bygradient descent without thenecessity ofstatistical averaging onacomputer. Task Consider a setofinstances a ofaproblem. Foreachin- stance, input dataconsist ofanalog input values If(k= 1,"
e69606729837aa1d0168c47f812cbccaba09dc83,An analog model neural network that can solve a general problem of recognizing patterns in a time-dependent signal is presented. The networks use a patterned set of delays to collectively focus stimulus sequence information to a neural state at a future time. The computational capabilities of the circuit are demonstrated on tasks somewhat similar to those necessary for the recognition of words in a continuous stream of speech. The network architecture can be understood from consideration of an energy function that is being minimized as the circuit computes. Neurobiological mechanisms are known for the generation of appropriate delays.
fd86cb9849f862f487c91f2c47feb54300423b62,"A model is presented for electron tunneling in proteins which allows the donor–acceptor interaction to be mediated by the covalent bonds between amino acids and noncovalent contacts between amino acid chains. The important tunneling pathways are predicted to include mostly bonded groups with less favorable nonbonded interactions being important when the through bond pathway is prohibitively long. In some cases, vibrational motion of nonbonded groups along the tunneling pathway strongly inluences the temperature dependence of the rate. Quantitative estimates for the sizes of these noncovalent interactions are made and their role in protein mediated electron transport is discussed."
4517ca6110dfb5de3f004f67653de3c33b8d6234,A new conceptual framework and a minimization principle together provide an understanding of computation in model neural circuits. The circuits consist of nonlinear graded-response model neurons organized into networks with effectively symmetric synaptic connections. The neurons represent an approximation to biological neurons in which a simplified set of important computational properties is retained. Complex circuits solving problems similar to those essential in biology can be analyzed and understood without the need to follow the circuit dynamics in detail. Implementation of the model with electronic devices will provide a class of electronic circuits of novel form and function.
45c2062e9c0391d855a81f3afff23bfecbc272aa,
7ba2b589d774ca6acbb52f3280e821caaaa8227f,
8a476494c556a3bcb20c7843c5667991adc41daf,"A simple model is presented for long distance electron transfer through a bridging medium. Assumptions about the bridge mediated interaction, inherent in many other models, are shown to be limits of the more general problem. The relative importance of through bond and through space coupling is discussed."
c2dd50aa7f12d7178b5aea41463428136049950b,
de04c95df1f22fc0da15831071e0561764d211ac,"Fluorescence lifetimes were determined for porphyrins linked to quinones at two different fixed distances by bicyclo[2.2.2]octyl spacers. Electron-transfer rate constants for the one- and two-spacer homologues were calculated from the lifetimes. The incremental effect of the second spacer is to decrease the electron-transfer rate by at least a factor of 500-1600 depending 
on solvent. Lifetime measurements in solvents of different polarities indicate that the electron-transfer rate decreases with increasing solvent polarity. Experiments at low temperature (77 K) revealed fast nearly temperature-independent electron transfer characterized by nonexponential decays in the fluorescence profiles of the one-spacer compound, in contrast to essentially 
monoexponential decays at room temperature. Low temperatures appear to freeze out the rotational motion of the chromophores and the observed fluorescence decays may be explained as electron transfer from an ensemble of rotational conformations."
24b9eebe49cf7e00cf50cf7b7d9243386a23fe7c,"A model for a large network of ""neurons"" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological ""neurons."" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed."
2d49d6e1e0d0ae667d2eb0ea9994cfa0a7383241,Quantum mechanical and semiclassical formulations of nonadiabatic electron transfer theory are usually implemented within a Born–Oppenheimer regime. Calculations on real weakly interacting systems are so difficult that this approximation is rarely questioned. The Born–Oppenheimer approximation becomes qualitatively wrong for electron transfers at very large distances. A model vibronic problem is exactly solved and compared with the Born–Oppenheimer result. Rate expressions are derived from the wave functions using the ‘‘golden rule’’ approximation. Electron propagation is intimately correlated to nuclear motion so that the vibrational energy left on the donor critically affects the electronic decay length. Several deviations from the usual predictions appear for transfers over very large distances.
791c4cc3cade9b8eb1598bb4f144e2e3c41278a5,"We report the syntheses of three rigidly linked side-by-side porphyrinquinone molecules 2-4 whose edge-to-edge distances are fixed at 6, 10, and 14 A (Figure 1). The spacer units bicyclo[2.2.2]octane and bibicyclo[2.2.2]octane prevent translational 
displacement and only allow rotational freedom. Our preliminary studies of the fluorescence yields of both the free base PLQ 2a-4a and Zn PLQ 2b-4b systems, relative to 1, a similar porphyrin not equipped with a quinone, reveal a distance dependence. This suggests that 2-4 will be useful systems for studying the incremental effect of distance on photochemical electron transfer from excited-state porphyrins to quinones."
8e9867aaf433c6ae665d25ec13d94302a9ab6cec,"A modelforalarge network of""neurons"" withagraded response (orsigmoid input-output relation) is studied. Thisdeterministic system hascollective properties in veryclose correspondence withtheearlier stochastic model based onMcCulloch-Pitts neurons. Thecontent-addressable memoryandother emergent collective properties oftheorigi- nalmodelalso arepresent inthegraded response model. The idea that suchcollective properties areusedinbiological sys- temsisgiven addedcredence bythecontinued presence ofsuch properties formorenearly biological ""neurons."" Collective analog electrical circuits ofthekinddescribed will certainly function. Thecollective states ofthetwomodels haveasimple correspondence. Theoriginal modelwill continue tobeuseful forsimulations, because its connection tograded response sys- temsisestablished. Equations that include theeffect ofaction potentials inthegraded response system arealsodeveloped. Recent papers (1-3) haveexplored theability ofasystem of highly interconnected ""neurons"" tohaveuseful collective computational properties. Theseproperties emergesponta- neously inasystem having alarge numberofelementary ""neurons."" Content-addressable memory(CAM)isoneof thesimplest collective properties ofsucha system. The mathematical modeling hasbeenbasedon""neurons"" that aredifferent bothfromreal biological neurons andfromthe realistic functioning ofsimple electronic circuits. Someof these differences aremajor enough thatneurobiologists and circuit engineers alike havequestioned whether real neural orelectrical circuits wouldactually exhibit thekindofbe- haviors foundinthemodelsystem evenifthe""neurons"" wereconnected inthefashion envisioned. Twomajor divergences between themodelandbiological orphysical systems stand out.Realneurons (andreal physi- caldevices suchasoperational amplifiers that might mimic them) havecontinuous input-output relations. (Action po- tentials areomitted until Discussion.) Theoriginal modeling usedtwo-state McCulloch-Pitts (4)threshold devices having outputs of0or1only. Realneurons andreal physical circuits haveintegrative timedelays duetocapacitance, andthetime evolution ofthestate ofsuchsystems should berepresented byadifferential equation (perhaps withaddednoise). The original modeling usedastochastic algorithm involving sud- den0-1or1-0changes ofstates ofneurons atrandom times. Thispaper showsthat theimportant properties oftheorigi- nalmodelremain intact whenthese twosimplifications of themodeling areeliminated. Although itisuncertain wheth- ertheproperties ofthese newcontinuous ""neurons"" areyet close enough totheessential properties ofrealneurons (and/or their dendritic arborization) tobedirectly applicable toneurobiology, amajor conceptual obstacle hasbeenelimi- nated. Itiscertain that aCAM constructed onthebasic ideas"
62bb2ff635b210f63b1fdbd91d5341005470530e,"Working Memory Binding (WMB) entails the integration of multiple sources of information to form and temporarily store unique representations. Information can be processed through either one (i.e., Unimodal WMB) or separate sensory modalities (i.e., Crossmodal WMB).


OBJECTIVE
In this study, we investigated whether Crossmodal WMB is differentially affected by normal or pathological aging compared to Unimodal WMB.


METHOD
Experiment 1: 26 older and 26 younger adults recalled the target feature matching the test probe to complete a previously displayed color-shape binding (visually presented in the Unimodal condition; auditorily and visually presented in the Crossmodal condition). Experiment 2: 35 older and 35 younger adults undertook the same paradigm while carrying out articulatory suppression to limit verbal recoding. Experiment 3: 24 Alzheimer's disease (AD) patients and two groups of 24 healthy matched controls (tested respectively with the same and an increased memory load compared to the patients) were recruited to perform a similar task.


RESULTS
Results show no age-related additional cost in Crossmodal WMB in respect to Unimodal WMB. AD patients had poor attainment in both WMB tasks regardless of specific binding condition.


CONCLUSION
These findings provide evidence identifying WMB per se to be impaired in AD, regardless of the type of to-be-bound material. This supports the view that WMB is a suitable cognitive marker for AD. (PsycINFO Database Record (c) 2020 APA, all rights reserved)."
a36e7dd882f608a7e2c5b93ee146bc47fc016093,
dc9903b85a5f861157794b4d96f0930bf05e27f3,"Modele semi-empirique pour prevoir la variation de la vitesse de transfert electronique, assistee par photons, avec la distance entre groupes redox, relies par un motif moleculaire rigide volumineux. Bon accord des previsions avec la decroissance observee de l'element matriciel d'effet tunnel optique avec la distance pour les complexes de Ru a coordinat dithiaspins et valence mixte de Stein-Lewis-Seitz-Taube"
fbfaf60a72f4e12bc3ea73e518e44a4f82c56edf,"Studies of redox reactions between the blue copper protein azurin and inorganic reagents have suggested the formation of discrete complexes between the reaction partners prior to the actual electron-transfer step. To get an insight into the structural nature of the complexes formed, we have studied the interaction (i) between oxidized Pseudomonas aeruginosa azurin and the anion Fe(CN)6(3-) and (ii) between reduced azurin and Cr(CN)6(3-). At low ionic strengths, stoichiometric binding of one Fe(CN)6(3-) ion to the oxidized protein is observed. In the high-resolution proton magnetic resonance spectra of the reduced protein, specific broadening of the assigned residues is observed upon titration with the redox-inert Cr(CN)6(3-) ion. Analysis of this paramagnetic spectral broadening in terms of the three-dimensional structure of the protein has led to the proposal that the binding site of the anions lies approximately midway between lysine residues 85 and 92. Evidence in support of this conclusion is provided by parallel studies on Alcaligenes faecalis azurin, which lacks these lysine residues. A similar site on the surface of Pseudomonas azurin has recently been identified by affinity labeling with chromous ions as an electron-transfer locus [ Farver , O., & Pecht , I. (1981) Isr . J. Chem. 21, 13-17]. The results presented here suggest that this region on the protein surface also may be employed by anionic electron-transfer agents."
6d66407ae982930953fed521eec1eb5b1503b972,"Intramolecular reactions inside macromolecules (e.g., binding of ligands to iron inside heme proteins) may often be coupled to slow random fluctuations in the reaction center geometry. This motion is ‘‘perpendicular’’ to the reaction coordinate. It can be described as bounded diffusion in the presence of a binding potential field and an intramolecular rate constant which depends on the perpendicular degree of freedom. The diffusion equation is solved under the appropriate reflective boundary conditions. The transient decay of the total population is multiexponential (power law) for small diffusivity, changing to monoexponential kinetics for large diffusivity. For large times or large diffusivity, direct integration is very tedious, but an eigenvalue expansion converges rapidly. It also allows the calculation of the ‘‘average survival time’’ (an extension of the ‘‘first passage time’’) a natural candidate for replacing the reciprocal rate constant in multiexponential kinetics. An example is given for electron transfer between two loosely bound sites in a macromolecule. The average survival time shows a non‐Kramers dependence on diffusivity, of the type found in the binding kinetics in heme proteins."
786b660e60069ecf01673ade33e287d6adb022b7,
c44c253062c88449b19e14b198b13970171022bf,"A model for the dependence of the potential energy barrier on a ‘‘protein coordinate’’ is constructed. It is based on a two dimensional potential energy surface having as variables the CO–iron distance and a conceptual protein coordinate. The distribution of barrier heights observed in kinetics follows from an initial Boltzmann distribution for the protein coordinate. The experimental nonexponential rebinding kinetics at low temperatures or large viscosities (when the protein coordinates can be assumed ‘‘frozen’’) can be fit with a simply parametrized energy surface. Using the same energy surfaces and the theory of bounded diffusion perpendicular to the reaction coordinate, we generate (in qualitative agreement with experiment) the survival probability curves for larger diffusivity, when the constraint on the protein coordinate is relaxed. On the basis of our results, the outcomes of new experiments which examine the concepts underlying the theory can be predicted."
15526b4b249d7855b265576e1f59e3a79d590e1f,"Using high-resolution 13C nuclear magnetic resonance, we examined the mobilization of endogenous trehalose in suspensions of yeast asci. Sporulation of yeast cells in [1-13C]acetate resulted in incorporation of label into the C-3 and C-4 positions of trehalose within the asci. During germination of these asci with [1-13C]glucose, the consumption of both endogenous trehalose and exogenous glucose were followed simultaneously by 13C nuclear magnetic resonance, as was the formation of glycerol and ethanol, their glycolytic and products. Time courses for carbohydrate consumption indicated that trehalose, although it decreased to 25% of its initial value upon germination, was not preferentially catabolized and did not provide the primary energy supply for germination with glucose. The ratio of trehalose to glucose catabolized was 0.09. Exogenous glucose levels appeared to regulate trehalose mobilization since trehalose was only consumed when sufficiently high levels (more than 2 mM) of glucose were present. Upon glucose depletion newly synthesized [1-13C]trehalose was observed. Nuclear magnetic resonance spectra of extracts confirmed the trehalose peak assignments and showed products of [1-13C]glucose catabolism. In addition by quantitating trehalose consumption and 2-deoxyglucose incorporation in dormant yeast asci, we found that 3.8 +/- 0.l4 molecules of 2-deoxyglucose were incorporated for each trehalose molecule consumed. Trehalose can therefore function as a carbohydrate source for ATP formation during dormancy."
2775ba0839f32f90528753b902db5660cca7a852,"Although brain and computer have similar abilities to store and retrieve memories, they go about performing these tasks in different ways."
388e813d325d1dc928862472455572e2f2cc1c16,
438311bf7159e494a68b25598d948e05a022a5f5,
88d9c82d58c4020a252d19662a02c5064e77e1dd,
98b4d4e24aab57ab4e1124ff8106909050645cfa,"Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
a2d47dee69636f5336275071ecb354c0b00adbb6,
a559e6afcaa91ba3f4e67134b057af6b1dd206a8,
6a476779f45e4fe1b67814917ed3969cd8ee0b98,"The effect of EF-Tu.GTP on the codon-anticodon interaction of AA-tRNA was studied by using as a model system the interaction of AA-tRNAs with complementary anticodons, namely, dimerization between yeast or Escherichia coli Phe-tRNAPhe (anticodon GmAA) and E. coli Glu-tRNAGlu (anticodon s2UUC) or nonacylated tRNAGlu in the presence or absence of EF-Tu.GTP. The present data indicate that the ternary complexes Phe-tRNA-EF-Tu.GTP and Glu-tRNA-EF-Tu.GTP can form dimers with a binding constant of (0.9 +/- 0.2) X 10(6) M-1, which is identical in magnitude with that of the dimer of the nonacylated tRNAs tRNAPhe-tRNAGlu and also with that of the complex Phe-tRNA-EF-Tu.GTP with nonacylated tRNAGlu. These results show that the anticodon region is not affected by complexation with EF-Tu.GTP; however, this conclusion does not preclude the possibility of structural changes in the anticodon loop that have no effect in energetic terms. In addition, this model codon-anticodon interaction does not stimulate hydrolysis of the GTP bound in the ternary complex."
8976beba14f1b546386d8c834f9384063e8c5484,A magnetic-field-induced optical linear dichroism has been observed in photolyzed $\ensuremath{\beta}$-hemoglobin at temperatures below 20 K. Nonadiabatic reaction rate theory for spinforbidden processes predicts a magnetic modulation of the tunneling rate which may account for the signal observed. The effect observed should be sensitive to crystal-field energy-level splittings for the iron atom in the heme group.
b688acac8ffdbea207bdd1069c8b768fcd10225d,"D-Tyr-tRNA can take part in peptide bond formation with N-AcPhe-tRNA on ribosomes programmed with the hexanucleotide UUUUAC. None of the steps leading to peptide bond formation exhibit high stereoselectivity. Ternary complex formation with EF-Tu.GTP favors L-Tyr-tRNA by a factor greater than 25. The complex formed with D-Tyr-tRNA was not protected from hydrolysis, which suggests that the D-amino acid is improperly bound to the protein. The rate of EF-Tu-promoted dipeptide formation was 30-fold faster with L-Tyr-tRNA. The ratio of moles of GTP hydrolyzed to dipeptide formed was 1.4 for L-Tyr-tRNA and 4 for D-Tyr-tRNA. The excess of GTP hydrolyzed to peptide bonds formed is evidence for kinetic proofreading in AA-tRNA selection. The combined effects of the partial discrimination at each stage, from the aminoacylation to the peptide formation, favor L-tyrosine by a factor greater than 10(4) and would virtually exclude D-tyrosine from being incorporated under conditions where L-tyrosine was also present."
ba79279eade5d1585cc51371800d839b39cadac6,"Escherichia coli tRNAPhe (anticodon GAA) as well as yeast tRNAPhe (anticodon GmAA) forms a strong complex with E. coli tRNAGlu (anticodon s2UUC) through an interaction between their complementary anticodons. This interaction inhibits aminocylation of tRNAPhe but not the formation of a complex with elongation factor Tu. Moreover, at 0 degrees C, tRNAGlu strongly inhibits the binding of Phe-tRNA to poly(U)-programmed ribosomes via either the enzymic (EF-Tu-promoted) or nonenzymic pathway. At 15 degrees C, tRNAGlu effectively inhibits polyphenylanine synthesis in the E. coli system. The inhibition is reversed at 37 degrees C, where the Phe-tRNA.tRNAGlu dimer is dissociated. Calculations based upon the E. coli intracellular concentrations of tRNAs and the published rates of association and dissociation of the tRNA dimers suggest that this interaction may inhibit protein synthesis in vivo at temperatures below 15 degrees C."
fcdc7a8442f94114923dcf726ace447efdb73b54,
db0b909610a19c58fd35b7c921145797fae5bc41,"A mechanism for proofreading biosynthetic processes requiring high accuracy is described. The previously understood ""kinetic proofreading"" mechanism of enhancing accuracy has distinguishing characteristics such as the non-stoichiometric use of substrate or cosubstrate that have allowed its identification in aspects of DNA and protein synthesis. The proofreading scheme developed here, though generically related, lacks all the previous identifying features. A DNA polymerase proofreading in this manner need neither generate dNMP nor have a 3' leads to 5' exonuclease activity. Protein synthesis could be proofread even with stoichiometric GTP consumption or without elongation factor Tu . GTP. The kinetic scheme that generates this proofreading makes use of an ""energy relay"" from previous substrate molecules and is a representative of a class of nonequilibrium processes displaying dynamic cooperativity. This proofreading mechanism has its own identifying characteristics, which are sufficiently subtle that they would have generally escaped notice or defied interpretation."
ebe9f39df14f0b679fd99afba358498c7d7202eb,A simple quantum mechanical theory of electron tunneling appropriate for biological electron transfers is developed. The theory predicts discrepancies between optical and thermal tunneling data and clarifies how fast charge separation is produced in photosynthesis. Barrier heights and intermolecular separations are determined for a variety of biomolecular electron transfer experiments.
449a8ff9ff7993a0959d02ca79b704d6853d13c5,"The molecular mechanism of hemoglobin cooperativity was studied kinetically by flash photolysis on mixed-state hemoglobins which consist of three ferrous carboxy subunits and one hybrid ferric subunit including fluoromet, azidomet, cyanatomet, and thiocyanatomet. The effects of conformational transitions on the hybrid subunit were detected by kinetic absorption spectroscopy after the CO was fully photodissociated from the binding sites by a large pulse of light from a tunable dye laser. The hemoglobin conformational transition rate was observed to depend on its state of ligation. At 22 degrees C, pH 7, and 0.1 M phosphate, the deoxy R yields T conformational change rate is 4 x 10(4)s-1. The rate decreases to 1.4 x 10(4)s-1 for singly ligated hemoglobin. The R yields T conformation change alters the energy separation between high- and low-spin states for azidomet, cyanatomet, and thiocyanatomet subunits by about 700, 300, and 300 cal/mol, respectively. There are two possible implications of this result: (1) the iron atom spin state is not the only major factor in the determination of its position with respect to the heme plane or (2) the change with conformation of the protein force exerted by the proximal histidine on the iron atom (for an iron to heme-plane displacement of less than 0.3 A) is less than 50% of that expected from simple models in which this motion is responsible for cooperativity."
9d300dd1fe2d6ad15938d11fc5eb04bdd1b59c35,
78d71397ba4cf05f4e32bef1b665bb67422f2120,"Wehypothesize that theorigin ofthegenetic codeisassociated withthestructure ofthetRNAthat existed inprimal cells. Thesequences ofmodern tRNAcontain corre- lations whichcanbeunderstood as""fossil"" evidence ofthe secondary structure ofprimal tRNA.Kinetic proofreading through diffusion canamplify alowlevel ofintrinsic selectivity oftRNAforits aminoacid. Experimental tests ofthetheory are suggested. Whatistheorigin oftheparticular genetic code existing now? All life knowntoday makes useofasingle language fortrans- lating mRNAinto proteins, inwhich each three-letter ""triplet"" ofaprotein-coding sequence ofmRNAcorresponds toapar- ticular amino acid orstop instruction. Yetknowing thecode tells uslittle about thereason this particular genetic code exists rather than someother. Speculation about theorigin ofthegenetic code began before thecode wasdeciphered (1). Atoneextreme ofthese speculations istheidea that thecode hasasits basis a"
8c9e9e4e8caf7d788b3a14c50b2881f4cb21c115,"We hypothesize that the origin of the genetic code is associated with the structure of the tRNA that existed in primal cells. The sequences of modern tRNA contain correlations which can be understood as ""fossil"" evidence of the secondary structure of primal tRNA. Kinetic proofreading through diffusion can amplify a low level of intrinsic selectivity of tRNA for its amino acid. Experimental tests of the theory are suggested."
0c1bf4c145c62ff0c1bdc5ac80e94b237e7f3e6b,"The enzymatic aminoacylation of tRNA can be viewed as a means of proofreading either the amino acid or the tRNA or both. We have conducted further experimental tests of kinetic proofreading in discriminating between cognate and noncognate amino acids and tRNAs as follows: (formula: see text). In cases (i) and (ii) the amino acids are proofread, in cases (iii) and (iv) the tRNA is proofread, and in case (v), both the amino acid and the tRNA are proofread. ATP consumed per acylation was 400, 1.5, 40, 25, and 1000, respectively. High ATP/aminoacylation ratios are diagnostic for kinetic proofreading."
6af8ecd30600ec0bd276813825d8edece51e4217,"Evidence for the constructs central to vibronically coupled electron transfer has been obtained. Our experiments show the existence of a weak (f congruent to 10(-6)) charge-transfer absorption band in the near infrared for the bound donor-acceptor complex, cytochrome c-Fe(CN)6. Such a charge-transfer band had been predicted from the theory of such transfers. The experimental method, using a form of excitation modulation spectroscopy, measures only the optical absorption that induces charge transfer between the donor and the acceptor (and does not detect other absorptions) and allows the study of charge-transfer bands whose absorbances are small compared to the sample absorbance. The energy position and oscillator strength of the band agree with the general predictions of this vibronically coupled tunneling theory. We suggest that, in this system at room temperature, the electron transfer can be described by this tunneling theory. This model system result gives credence to the short electron transfer distances the theory has predicted for biological electron transfers."
8dff1b3560f85421f5b55e939520cf6e363672f4,"A detailed investigation of the applicability of the vibronically coupled electron tunneling theory in biomolecules can be made by a quantitative study of a weak charge-transfer optical absorption band that has been predicted by this theory. The measurement of the position, width, and molar extinction coefficient of this band is examined in the bound model system cytochrome c-Fe(CN)6 at room temperature and demonstrates that the theory is quantitatively applicable in this system. The size of the parameters measured is typical of those relevant for biological electron transfers. The comparisons lend credibility to the generality of vibronically coupled electron theory in biomolecules and its short transfer distances."
9279e322314dd3c50071b18a1011c4bc9f795b6e,
cef5fda2efcad4429da04c17f62135a3cad077a4,
fe913adf7762c1acaaedf5c4a658831ecfe65b12,"The usefulness of optical measurements in the 1 – 20μ region in the understanding of electron‐phonon coupling in transition metals is discussed. There are many possible barriers in the way of exact simple relations of optical constants to superconductivity, and some of these are described. In many transition metals the relaxation rate has the form α + βω2. Possible origins of the term are given, of which the two most interesting are the rapid variation of parameters near the Fermi energy, and electron‐electron collisions. It is shown that the measured Baber resistivity term in palladium is in qualitative agreement with this interpretation of the βω2 optical relaxation rate in palladium."
02b6268413128b46933b6f5c3b618bdab279c6e3,"The Mahan, Nozi\`eres, De Dominicis theory of x-ray thresholds is extended to include exchange effects which are found to reduce threshold exponents and increase the orthogonality index. For the lithium $K$ edge this represents an important previously neglected feature. Results of detailed numerical calculations of threshold exponents for Li are presented which differ considerably from previous values and which are in better agreement with edge data."
150aca4fe2b362d2bc25f49c4a45488162ae61b0,"Using a novel technique of modulated photo-dissociation of carbon monoxide from hemoglobin, we have obtained the rates for conversion between the two quaternary states, R, and T, at 3-fold ligation. Our measurements at pH 7 and 22 degrees give rates of 780 +/- 40 sec-1 for going from R to T, and 2500 +/- 200 sec-1 from T to R. This yields an equilibrium constant of 0.31 +/- 0.04, which is in good agreement with previous estimates. The degree of agreement between this equilibrium constant and that predicted from the allosteric model provides a new, quantitative test of the allosteric description. A sequential model for the change in structure was found incompatible with the data, even if kinetic subunit inequivalence was assumed. The technique described here is quite general and can be used as long as the system under investigation can be repetitively excited in a regime in which it responds linearly to the excitation."
409f8d744845726fa44b6c0c596ba455a815e03f,"Kinetic proofreading isareaction scheme withastructure morecomplicated thanthat ofMichaelis ki- netics, which leads toaproofreading forerrors intherecog- nition ofacorrect substrate byanenzyme. Wehavemea- sured thestoichiometry between ATPhydrolysis andtRNAIle charging, using theenzyme isoleucyl-tRNA synthetase (L-iso- leucine:tRNAIe ligase (AMP-forming), EC6.1.1.5) andthe aminoacids isoleucine (correct) andvaline (incorrect). The enzymatic deacylation ofcharged tRNA,whichwouldnor- mally prevent meaningful stoichiometry studies, waselimi- nated bytheuseoftransfer factor Tu'GTP, (which binds strongly tocharged tRNA) inthereaction mixture. Forisoleu- cine, 1.5ATPmolecules arehydrolyzed pertRNAcharged, butforvaline, 270. These stoichiometry ratios arefundamen- taltokinetic proofreading, fortheenergy coupling isessen- tial andproofreading isobtained only bydeparting from1:1 stoichiometry between energy coupling andproduct forma- tion. Within theknownreaction pathway, these ratios dem- onstrate that kinetic proofreading induces areduction iner- rors byafactor of1/180. Anoverall error rate ofabout 10-4 fortRNAcharging isobtained byakinetic proofreading using afundamental discrimination level ofabout 10-2, and iscompatible withthelowinvivo error rate ofprotein syn- thesis. Manybiochemical reactions, particularly those associated"
6a784ac8030161d8ad3b9a0fce1668a71cfc8649,
e1b8377f70b6183d311b3ca6d0bc7ae9debcce9c,"Kinetic proofreading is a reaction scheme with a structure more complicated than that of Michaelis kinetics, which leads to a proofreading for errors in the recognition of a correct substrate by an enzyme. We have measured the stoichiometry between ATP hydrolysis and tRNAIle charging, using the enzyme isoleucyl-tRNA synthetase [L-isoleucine:tRNAIle ligase (AMP-forming), EC 6.1.1.5] and the amino acids isoleucine (correct) and valine (incorrect). The enzymatic deacylation of charged tRNA, which would normally prevent meaningful stoichiometry studies, was eliminated by the use of transfer factor Tu-GTP, (which binds strongly to charged tRNA) in the reaction mixture. For isoleucine, 1.5 ATP molecules are hydrolyzed per tRNA charged, but for valine, 270. These stoichiometry ratios are fundamental to kinetic proofreading, for the energy coupling is essential and proofreading is obtained only by departing from 1:1 stoichiometry between energy coupling and product formation. Within the known reaction pathway, these ratios demonstrate that kinetic proofreading induces a reduction in errors by a factor of 1/180. An overall error rate of about 10(-4) for tRNA charging is obtained by a kinetic proofreading using a fundamental discrimination level of about 10(-2), and is compatible with the low in vivo error rate of protein synthesis."
54ba8f5e8d4d2f05906d4d8e61e7b9c1971df09a,"Magnetic circular dichroism (MCD) and absorption have been measured in various derivatives of hemoglobin in an applied field of 16 kG at temperatures ranging from 77 to 294 °K. The visible and Soret bands have been studied. We develop a spin–orbit coupling model to help explain the sign, magnitude, and 1/T temperature dependence of the MCD of deoxyhemoglobin. The results for deoxyhemoglobin are compared with those for unligated separated alpha and beta chains. Differences in spectra occur which can only be attributed to the different temperature dependent heme environments for chains and for tetramers. Cyanomethemoglobin and oxyhemoglobin are also discussed."
6d81efa90199823e06fb93f15c35c3ff83176a4b,
9c05e4203a0a9c43adb963607e82e435eef9852d,"It is our purpose to review recent experiments on haemoglobin in order to discuss them in terms of the two state model of cooperativity. Excellent previous reviews are available of the chemistry of haemoglobin (Antonini & Brunori, 1971; Gibson, 1959b) which are referred to when possible. The plethora of data necessitates that a selection must be made in a review. An intentionally wide range of experiments is selected to exhibit"
1353f199e8134e192d92ea87d6040f40eb01cf3f,"We report the construction of a system to measure the circular dichroism of transient phenomena. The technique employs rapid modulation of the polarization of an analyzing beam, and consequently significant information can be obtained in a single run. The system is sensitive to changes in polarization strength of 2×10−5 occurring in 1 msec. The particular system described has been constructed to follow transient structure changes in hemoglobin following laser photolysis of CO hemoglobin. The technique is quite general, however, and design details are presented which would be useful in any attempt to apply modulation spectroscopy to transient measurements."
657591945c8ce123226f729713b7ecb8182df103,"The specificity with which the genetic code is read in protein synthesis, and with which other highly specific biosynthetic reactions take place, can be increased above the level available from free energy differences in intermediates or kinetic barriers by a process defined here as kinetic proofreading. A simple kinetic pathway is described which results in this proofreading when the reaction is strongly but nonspecifically driven, e.g., by phosphate hydrolysis. Protein synthesis, amino acid recognition, and DNA replication, all exhibit the features of this model. In each case, known reactions which otherwise appear to be useless or deleterious complications are seen to be essential to the proofreading function."
9f3505f56d4f91e9ac8f55a27b7d968eb5256a08,
cba04f5cdaaa98e4ae27420befa50142ee9971f8,"A theory of electron transfer between two fixed sites by tunneling is developed. Vibronic coupling in the individual molecules produces an activation energy to transfer at high temperatures, and temperature-independent tunneling (when energetically allowed) at low temperature. The model is compared with known results on electron transfer in Chromatium and in Rhodopseudomonas spheroides. It quantitatively interprets these results, with parameters whose scale is verified by comparison with optical absorption spectra. According to this description, the separation between linking sites for electron transfer is 8-10 A in Chromatium, far smaller than earlier estimates."
dec10843df514b4f55cf38c586991a179719d8eb,
d81c042fa44105773cf939c9bad31d6480bb5027,
1a3f0f2484ace9b1d2c3ad366af5e47fec702fdb,
af6199d1ad0ce0422a502c3af8acc94a4fdb5d41,
d5106971aff56386b64088c84f5cac15df3ee26f,
49a6e4eae70c2ef94509eaf1a6b9797b287810bd,
569059648085600ebe57dc6488ad8cc3aaf76ca3,
5fe85916399a5b6d8240b28e6341fb5e340f8f8f,
672e4ff12c84d2aa914f22d24a6f5c293de05a99,"The potential produced by isoelectronic impurities is investigated and shown to be critically dependent upon screening. Two methods of calculation of the screened potential are used in this paper, one based on a first-principles wave-function approach and the other using the semiempirical theory of energy bands in semiconductors. The relaxation of the host crystal is also taken into account and shown to be important. The results are in satisfactory agreement with experiment."
ae32073027fd56e956bd2ff1282f4ed35d0d94db,
75aaccc42bece9dc5ca264b0d76d570ef618756d,
118ae12267cdc6d6c340e1a55a442e7fd06039c4,
47a622fe1024466ea22b54f47aa15120a646e2ac,
6f47d09f63052bf6eba403ce49d5962408304086,
b0d8ad0bb5d89d0b43544ba15691fe70acd5e0ee,
0089189a16f73a6487f44982929cb0b3d4ed6521,
29f18d31b73c095c910d23824e9c4ba7d13f16f3,"A sum rule is constructed which relates the third moment of the imaginary part of the dielectric function at zero wave vector to an integral of the product of the Laplacian of the crystal potential and the fluctuation of the electron density. This sum rule, though rigorous, will be of real use only when the real solid can be replaced by a solid of pseudoatoms having only valence electrons. A method is found for obtaining an ""experimental"" third moment from other known moments. In this approximation some of the recent empiricism about bonding in crystals can be given a more rigorous basis. A quantitative application of the theorem to the prediction of the dielectric constant of GaAs is sketched."
aa0c2dcb89adbeeff90de180e05a7bba8c3072e4,
140e47713a05723d9ca7448c422e583ad353a45d,
226ca41ffd95daf114c2ac6f5742f287ab3075ec,"To calculate correctly the scattering of light by phonons or impurities in a crystal, the true asymptotic scattering states of the coupled system of crystal plus light (polaritons) should be used. When the light frequency is close to one exciton or optical-phonon frequency, the polariton is entirely excitonlike, and the polariton scattering can, in the Born approximation, be related to exciton scattering properties. If the exciton itself interacts strongly with an imperfection in the crystal, it is not permissible to treat either the exciton scattering or the exciton-photon interaction as perturbations. This problem of resonant scattering of polaritons is solved for short-range exciton-impurity interactions. Radiative damping and spatial dispersion appear in this solution in a natural fashion. Giant oscillator strengths of bound-exciton transitions are likewise automatically obtained. The proper inclusion of radiative damping and spatial dispersion keeps all cross sections finite. The relation between the theory and experiments is briefly discussed."
7786a037155ae47b45c775b09d1c7b9611d94800,"The relation between the systematics of superconductivity in the transition metals and the Periodic Table suggests that the transition temperature is chiefly a short-range or ""chemical"" property. When a local representation of phonons and an angular momentum decomposition of electron wave functions are used, the conventional description of electron-phonon interactions contains chiefly scatterings which change the angular momentum of the electron. This selection rule makes possible the writing of the electron-phonon coupling constant $\ensuremath{\lambda}$ as the quotient of two parameters, each of which is of a chemical nature. This simplification is possible for materials having a high density of $d$ states at the Fermi energy. For such materials, $\ensuremath{\lambda}$ is little affected by the density of states. The theory is compared with the superconducting transition temperatures of transition metals and their alloys."
2d5b045915554ee716034e1bc3ee7d31cb4ea932,
3a305c2e148cb15d20e374e778789c742c2c7728,"A new approach to transport theory in classical gases is described for a hard-sphere gas. The approach is designed to incorporate collective effects (sound waves, mean free path) at an early stage. A pseudopotential approximation to the $N$-body Liouville equation is given. By working only with symmetric distribution functions, a simple analogy to a quantum-mechanical many-body Hamiltonian is possible. The linearized Boltzmann equation is obtained as the ""Hartree"" solution to the single-particle states of this Hamiltonian. Sound waves appear as long-wavelength ""single-particle"" excitations. Collective excitations include the excluded volume correction to the velocity of sound. The ""single-particle"" solutions have the advantage of introducing the appropriate long-range space-time correlations into the basis functions for higher-order approximations. Because of the pseudopotential approximation in the present paper, there are short-range divergences in higher orders of perturbation theory, but no apparent long-wavelength divergences."
3d737f7b5bbfcd4c18e9adb348440609c178cb7b,
59ad04706284a000d709a0d64bb0dbb0eae4432d,"A general formalism for phonon-induced optical absorption in metals is derived. It has been applied to solid sodium. The lattice of sodium is approximated by a single reciprocal-lattice point. Its lattice wave number $G$ is somewhat greater than $2{K}_{F}$, the Fermi wave number. The energy-loss function of such an electron gas corresponding to a wave vector q has been calculated. For intermediate q with direction around G, the response function of a noninteracting electron gas has a gap in the continuum. As a result, in the presence of electron-electron Coulomb interactions, a new collective state and a redistribution of the energy-loss function have been found in the vicinity of the gap. The deviation of the phonon-induced optical absorption of sodium from the Drude theory has been calculated by using this energy-loss function and also taking into account the umklapp process. Conventional calculations have only taken into account the effects of single-particle excitations, and have neglected the collective states and the redistribution of the energy-loss functions, which are essential to the optical absorption. We find our results are qualitatively different from those of conventional calculations. But our results show that the effects are too small to explain the anomaly appearing in the infrared absorption of sodium."
64eea2eec32a9272a56e49dde6b62651419bc7a7,
e1ef1f23b7e4b0f097ff945e0b3e3325ab02c307,
e6449a6212d6e0a6109d4baf1192014ff451bcc1,
ec87f465d7dec7e48629e87b5386540e8f99281c,"The one-exciton, one-magnon system is treated for a suitable antiferromagnetic insulator, Mn${\mathrm{F}}_{2}$. Zone-edge excitons and magnons are found to interact quite strongly. Bound states are found to exist under favorable circumstances. The line shapes of magnon sidebands of excitonic transitions are modified. Expressions are given for the exciton-magnon coupling constants in terms of the parameters describing interionic exchange."
9e71c365d3fdfc8ccb605e278df2e0f6fe43dec8,
c0566e730f4f3e5976aa7a3e6358ab92f723722a,
4efa2e839150d7bc145cef1b4adb03063d5cfde0,
58c44246e3060ce20f4b3f85a7285c9308bb0eff,
55afaa0b7afca02f997e2dd4cd615ca9a1466e5f,
5e3d94272b8a2bf6500f75eaf648e42e7808ae23,
92966eb9997d27802a6ad5aa735d0b03e77040a0,A theoretical model to explain the negative polarization of moonlight at small lunar phase angles is developed. The model is based on the polarization of light in the diffraction region bordering the geometric shadow of an opaque dielectric obstacle.
ae393168948c7c4c2c68cdf71a00b10fb0e2c873,It is reported that a hitherto unexplained optical transition in ZnTe is due to oxygen substituting isoelectronically for Te. A classification of isoelectronic traps into isoelectronic donors and acceptors is made. One striking difference between these two classes is apparent in the phonon sidebands of the $J=1$ and $J=2$ transitions occuring at each center.
028682a168f0c351a8f268755bd3ee65f43e62f6,
1c9f6a0ccdef25149046021110b6ec32a4feb642,
1dc1d41d9fd0d84501873224224797b5488f5382,
312eea05386fadbf893838571d6020acbd759c6d,
3dc445e63311d9910a18aa7bc109057cdaadd4bd,"The measurement of the two-quantum fundamental absorption edge in KI and CsI at 300, 77, and 4\ifmmode^\circ\else\textdegree\fi{}K is described. Selection rules, exciton models, phonon coupling, and the theory of two-quantum absorption are briefly discussed. Comparison of the present results with theory and the one-quantum absorption leads to the quantitative understanding of the shape and magnitude of the two-quantum spectrum. It is shown that one-electron band structure and Wannier excitons, rather than excitation models, provide the best understanding of the low-energy part of the fundamental absorption edges in the materials studied."
96642b8e94cab25de4361b730ec6eedf0af06b3f,
dfc3b100ad282bbe0a7f95737ec5c543bca5d6c7,
3421be203c68164cd50825b7ba474dea0867da7a,"Pleasant fall weather added to the high spirits and congeniality of a group of physicists and chemists attending a symposium on molecular excitons as guests of duPont's Central Research Department. Their common interest in excitons had attracted participants from academic and research institutions across the country, and from Canada as well. Welcoming the visitors, Central Research Director P. L. Salzberg announced that the symposium marked the belated dedication of the site of the meeting, duPont's newly constructed Physical Research Building."
9dcb8850a7756b16ba9889df5626040531250442,
e1df44005f315a165b27202b47893f4273a58b18,
e8d8dc915ba005c171d7088a8fca697b05b320dc,
fad3907910f223cd9d3293bb132a2c937f55cbc2,
29f414c1fb8f392de905e64cc110f072490e867a,"Three sharp lines $A$, $B$, and $C$ are seen in absorption and fluorescence near the band edge of GaP at 20\ifmmode^\circ\else\textdegree\fi{}K and below. The lines are associated with excitons bound to defects. The Zeeman splitting of the lines allow the $g$ values of the holes and electrons to be determined. $A$ and $B$ both arise from an exciton bound to an ionized donor, the energy difference between $A$ and $B$ coming from electron-hole $\mathrm{jj}$ coupling. Both lines cooperate strongly with phonons, and this together with the splitting indicates a tightly bound state, yet it lies very close to the indirect exciton. Consequently $A$, $B$ involves another exciton, probably made from an electron and hole both at k=0. Line $C$ is produced by an indirect exciton bound to a neutral donor, probably sulfur. Despite the possibility of valley-orbit splitting of the conduction band, the spectrum of $C$ is quite simple."
657345adfc8171975e0bbe08f145ce4782cf6d36,"It is shown that the ordinary semiclassical theory of the absorption of light by exciton states is not completely satisfactory (in contrast to the case of absorption due to interband transitions). A more complete theory is developed. It is shown that excitons are approximate bosons, and, in interaction with the electromagnetic field, the exciton field plays the role of the classical polarization field. The eigenstates of the system of crystal and radiation field are mixtures of photons and excitons. The ordinary one-quantum optical lifetime of an excitation is infinite. Absorption occurs only when ""three-body"" processes are introduced. The theory includes ""local field"" effects, leading to the Lorentz local field correction when it is applicable. A Smakula equation for the oscillator strength in terms of the integrated absorption constant is derived."
3e32532692884c7e0f75afa6c8918ed21cb4dff3,
a179e1e8e6a80a392e3447fddd51aa8fbe703412,"The classical dielectric theory of optical properties is a local theory, and results in a dielectric constant dependent only on frequency. This dielectric behavior can be written as a sum over resonances, each resonance occurring at a particular frequency. The spatial dispersion (i.e., nonlocal dielectric behavior) effect considered here is the effect of the wave-vector dependence of the resonant frequencies on optical properties. The additional boundary condition needed for the application of such a theory is discussed for the case in which the resonance is due to an exciton band and the wave-vector dependence to the finite exciton mass. Experimental data presented on the reflection peaks due to excitons in CdS and ZnTe exhibit gross departures from the reflectivities expected from classical theory. Particularly striking are sharp subsidiary reflectivity spikes. The departures from classical results are all well represented by calculations based on the theory of spatial resonance dispersion and a simple approximation to the drived boundary condition."
d45df0fbef0eaffdf1995575843d7a6da18edc84,"An experimental arrangement is proposed with which two optical masers may be made to beat to produce a far‐infrared source. The beating process occurs in a semiconductor which lacks inversion symmetry, allowing the third‐order process. By selecting a semiconductor with an appropriate energy gap, the energy denominators in the matrix element may be made small, enhancing the infrared intensity. Using the exciton states as the virtual intermediate states, the power output is calculated in the effective‐mass approximation. For beating a ruby maser at T=−180°C (6934 A) with one at T=20°C (6943 A), a CdSe sample at 4.2°K is proposed because its exciton energies are just higher than the maser energies. For input power of 10 kW for each energy, a 19‐cm−1 infrared beam of over a watt is expected. The competing process of two‐photon absorption heats the sample to approximately 65°K during a pulse, but this effect is not disastrous."
fe33ddf2f062e68578c21b236ed802996677b7da,
3129218517a3ce912c0522ae6176aca595a87c6c,"An experimental and theoretical study on the Faraday rotation in cadmium sulfide is presented. The contribution of the free carriers is calculated from the experimental curves, and an electron effective mass of 0.20 ± 0.01 is obtained. A detailed description of the experimental arrangement is given, which allows a precision of ± 0.1 degrees."
3a673849977a3bb9ffc12a341ed608c5951eddd8,"The absorption and fluorescent spectra of ""pure"" CdS platelets (impurity concentrations \ensuremath{\sim}${10}^{17}$/cc) have been studied at low temperatures. In addition to intrinsic exciton lines, many sharp absorption and fluorescent lines were observed at slightly lower energies. Many of these lines are due to transitions involving bound excitons, in which the light creates (or destroys) an exciton bound to a neutral or charged donor or acceptor. Arguments from the known-band symmetries and electron $g$ values permit the identification of the generic centers (neutral donor, neutral acceptor, charged center) with which particular transitions are associated. Transitions in which excitons from the second valence band are bound were also observed, in some cases lifetime broadened by phonon transitions to excitons from the first valence band. States due to neutral donors and neutral acceptors are usually both observed in the same crystal. This nonequilibrium situation is caused by the trapping of holes made by the light which is being used to study the crystal. The ""bleaching"" of the trapped holes by the application of infrared light during ordinary transmission measurements near the band gap supports the generic transition assignments given. Centers of appropriate symmetries exhibit splittings due to electron-hole and hole-hole j-j coupling. The magnitudes of these splittings agree with crude theoretical estimates. The oscillator strength per center should be directly related to these splittings for the case of excitons bound to neutral impurities. Several different donors are discernible, but only a single acceptor is observed. The generic classification and energies of the observed centers should make possible the combined chemical and optical identification of the corresponding donors and acceptor in doped crystals. The arguments which permit the identification of the centers can easily be generalized to the case of cubic crystals."
98d342c2b5af8e46dbad7efc85682b6e483a3dfe,
bdf1211a9cf2dc9acb42d1a284dc9de94be570d8,"This paper discusses how the fluorescence from semiconductors might be useful in constructing an optical maser. Attention is given to the sharp line emission which occurs at low temperatures in CdS and which arises from excitons bound to impurities. Some recent experimental results are summarized which give information concerning the fluorescent efficiency and the depth to which crystals are excited using ultraviolet light for excitation. Possible maser geometries are discussed and the opportunities for using an evacuated ground state are pointed out. There appear to be several severe difficulties in the way of success. These are partly associated with the small depth of penetration of the exciting light, with the low fluorescent efficiencies available and with the inability to grow large perfect crystals with controlled impurity content. An improvement in the art of crystal growing is probably necessary before the effects described here can be expected to result in a useful optical maser."
f09e7d99953ae7d8bf9383bded0de640e6e0cad5,
0da35bb243775df0da6d0d1fc579f59a8280f22d,"The theoretical effects of a finite slope condition band crossing on the direct exciton energy levels of wurtzite compounds is investigated. The lack of an experimental confirmation of these effects places an upper limit of about 10−10 ev‐cm on the slope of the conduction band at k = 0 in CdS, and a slightly larger limit in CdSe. Theoretical estimates of these slopes are also calculated. Another possible method of observing the magnetic effects due to the nonzero wave vector of the excitons is noted. The striking increase of the exciton oscillator strength in CdSe in a magnetic field is attributed to the magnetic compression of the excitons. A brief comparison of experimental energy band parameters for CdS and CdSe is given."
197d00b277acd2fd8ee58c9f59f35458d777f967,"Exciton absorption occurs, for weak exciton lines, at an energy which is the energy of an exciton having a wave vector equal to that of the light in the medium. These excitons have a finite wave vector, and therefore, a finite velocity. In a uniform magnetic field, the Lorentz force on the electron and hole due to the center-of-mass velocity produces a magnetic perturbation in addition to those ordinarily considered. The measurement of such a perturbation measures the velocity of an exciton of known wave vector, and therefore determines the total exciton mass. In addition, the measurement of this effect which depends on the exciton velocity provides a positive distinction between exciton absorption lines and absorption lines due to impurities. It is shown that this perturbation can be measured by the measurement of the Stark effect on excitons in the presence of a uniform magnetic field. The exciton mass for the $n=2$ states of excitons formed from the top valence band in CdS was measured by this technique, and found to be 0.92\ifmmode\pm\else\textpm\fi{}0.18 in reasonable agreement with the mass calculated from independent experiments. The Stark effect in the absence of a magnetic field was also studied to ensure an understanding of the effect in the presence of a magnetic field. The stark effect in a magnetic field sometimes exhibits peculiar behavior which was attributed to an extraneous Hall field. This interpretation gives an estimate of ${\ensuremath{\omega}}_{c}{\ensuremath{\tau}}_{r}\ensuremath{\approx}2$ for electrons in ""good"" CdS crystals at 1.6\ifmmode^\circ\else\textdegree\fi{}K and at 31 000 gauss."
2acda0ea35e6407a8ceda3b81f9cd120bd15ae56,
2e8e42210ef5631d1a767ce9ec4c3ff536611cb2,
79149595456004e1d906006094c862ce79756c14,"Selection rules for indirect radiative transitions and for intervalley scattering are investigated for Ge and Si. Comparison with experimental results of Haynes and of Benoit \`a la Guillaume supports (1) the present picture of the band structure of Ge with a conduction band minimum at the zone boundary, (2) the assignment of ${L}_{{2}^{\ensuremath{'}}}$ as the symmetry of the $\mathrm{LA}$ phonon at the zone boundary in the [111] direction. The absence of the $\mathrm{LA}$ phonon in the Haynes radiative emission experiment still requires explanation. To obtain the required selection rules we take the product of two irreducible representations $i$ and $j$ that belong to different wave vectors k and ${\mathbf{k}}^{\ensuremath{'}}$ The resulting character product is expressed in a form appropriate to a third group ${G}_{{\mathrm{k}}^{\ensuremath{'}\ensuremath{'}}}$ at k\ensuremath{''}=k+k\ensuremath{'}. If the elements of ${G}_{{\mathrm{k}}^{\ensuremath{'}\ensuremath{'}}}$ are applied to k to generate a star and $N(C)$ is the number of points of the star invariant (or equivalent) under any element $R$ in the class $C (\mathrm{of} {G}_{{\mathrm{k}}^{\ensuremath{'}\ensuremath{'}}})$ then the character product is $N(C)〈{\ensuremath{\chi}}^{i}(R){\ensuremath{\chi}}^{j}(R)〉$ i.e., $N(C)$ times the product of the characters averaged over those $R$ in $C$ which belong to ${G}_{{\mathrm{k}}^{\ensuremath{'}}}$ and ${G}_{{\mathrm{k}}^{\ensuremath{'}\ensuremath{'}}}$ (i.e., those whose characters can be found in the tables at k\ensuremath{'} and k\ensuremath{'}\ensuremath{'})."
e409be90277beb9b502aa453eea6ba3fdb124452,
f47aa80daebe5c8a5bccfb88c4a2d8358b1c931c,"The absorption coefficient for polarized light at photon energies less than that of the lowest lying direct exciton was measured for single crystals of ZnO at temperatures ranging from 20° to 200°K. Analysis of the results shows that the absorption is in agreement with that calculated for a process involving the simultaneous creation of an exciton and absorption of a phonon, both particles having a small wave vector. This agreement provides evidence that the absolute minimum of the conduction band cannot lie lower than the lowest lying direct exciton level, and, therefore, that the absolute minimum probably occurs at the center of the Brillouin zone. Values for the hole and electron masses were estimated from the analysis."
0629fcc0363a5f04d2fa0c5b5fb06b785d649a67,
55a00cfc510d1e3cd8dce1be6990e5e7a534036f,
57b4b271a718731bc1b56b5b9f53ed1c1f7b2309,
b51823702427c831bb68f1c360f9a49f4054ce06,"Measurements of the absorption coefficient between 10 and 300 ${\mathrm{cm}}^{\ensuremath{-}1}$ have been made with crystals of CdS in polarized light between 20 and 300\ifmmode^\circ\else\textdegree\fi{}K, at wavelengths near 5000 A. Analysis of the results at various temperatures near 70\ifmmode^\circ\else\textdegree\fi{}K shows that the absorption is in agreement with that calculated for a process involving the simultaneous creation of an exciton and the absorption of a phonon, both particles having a small wave vector. This agreement is strong evidence that the conduction band minimum and the valence band maximum in CdS both occur at the center of the Brillouin zone."
d847b3058c5ad77e52b50a76edb902236c17fdeb,
263406177207a2665be1287b0920de203d41ecdd,"S>The reflectance and fluorescent spectra of hexagonal CdS cn-stals were measured at 77 deg and 4.2 deg K using polarized light in the region of 5000 A. Structure not previously reported was found in the reflectivity curves which leads to the identification of three exciton series. These can be understood in terms of the splitting of the valence band into three levels at k = 0. The observation of excited exciton states and the polarization properties of the excitons make possible. (l) The determination of two of the three exciton binding energies, (2) the determination of the enengy splittings of the three valence bands, (3) the verification of the symmetry assigrments of the valence and conduction bands, and (4) correlation of the work of others with the present work, showing that the definite intrinsic effects are consistent both with our observations and our interpretations. The fluorescent experiments strongly suggest that the radiative decay of excitons occurs not directly, but from localized impurity exciton states in agreement with theory. (auth)"
811009769d2eb439bafadbdb02f12e5bc74fd51a,
ce341f660caf2b1268741962638cbcdc8e03a9ec,
2a6e0e2b80d494d30e6a48d14aeb1881b2faa3d2,
a73cfb8f068d6793ef3daf0bd54878940c41f65b,
ed28bb63527b712bf5d22f88c32f75ae94a3e924,
486bbd4afb8475f9d1a43fb4e1030642e2ee49ae,
20c866ee0dc88ef06857ff0b55a8d2531436f5dd,
bfca95498e624b7ab068b8c5c92f4a7f961063a3,
afa553494acea30086c3b8ae1959a9930e751476,
64bb6c616339115b3e9014378f21a8895d0e05a2,"It was found that the refractive index of heat-strengthened plate glass diminishes continuously from surface to center. As a consequence (1) prisms of such glass form spectral lines that are greatly curved, (2) plates of glass with plane parallel ends refract like cylindrical, concave lenses. This change of refraction is probably due to the condition of strain in the glass rather than to a relative change in the concentration of the component oxides."
a654760aa2edf93523fd6fc78f7435d4b444ee27,
bb83e6835cf6826114efda76b2cdf20f98816bfa,An improved cosmic‐ray meter is described in detail. Directions for its use in the measurement of cosmic rays are given; and various errors and ways of minimizing them are discussed.
d88d49848307d2ebe1643132db0bc2307b7d7759,
262a0a435bf8ea4bab7762537438f5359bd45a0e,
ea3b944e8a04ed9d52bcb435d3371e774a30a1fa,"A technique for preparing durable Schumann plates is described enabling them to be prepared with comparative ease. A new method of supersensitizing, and various technical points relating to their handling are summarized."
017473e5dd6b7c29b14ef752f4d21a49a5189a01,
09b42aeb687c21a034c18afe537daef643fec8d0,
0d74d1ae3064d81f9920f3bc538703d1d2af315e,
291f7436fd7c7620b20abc615d7d51113cc1ab91,
30f065212ecf1565460c6918d44d059dda6b76fb,
3bb6254156164f598f577cdc8d66de38f7af6aba,"Kinetic proofreading is a reaction scheme with a structure more complicated than that of Michaelis kinetics, which leads to a proofreading for errors in the recognition of a correct substrate by an enzyme. We have measured the stoichiometry between ATP hydrolysis and tRNAIle charging, using the enzyme isoleucyl-tRNA synthetase [L-isoleucine:tRNAIe ligase (AMP-forming), EC 6.1.1.5] and the amino acids isoleucine (correct) and valine (incorrect). The enzymatic deacylation of charged tRNA, which would normally prevent meaningful stoichiometry studies, was eliminated by the use of transfer factor Tu'GTP, (which binds strongly to charged tRNA) in the reaction mixture. For isoleucine, 1.5 ATP molecules are hydrolyzed per tRNA charged, but for valine, 270. These stoichiometry ratios are fundamental to kinetic proofreading, for the energy coupling is essential and proofreading is obtained only by departing from 1:1 stoichiometry between energy coupling and product formation. Within the known reaction pathway, these ratios demonstrate that kinetic proofreading induces a reduction in errors by a factor of 1/180. An overall error rate of about 10-4 for tRNA charging is obtained by a kinetic proofreading using a fundamental discrimination level of about 10-2, and is compatible with the low in vivo error rate of protein synthesis. Many biochemical reactions, particularly those associated with protein synthesis or DNA replication, exhibit high specificity in the selection between similar substrates. The overall error rate in selecting between two similar amino acids (1) in protein synthesis is believed to be about 3 in 104. The normal error rate in DNA synthesis (without post-replication repair) is about 1 in 108 or 109. These low error rates are biologically essential. The elementary description of specificity in biochemical reactions is based on discrimination in a Michaelis complex. Suppose enzyme c is to recognize substrate C, reacting on it to produce Cproduct, but should not recognize similar substrate D. Within an elementary kinetic framework C + C Cc p Cproduct + C [1] D + c Dc Dprduct + C The relative amounts of Cproduct and Dproduct (if C and D are present in equal concentrations) are determined by the kinetic parameters for the two reactions. Since c is supposed to recognize C, we suppose that along the reaction pathway the free energy for C is generally lower than for D. This kind of elementary reaction has an ability to discriminate between C and D that is limited by the maximum depression AG of the free energy of C below D along the reaction path. The smallest fraction of errors obtainable in an elementary fashion is fo = exp (-AG/RT). [2] When C and D are sufficiently similar, AG will not be large, and fo may be smaller than would be biologically optimal. ""Kinetic proofreading"" (2) is a method of using twice (or more) the same Michaelis kinetic ability to distinguish between C and D, resulting in an error rate as small as fo2 (or higher power) instead of fo for a given AG. The essential features of kinetic proofreading are contained in the reaction scheme C + C Cc 2g!4. (Cc) p C + -Cproduct"
3fe2c8cd00f1ed3831b26d3ecc4896c767bd4499,
4556be8620151212fa9054070277eaa7f52d2a90,"The specificity with which the genetic code is read in protein synthesis, and with which other highly specific biosynthetic reactions take place, can be increased above the level available fiom free energy differences in intermediates or kinetic barriers by a process defined here as kinetic proofreading. A simple kinetic pathway is described which results in this proofreading when the reaction is strongly but nonspecifically driven, e.g., by phosphate hydrolysis. Protein synthesis; amino acid recognition, and DNA replication, all exhibit the features of this model. In each case, known reactions which otherwise appear to be useless or deleterious complications are seen to be essential to the proofreading function."
4a2c685b54a57c1b2e58131d2e29524a8252c347,
4d0a1af860042cdcb2844e86383c640f4dbdedad,
50649ac832da5da68f8fd18e0ce68ade51a94a1f,
5194db6a5d9bca99c8256c3f2178e3c9bffae287,"The olfactory bulb of mammals aids in the discrimination of odors. A mathematical model based on the bulbar anatomy and electrophysiology is described. Simulations produce a 35-60 Hz modulated activity coherent across the bulb, mimicing the observed field potentials. The decision states (for the odor information) here can be thought of as stable cycles, rather than point stable states typical of simpler neuro-computing models. Analysis and simulations show that a group of coupled non-linear oscillators are responsible for the oscillatory activities determined by the odor input , and that the bulb, with appropriate inputs from higher centers, can enhance or suppress the sensitivity to partiCUlar odors. The model provides a framework in which to understand the transform between odor input and the bulbar output to olfactory cortex."
621b9eb16fa51fe8bee083f3915ab56e68ac6fe7,
6e5c39ee2eb6a0b66e4192460e6f3549e786e783,
794f7ea26c8b2481fce3b336c56d5cb46a25618e,"Animals that are primarily dependent on olfaction must obtain a description of the spatial location and the individual odor quality of environmental odor sources through olfaction alone. The variable nature of turbulent air flow makes such a remote sensing problem solvable if the animal can make use of the information conveyed by the fluctuation with time ofthe mixture ofodor sources. Behavioral evidence suggests that such analysis takes place. An adaptive network can solve the essential problem, isolating the quality and intensity of the components within a mixture of several individual unknown odor sources. The network structure is an idealization of olfactory bulb circuitry. The dynamics of synapse change is essential to the computation. The synaptic variables themselves contain information needed by higher processing centers. The use of the same axons to convey intensity information and quality information requires timecoding of information. Covariation defines an individual odor source (object), and this may have a parallel in vision. Humans rely chiefly on vision for their description of the world around them. As a result, most of the olfactory psychophysics and electrophysiology literature is chiefly concerned with the question of identifying or analyzing a single odor presented to the nose. Such studies ignore deeper questions of the function of olfaction in highly olfactory animals, which is to define and locate individual odor sources in a complex environment. This computational problem is also posed by vision. The visual system must transform detailed retinal images into a much smaller amount of significant information describing physical objects and their locations. In a natural environment, vision allows us to parse our environment into objects even when the objects are unfamiliar and many objects are simultaneously present. Consider an animal that has only an olfactory sense. To what extent can it solve the remote sensing problem, understanding the nature and location of distant objects, particularly when the objects are not familiar? In the visual system, analysis of the computational problem provided by the data and the task led to an understanding of what the ""early"" visual system must compute (1). I describe here a similar problem for the early processing in the olfactory system. A description of a natural olfactory environment illustrates the one important problem the earliest processing of an olfactory system must solve if the system is to be able to define and localize unknown odor sources in a complex odor environment. Behavioral evidence suggests that animals can solve this problem. This problem has a mathematical solution, which can be implemented by an elementary learning network. Dynamic changes of the synaptic strengths are essential to this solution. Part of the information needed by higher processing areas is explicitly contained in the connection strengths themselves. The last two sections present simulations of these ideas and discuss their relation to olfactory physiology and psychophysics. The Olfactory Environment and Task In most olfactory environments, the simple diffusion of odorant molecules is a negligible means of dispersing odorants. Odors of distant objects are brought to the nose by wind. Odorant molecules leaving the object follow the path of the air packet to which they are added. This packet already contains odorant molecules from other objects further upwind. The packet will move with the local wind, mixing slowly with other nearby packets containing odors from other objects, due to microturbulence in the air. Local winds fluctuate markedly in both magnitude and direction. As a result, the odor plume has a complex spatial structure (2) and is increasingly mixed with odors from other parts of the environment as time increases. Thus, the stimulus at the nose due to distant objects contains mixtures of odors from many sources, whose relative contributions are constantly changing. Physical studies with a single odor source and detector in a natural environment verify these ideas (3). (Because velocities and time scales of turbulence are very different for water and air, aquatic environments are not necessarily similar to atmospheric environments.) Highly olfactory animals need to be able to understand the location and odor properties of the various sources or ""odor objects"" in their space in order to hunt for food or flee from danger. When a single odor source is present, the problem is simple. The perceived odor quality (i.e., the relative strengths of different components of an odor) is a fixed property of this sole source. The strength of the odor will fluctuate with wind direction. For an object that is near, the odor will be strong only for a narrow range ofwind directions, and when the wind shifts from that direction the odor will quickly grow weak. An object that is further away has an odor plume that is more contorted and on average is also broader and weaker. Its odor strength will correlate less well with the local wind direction. The relative time scale of fluctuation also contains information about distance. Thus, when a single odor source is present approximately upwind, information about the direction and even the approximate distance of the object is available to a stationary olfactory animal. The essential computation is an analysis of the fluctuations of the odor intensity with time and the relation of these to fluctuations in the local wind direction. The problem of one odor object is trivial compared to the real problem ofmany unknown objects. Except when sniffing an object at the nose, an animal is always in an environment of mixed and changing odor patterns. If each odor object were to stimulate a different set of receptor cells in the sensory epithelium, then the problem of separating objects would be simple. However, physiological studies of the vertebrate olfactory system generally indicate broadly tuned receptor cells and the excitation of a large number of cells by a given odor, so odors of different objects are believed to be discriminated on the basis of the patterns of excitation, as 6462 The publication costs of this article were defrayed in part by page charge payment. This article must therefore be hereby marked ""advertisement"" in accordance with 18 U.S.C. §1734 solely to indicate this fact. Proc. Natl. Acad. Sci. USA 88 (1991) 6463 recently reviewed by Kauer (4). Similar conclusions are reached through behavioral studies (5, 6). Broad tuning makes the problem of separating unknown objects in natural environments appear very difficult. Suppose that it is possible to analyze the fluctuations of the instantaneous stimulation pattern in such a way that the contributions of different odor sources to the mixture can be separated. By next analyzing the fluctuations of each odor object intensity in conjunction with fluctuations in the local wind direction, the animal will generate an excellent representation of the location and isolated odor of each odor source in its vicinity. This paper investigates the hypothesis that the function of the earliest part of olfactory processing in highly olfactory animals is to accomplish this first separation task-namely, to analyze the fluctuations of multisource environments into individual odors. Behavioral Evidence for Fluctuation Analysis Many animals are able to search for a known odor such as a favorite food. In concept, the animal computes a projection of the ambient odor input against the known template and thus measures the intensity of the target odor present. If the environment has a substantial odor, whose excitation pattern strongly overlaps the pattern of the search template (but with different relative amplitudes), a simple template match to look for a weak target odor becomes impossible. The environment itself will always appear to have an appreciable component of the target odor. A fixed environment could be removed by subtractive adaptation, but realistic environments reflect constantly changing strengths and mixtures of odors from a variety of sources. That an animal with a broadly tuned set of sensory cells can locate a weak known odor source indicates that an analysis of fluctuations is being carried out to provide information not available from elementary processing. Experiments in Limax maximus have demonstrated the importance of fluctuations for odor learning. Hopfield and Gelperin (7) carried out learning experiments on mixtures of two food odors A and B. Slugs were aversively conditioned while on a mesh 1 cm above a piece of filter paper painted with a solution containing a 1:1 mixture of odorants A and B. When later tested, the animals were found to be aversively conditioned to the 1:1 mixture AB, but they had the same nonaversive behavior as control animals to the individual odors A and B. These experiments indicate that the mixed odor object AB is perceived as an odorant distinct from A or B. When different animals were trained with the same protocol, except that the initial conditioning was done with alternating stripes ofA and B 0.9cm apart, the slugs exhibited aversive behavior to A alone, B alone, and the mixture AB. The distance from the odor source to the slugs was always greater than the separation between the stripes. The diffusional mixing time for stripe sources of this separation is -1 sec. Thus, the odor available to the animals must always be somewhat mixed and fluctuating due to thermally induced air flows within the closed chamber. Since conditioning with odor stripes results in aversive response to the individual odors A and B (as well as to the mixture AB), the existence of the individual odors A and B was presumably deduced from odor fluctuations. The crucial difference between the two experiments was the presence of fluctuations in the relative amount of A and B presented by the stripes but not by the 1:1 AB mixture, which allowed the animals to come to differ"
835aa62ff100a8fa75e036c68c2295096dbe2e44,
83a2e6260c0a842378512c85c3c9aea7c5097a42,
97a756be586efd9a3bb4a88f955166306f88b0a0,
9dcf1f498e1a56131e34058d69ec2f9803aa1e3a,
a6b9658198e878838b7b85d29eaa9049ca965492,
b28813640be477fa050305ffe2def135feea05d6,
cf1bae64b56ef96e14ef48ed3df28db0241fafe1,"A lthough living systems obey the laws of physics and chemistry, the notion of function or purpose differentiates biology from other natural sciences. Organisms exist to reproduce, whereas, outside religious belief, rocks and stars have no purpose. Selection for function has produced the living cell, with a unique set of properties that distinguish it from inanimate systems of interacting molecules. Cells exist far from thermal equilibrium by harvesting energy from their environment. They are composed of thousands of different types of molecule. They contain information for their survival and reproduction, in the form of their DNA. Their interactions with the environment depend in a byzantine fashion on this information , and the information and the machinery that interprets it are replicated by reproducing the cell. How do these properties emerge from the interactions between the molecules that make up cells and how are they shaped by evolutionary competition with other cells? Much of twentieth-century biology has been an attempt to reduce biological phenomena to the behaviour of molecules. This approach is particularly clear in genetics , which began as an investigation into the inheritance of variation, such as differences in the colour of pea seeds and fly eyes. From these studies, geneticists inferred the existence of genes and many of their properties, such as their linear arrangement along the length of a chromosome. Further analysis led to the principles that each gene controls the synthesis of one protein, that DNA contains genetic information, and that the genetic code links the sequence of DNA to the structure of proteins. Despite the enormous success of this approach, a discrete biological function can only rarely be attributed to an individual molecule, in the sense that the main purpose of haemoglobin is to transport gas molecules in the bloodstream. In contrast, most biological functions arise from interactions among many components. For example, in the signal transduction system in yeast that converts the detection of a pheromone into the act of mating, there is no single protein responsible for amplifying the input signal provided by the pheromone molecule. To describe biological functions, we need a vocabulary that contains concepts such as amplification, adaptation, robustness, insulation , error correction and coincidence detection. For example, to decipher how the binding of a few molecules of an attractant to receptors on the surface of a bacterium can make the bacterium move towards the attractant (chemotaxis) will require understanding …"
cf5c83e431473eb961e8e952e762979674615d49,
dd934b5e9d11e33967bd08a8e12ff768ecd2ce29,"Synaptic plasticity 'rules' enable a neural system to 'learn' or 'adapt'. Plasticity must also be capable of keeping a network in a functional state by repairing the damage that spontaneously takes place due to random synapse-changing processes. This repair should take place during normal operation of the system. We examine a (simulated) network of model neurons that has been designed to carry out olfactory tasks, in which the intensity-invariant recognition of a multiple odors is achieved by appropriate network design. Since the connections which should (and should not) be present are known and understood, it is possible to derive a synaptic plasticity rule, based on the timing of preand post-synaptic spikes in the operational designed network, that will maintain a functional network by selecting appropriate synapses to maintain or delete. This derived timing-based plasticity rule is strikingly similar to those experimentally found in LTP. In addition to enabling functional stability of known odor recognition in the appropriately designed system, we find that the same learning rule enables the system to perform single-trial learning of new odors in both unsupervised and supervised circumstances."
ecc097580398424576fd87ac3f67c41a3761fdd0,"Spectroscopy of Extreme Ultraviolet.\char22{}A method which has been developed of coating films with an emulsion suitable for work in this region is described in detail, and also an oil-cooled discharge tube of the internal capillary type which will stand an input of 2.25 KW and thus reduce the time of exposure and consequent fogging of the films from 10 to 100 fold. Transparency of oxygen, nitrogen and air between 500 and 1,800 \AA{}. was investigated. In a vacuum spectrograph with a grating of 50 cm. radius these gases were found to transmit light of from 1100 to 1225 \AA{}. even when at a pressure of 3 cm. At around 1 mm. spectrum lines were photographed from 800 to 1,800 \AA{}. and with oxygen at 0.001 mm., the spectrum was photographed to 430 \AA{}. These gases, then, are not as opaque to light in this region of the spectrum as has been generally supposed."
ee294e3301e84c2cbf0fd74c6c070a9403f02278,
f41207b7b9a2d804475e254dcfa77c6d9c22b646,
492244990cc9c91ab64dff3dedd277ddc687fcca,
d7961f7ceab5aef03600d0505088b0bad8a0580a,
3598822843b354beb60eff950bbd8faaa1e05fd1,"Recent findings demonstrate that selective retrieval practice (SRP), specifically the retrieval of subparts of material, not just retrieval of the entire encoded material, can enhance later memory performance. We present two experiments that investigated whether SRP enhances memory performance among older adults. We also examined to what extent this effect is enhanced by the level of integration of the studied material. We used a design that contrasts the performance of the groups in conditions with and without SRP. This design also allowed us to examine whether older adults present with faster forgetting compared to younger individuals when assessed over a long delay. In both experiments, participants were exposed to a learning phase in which they had to achieve a criterion of 70% correct recall and were then tested at 1 month. The SRP for the experimental group occurred 1 day and 1 week after the learning phase (the control group received no SRP). None of the items at 1-month delay was probed in the retrieval practice. Experiment 1 used integrated material (four short stories). Experiment 2 used less integrated material (16 sentences). Both age groups showed a decline in memory performance over 1 month, however, groups tested repeatedly showed better performance (irrespective of age or material). (PsycInfo Database Record (c) 2022 APA, all rights reserved)."
3ea4bc7a78e518ef0c00a4225fb9aef771229510,"It is well established that the more we learn, the more we remember. It is also known that our ability to acquire new information changes with age. An important remaining issue for debate is whether the rate of forgetting depends on initial degree of learning. In two experiments, following the procedure used by Slamecka and McElree (Exp 3), we investigated the relationship between initial degree of learning and rate of forgetting in both younger and older adults. A set of 36 (Exp 1) and a set of 30 (Exp 2) sentences was presented four times. Forgetting was measured via cued recall at three retention intervals (30 s, 1 hr, and 24 hr). A different third of the original sentences was tested at each delay. The results of both experiments showed that initial acquisition is influenced by age. However, the rate of forgetting proved to be independent from initial degree of learning. The conclusion is that rates of forgetting are independent from initial degree of learning."
629169c50dce06a89236afff7e6b1584de0bac52,
62aa79baadd5c10849ddbb205c8e4156fe72d133,"Research from a working memory perspective on the encoding and temporary maintenance of sequential instructions has established a consistent advantage for enacted over verbal recall. This is thought to reflect action planning for anticipated movements at the response phase. We describe five experiments investigating this, comparing verbal and enacted recall of a series of action–object pairings under different potentially disruptive concurrent task conditions, all requiring repetitive movements. A general advantage for enacted recall was observed across experiments, together with a tendency for concurrent action to impair sequence memory performance. The enacted recall advantage was reduced by concurrent action for both fine and gross concurrent movement with the degree of disruption influenced by both the complexity and the familiarity of the movement. The results are discussed in terms of an output buffer store of limited capacity capable of holding motoric plans for anticipated action."
6340267e45d55657e574ec1a6ae57a2c66b9e293,
d687959b63fcb07e0792c223da7327a45e344aa9,"Debate continues regarding the possible role of the hippocampus across short‐term and working memory tasks. The current study examined the possibility of a hippocampal contribution to precise, high‐resolution cognition and conjunctive memory. We administered visual working memory tasks featuring a continuous response component to a well‐established developmental amnesic patient with relatively selective bilateral hippocampal damage (Jon) and healthy controls. The patient was able to produce highly accurate response judgments regarding conjunctions of color and orientation or color and location, using simultaneous or sequential presentation of stimuli, with no evidence of any impairment in working memory binding, categorical accuracy, or continuous precision. These findings indicate that hippocampal damage does not necessarily lead to deficits in high‐resolution cognitive performance, even when the damage is severe and bilateral."
5394d71078de64f0ccb45ab2eab96e3e5ca9f8d0,
76c14e1667b03393d97c721cbde54bbd623637b4,
a94343901938e6c539a13abc56d7fbcdb6fa9eac,
aabf268345825171a9253ffe6e2b974198b16cba,
da01fef32472b6a5b8fa136a1d6a90d6c057ea30,"ABSTRACT A broad functional approach is taken to the analysis of human memory. The overall importance of episodic memory, the capacity to remember specific events, is illustrated by the devastating effect that loss of this aspect of memory has on the capacity to cope in the case of densely amnesic patients. Recent applied research has however focussed heavily on factors compromising the reliability of eyewitness testimony in the forensic field and on the creation of false memories. While acknowledging the progress made on this issue, it presents two dangers. The first is practical, the danger of generalising too readily from laboratory-influenced simulations that differ in important ways from the context to which they are applied. This suggests a need for fewer but more realistically representative studies. The second is a broad theoretical issue, that of extending the findings from this important but limited applied area, within which precise detail may be crucial, to the whole of memory, consequently failing to appreciate its many strengths."
dbbcb2c45ec35645768b76dec14386dabbf7b700,"The evolution of the concept of a multicomponent working memory is described with particular reference to the contribution from neuropsychology. Early evidence from patients with the classic amnesic syndrome, together with others showing the opposite deficit of impaired short-term but preserved long-term memory argued strongly for a separation between long- and short-term memory systems. Simulation of the short-term deficit in healthy participants using a dual task approach suggested the need to assume a three component system serving as a multi-purpose working memory comprising an overall attentional control system, the Central Executive, aided by separable temporary buffer stores for phonological and visuospatial information. An account is then given of the way in which evidence from patients was combined with the study of healthy participants to test and expand the model over subsequent years. This led to the need to propose a fourth component, the Episodic Buffer, a system that combines information from multiple sources and makes it accessible to conscious awareness. I conclude with a brief account of how the multicomponent approach resembles and differs from that of other current models of working memory."
08574e5f8feb2fd167df99ffd74072da23da0b21,
11724961f8dc6ce05b8578ab81794737c9100619,
11bfb450c8c0c60dda75650127b86016aa36a9b4,
192d505451bf6832b44b75c1be99108df6af94b8,
1b87edb3b89a2dd363f57185ea190e74a9aab9c4,
317404cbd1a565293969f1c9b6e93c53406d5201,"ABSTRACT Our understanding of human memory has gained greatly from the study of individuals with impaired memory but rather less from outstandingly high levels of memory performance. Exceptions include the case of London taxi drivers whose extensive route learning results in modification of their hippocampus. Our study involves a group whose extensive verbal learning potentially provides a similar natural experiment. The Muslim faith encourages followers to memorise the whole of the Qur’an, some 77,449 words in its classic Arabic form. Successful memorisers are known as “Hafiz”. We tested 10 Hafiz, 12 background-matched Muslim controls and 10 non-Muslim participants, on their detailed knowledge of the Qur’an and on their performance on standard measures of verbal and visuospatial learning. We found no differences between the three groups in their capacity to memorise verbal or visuospatial material and hence no evidence of generalisation of learning capacity in the Hafiz group. More surprisingly, however, half of the Hafiz group did not understand Arabic but were equivalent in Qur’anic memory to those who did. Given the importance that meaning is typically assumed to play in long-term memory, this was unexpected. We discuss the practical and theoretical implications of these results for verbal memory and long-term learning."
34bb509eece373ce0d21df534936c565f10505a4,"Though there is substantial evidence that individuals can prioritize more valuable information in visual working memory (WM), little research has examined this in the verbal domain. Four experiments were conducted to investigate this and the conditions under which effects emerge. In each experiment, participants listened to digit sequences and then attempted to recall them in the correct order. At the start of each block, participants were either told that all items were of equal value, or that an item at a particular serial position was worth more points. Recall was enhanced for these higher value items (Experiment 1a), a finding that was replicated while rejecting an alternative account based on distinctiveness (Experiment 1b). Thus, valuable information can be prioritized in verbal WM. Two further experiments investigated whether these boosts remained when participants completed a simple concurrent task disrupting verbal rehearsal (Experiment 2), or a complex concurrent task disrupting verbal rehearsal and executive resources (Experiment 3). Under simple concurrent task conditions, prioritization boosts were observed, but with increased costs to the less valuable items. Prioritization effects were also observed under complex concurrent task conditions, although this was accompanied by chance-level performance at most of the less valuable positions. A substantial recency advantage was also observed for the final item in each sequence, across all conditions. Taken together, this indicates that individuals can prioritize valuable information in verbal WM even when rehearsal and executive resources are disrupted, though they do so by neglecting or abandoning other items in the sequence. (PsycInfo Database Record (c) 2020 APA, all rights reserved)."
45d1af104e393a32a4772c0d5111636aed304a4a,"It is suggested that working memory comprises a system for the temporary storage and manipulation of information, forming an important link between perception and controlled action. Evidence is presented for a three-component model, comprising an attentional control system, the central executive, and two subsidiary slave systems. One of these the, the visuo-spatial sketch pad holds and manipulates spatial information, while the other, the phonological loop performs a similar function for auditory and speech-based information. Evidence is presented for the view that the phonological loop has evolved as a mechanism to facilitate the acquisition of language."
9fa28a3b1f61a2551fce8dd6629714603ab87d39,
fb6fba73e1952b8f658761a350f02fb2d1d28c35,"The multicomponent model aims to provide a broad theoretical framework enabling both more detailed fractionation and analysis of its components, and a capacity for it be used fruitfully beyond the laboratory. In its current form it comprises four interacting components. Two of these are modality-specific memory storage systems, one verbal-acoustic, the phonological loop, and one visuospatial, the sketchpad. Information in both these stores can be temporarily maintained via focused attention termed ‘refreshing’, while the phonological loop can also maintain familiar verbalizable material by subvocal or overt rehearsal. Both subsystems are controlled by a third component, the central executive, a supervisory system with limited resources. The central executive is principally concerned with internally directed attentional control processes but also has a role in the attentional selection of perceptual information. Information from these three components is coordinated with information from perception and long-term memory through the fourth component, a multidimensional, multimodal episodic buffer. This component is capable of holding up to around four episodic chunks, and is a valuable but essentially passive storage system, controlled by the central executive and accessible to conscious awareness. The multicomponent model has been systematically developed using a number of experimental tools. These include, principally, similarity effects to identify the type of coding involved, concurrent task methods to assess the contributions of the various subsystems to complex tasks, and neuropsychological evidence, in particular from the study of single cases with very specific deficits. The model continues to evolve and has proved successful both in accounting for a broad range of data on memory and related cognitive areas and in its application to the understanding of a wide range of cognitive activities and populations."
1ae38957b85b20196f8e84e71701d6b330a82e22,"The term “modal model” reflects the importance of Atkinson and Shiffrin’s paper in capturing the major developments in the cognitive psychology of memory that were achieved over the previous decade, providing an integrated framework that has formed the basis for many future developments. The fact that it is still the most cited model from that period some 50 years later has, we suggest, implications for the model itself and for theorising in psychology more generally. We review the essential foundations of the model before going on to discuss briefly the way in which one of its components, the short-term store had influenced our own concept of a multicomponent working memory. This is followed by a discussion of recent challenges to the multicomponent approach. These include claims that the concept of a short-term store be replaced by an interpretation in terms of activated long-term memory and whether existing evidence justifies inclusion of specialised modality-specific stores. We present reasons to question these proposals. We conclude with a brief discussion of the implications of the longevity of the modal model for styles of theorising in cognitive psychology."
1b810dc5799e7c6258d8d650a3da3d506e960a9b,
6653c09eb3f3098f318e8489f0f686422912c702,
69d228bb8191b2a3a65d6603f96720b9353e9813,"Multitasking, a common feature of everyday life, requires simultaneous maintenance and operation of a range of action-controlling task sets. We investigated the role of working memory in multitasking by means of the embedded task paradigm. This involved setting up a primary task with a variable task set, which then had to be maintained throughout the performance of a second embedded task with a fixed task set before it could be completed. We tested the hypothesis that the capacity to maintain the 2 task sets so as to avoid mutual interference would depend on working memory. We used Baddeley and Hitch’s (1974) multicomponent working memory approach to investigate this. Experiment 1 used articulatory suppression to examine the potential role of subvocal rehearsal, and found no impact on performance. Experiment 2 used backward counting to impose an additional executive load, and found a major impact on performance even with the simple task of counting backward in ones. This took the form of more pervasive effects of stimulus overlap that could be interpreted in terms of a change in the way the two tasks were managed. The differential impacts of the concurrent tasks indicated that multitasking is dependent on working memory, where it draws on limited-capacity executive resources but not on the capacity for temporary phonological storage."
72acbbd032472714a72882f94a06bbaf50be3416,"Measuring memory over long delays requires multiple sessions, often administered remotely (e.g. by telephone) to maximise convenience and participant access. However, the efficacy of testing delayed memory via telephone has not previously been examined. We administered the Crimes Test to young and older adults, with a one-week delay test, either in person or over the telephone. Testing via telephone had no detrimental effect, indicating this to be an appropriate method of examining delayed episodic memory."
81351eef632796516c346ab4125da7826558446c,
9f67c20ddb41523ae9ecb33d4a3fa01ce7830fe5,
a062d35eebe28eeb2694a4f7f4eebbf4a7643425,
d42e4ded4665c7f3a90daf3a8896d81f1d6c2c64,
ded5ab65467c77382f6e022d480c635cad50a7b8,
12016e1ea86ad450ebf1ea37c485dea9a09fc36b,
1e9bf3196b473c26a863ff920329a08895768b6d,
24102e0d298b5c5f0fd21afaa05c017cc38f4632,
25bdf95f6ef4e9817a57fd04b152a99b37ba408f,
2a4467599a6c78d16b2c1c0397542c4099ee68a8,
3aea53c560d988d99859fabeafa97ffeed4d1fdb,"Previous research on memory for a short sequence of visual stimuli indicates that access to the focus of attention (FoA) can be achieved in either of two ways. The first is automatic and is indexed by the recency effect, the enhanced retention of the final item. The second is strategic and based on instructions to prioritize items differentially, a process that draws on executive capacity and boosts retention of information deemed important. In both cases, the increased level of retention can be selectively reduced by presenting a poststimulus distractor (or suffix). We manipulated these variables across three experiments. Experiment 1 generalized previous evidence that prioritizing a single item enhances its retention and increases its vulnerability to interference from a poststimulus suffix. A second experiment showed that the enhancement from prioritizing one or two items comes at a cost to the recency effect. A third experiment showed that prioritizing two items renders memory for both vulnerable to interference from an irrelevant suffix. The results suggest that some but not all items in working memory compete to occupy a narrow FoA and that this competition is determined by a combination of perceptually driven recency and internal executive control."
404815c1230b84c09c1d1bce062039d6d0c8d35c,
41cc44d611354f1a4299aacc08855b1dfd6beda6,"Immediate serial recall of verbal material is highly sensitive to impairment attributable to phonological similarity. Although this has traditionally been interpreted as a within-sequence similarity effect, Engle (2007) proposed an interpretation based on interference from prior sequences, a phenomenon analogous to that found in the Peterson short-term memory (STM) task. We use the method of serial reconstruction to test this in an experiment contrasting the standard paradigm in which successive sequences are drawn from the same set of phonologically similar or dissimilar words and one in which the vowel sound on which similarity is based is switched from trial to trial, a manipulation analogous to that producing release from PI in the Peterson task. A substantial similarity effect occurs under both conditions although there is a small advantage from switching across similar sequences. There is, however, no evidence for the suggestion that the similarity effect will be absent from the very first sequence tested. Our results support the within-sequence similarity rather than a between-list PI interpretation. Reasons for the contrast with the classic Peterson short-term forgetting task are briefly discussed."
585f0008f9f5c6b928e53015cf3cca7ad9bc691c,
64452a490612487889fcd1556f77f0896660e5e7,
6794358b43361642996d852388ee29590b4fcdaa,
67f36f65b37e6e16b393f3e23f98ed4e60a2d000,Introduction to Working Memory. The Development of Working Memory. Vocabulary Acquisition. Speech Production. Introduction to Reading Development. Phonological Processing and Reading Development. Visual Word Recognition. Language Comprehension. Theoretical and Practical Issues.
7beae9fd732e6b59b0f3cccbde25d0846ba07a01,
804e6cac112af838f56761fedaca252c9b362be2,
891894be173dce71379cb55607ca8525ddc6c276,
8afa5eafcdd0085d71cdfd7174f82e9108af71ab,"Contents Preface Further Reading Leeds 1934-53 Chapter 1 - Growing up in Yorkshire London 1953-56 Chapter 2 - Psychology in the 1950s: Seeds of the Cognitive Revolution Princeton - Los Angeles 1956-77 Chapter 3 - The Trip of a Lifetime Leeds and Bristol 1957-58 Chapter 4 - In Search of a Job Cambridge 1958-67 Chapter 5 - From Cognitive Science to Applied Psychology Chapter 6 - Psychology Under Water Chapter 7 - Practical Applications and Theoretical Implications: Postmen and Watchkeepers Chapter 8 - Acoustic and Semantic codes: Evidence for Separate Memory Systems? Sussex 1967-72 Chapter 9 - From Full-time Research to a New University Chapter 10 -Amnesia San Diego 1970-71 Chapter 11 - California and New Directions in Memory Research Chapter 12 - The Emergence of Semantic Memory Returning to Sussex 1971-72 Chapter 13 - Working Memory and the Phonological Loop Stirling 1972-74 Chapter 14 - Working Memory and Visual Imagery Cambridge 1974-95 Chapter 15 - Returning to the Unit Chapter 16 - Encounters with the Law Chapter 17 - Stress: From Sky Diving to Anaesthetics Chapter 18 - When Long-term Memory Fails Chapter 19 - Working Memory and Language Chapter 20 - Boston and the Central Executive Chapter 21 - Psychology Around the World Bristol 1995-2003 Chapter 22 -The Episodic Buffer Chapter 23 - Patients, Parasites and Mobile Phones Stanford 2001-02 Chapter 24 - Working Memory in Context: Neuroscience, Emotion and Philosophy Returning to Yorkshire 2003 - Chapter 25 - Exploring the Episodic Buffer Chapter 26 - Summing up: From Behaviourism to Cognitive Neuroscience"
aff908bce5c0f0f1f13f024fb878178965e25a1f,
b793675393e9b1a56fefeaef51b2b4921516ce90,
b87119062579672a72ec7d18d238d06037f0b0d4,
bd9205b157a1341ac96bd6d1efffa0da76df1738,
c83a8c7962296da40abd4edcd92c58abb5d499d0,
c92f229169bd609a0d8541d9c7c823a54465859e,
cb1fdf465eb8dee638df2dc41cecf136079b6fa9,"We examined the role of executive control in stimulus-driven and goal-directed attention in visual working memory using probed recall of a series of objects, a task that allows study of the dynamics of storage through analysis of serial position data. Experiment 1examined whether executive control underlies goal-directed prioritization of certain items within the sequence. While instructing participants to prioritize either the first or final item resulted in improved recall for these items, an increase in concurrent task difficulty reduced or abolished these gains, consistent with their dependence on executive control. Experiment 2 examined whether executive control is involved in the disruption caused by a post-series visual distractor (suffix). A demanding concurrent task disrupted memory for all items except the most recent whereas a suffix disrupted only the most recent items. There was no interaction when concurrent load and suffix were combined, suggesting that deploying selective attention to ignore the distractor did not draw upon executive resources in our task. A final experiment replicated the independent interfering effects of suffix and concurrent load while ruling out possible artefacts. We discuss the results in terms of a domain-general episodic buffer in which information is retained in a transient, limited capacity privileged state, influenced by both stimulus-driven and goal-directed processes. The privileged state contains the most recent environmental input together with goal-relevant representations being actively maintained using executive resources."
d8fc71eb4213eeb9f13dc1958156a5a1ccb6912d,
e7d1b9920b67069ad2b77d79c4b7d0f259f72d5e,
e977ef6395196f3e5295a9790e672b2e93304984,
ea383b456d3a65fbe4f3e9df45cce4e7d2c04e3e,"In visual working memory tasks, memory for an item is enhanced if participants are told that the item is relatively more valuable than others presented within the same trial. Experiment 1 explored whether these probe value boosts (termed prioritization effects in previous literature) are affected by probe frequency (i.e., how often the more valuable item is tested). Participants were presented with four colored shapes sequentially and asked to recall the color of one probed item following a delay. They were informed that the first item was more valuable (differential probe value) or as valuable as the other items (equal probe value), and that this item would be tested more frequently (differential probe frequency) or as frequently (equal probe frequency) as the other items. Probe value and probe frequency boosts were observed at the first position, though both were accompanied by costs to other items. Probe value and probe frequency boosts were additive, suggesting the manipulations yield independent effects. Further supporting this, experiment 2 revealed that probe frequency boosts are not reliant on executive resources, directly contrasting with previous findings regarding probe value. Taken together, these outcomes suggest there may be several ways in which attention can be directed in working memory."
ef7a94296e5029be28f0dbca111a64230c07a1de,
f35583b6abc59f19b604c110c80889db0fc02f35,"Immediate serial recall of sentences has been shown to be superior to that of unrelated words. This study was designed to further explore how this effect might emerge in recall and to establish whether it also extends to serial recognition, a different form of response task that has relatively reduced output requirements. Using auditory or visual presentation of sequences, we found a substantial advantage for sentences over lists in serial recall, an effect shown on measures of recall accuracy, order, intrusion, and omission errors and reflected in transposition gradients. In contrast however, recognition memory based on a standard change detection paradigm gave only weak and inconsistent evidence for a sentence superiority effect. However, when a more sensitive staircase procedure imported from psychophysics was used, a clear sentence advantage was found although the effect sizes were smaller than those observed in serial recall. These findings suggest that sentence recall benefits from automatic processes that utilise long-term knowledge across encoding, storage, and retrieval."
f6046b8fd3bb31e2caabce77637b6234af433af6,
f959416253fe5477e0794ce8c9f3cfb5c9cbd60c,"Recent research has indicated that visual working memory capacity for unidimensional items might be boosted by focusing on all presented items, as opposed to a subset of them. However, it is not clear whether the same outcomes would be observed if more complex items were used which require feature binding, a potentially more demanding task. The current experiments, therefore, examined the effects of encoding strategy using multidimensional items in tasks that required feature binding. Effects were explored across a range of different age groups (Experiment 1) and task conditions (Experiment 2). In both experiments, participants performed significantly better when focusing on a subset of items, regardless of age or methodological variations, suggesting this is the optimal strategy to use when several multidimensional items are presented and binding is required. Implications for task interpretation and visual working memory function are discussed."
038ad8247f522273ae00a220c156c873999a5eda,"The contents of visual working memory are likely to reflect the influence of both executive control resources and information present in the environment. We investigated whether executive attention is critical in the ability to exclude unwanted stimuli by introducing concurrent potentially distracting irrelevant items to a visual working memory paradigm, and manipulating executive load using simple or more demanding secondary verbal tasks. Across 7 experiments varying in presentation format, timing, stimulus set, and distractor number, we observed clear disruptive effects of executive load and visual distraction, but relatively minimal evidence supporting an interactive relationship between these factors. These findings are in line with recent evidence using delay-based interference, and suggest that different forms of attentional selection operate relatively independently in visual working memory."
1cfb54562255dc9b8d1275991bfd7c9c120e3603,
1d6326f270898beb4f916be43e13f2298ccdc153,
251977e78a80517f40ebd9c34d5e5ecf26c6e3da,"The concept of modularity is used to contrast the approach to working memory proposed by Truscott with the Baddeley and Hitch multicomponent model. This proposes four sub components comprising the central executive, an executive control system of limited attentional capacity that utilises storage based on separate but interlinked temporary storage subsystems. One, the phonological loop, is concerned with the temporary storage of verbal materials and another, the visuo-spatial sketchpad stores visual information. A fourth component, the episodic buffer, allows the various components to interact and enables their content to become available to conscious awareness. After a brief description of the relevance of the model to language acquisition, an account is given of the way in which it has developed in recent years and its relationship to other approaches to working memory."
583ccb6d550e4ea2a8111177746009a8e74df236,
639a5d1aaee7232967284f29754f92b97d983267,
7f1e644e89ed0ac69574dbedf35f6698c30e5f11,
8f58ce5c8645ba9f073a542ae07df6d899871b04,
9441d448d0824b62e326856f987b9475f49ee13a,
9b5a43037028e26bbb3145db1ba3dd654d57c792,"Introduction: Magnetic Stimulation in studies of cortical function. Vincent Walsh University of Oxford. Magnetic stimulation of the human brain is one of many techniques available for studies of perceptual, motor and cognitive functions. The main advantages claimed for TMS are the reversible nature of the interference, its brevity and the ability to selectively interfere with different cortical functions. I will discuss the unique problem space occupied by studies using TMS, outline the main methodological modes in using it and discuss the spatial, temporal and functional resolution. I will give examples of the use of TMS in studies of learning, cortical inteactions and speech which serve as examples of the kinds of contribution use of the technique can make to studies of cortical function. Transcranial magnetic stimulation as a tool for probing and manipulating motor cortical organisation J C Rothwell Institute of Neurology, London Transcranial magnetic stimulation is now commonly used in experimental psychology as a method of inducing a temporary functional ""lesion"" in parts of the cerebral cortex. Here, I will summarise three new pieces of work that use TMS in a different way, to study and manipulate the excitability of connections between two areas of cortex. All three studies examined the connection between the premotor areas and the motor cortex in healthy human subjects. In the first study, we used a paired pulse design to test whether single stimuli over sites anterior to motor cortex could affect its excitability as measured by the EMG response to a standard test stimulus. Two small figure of eight coils were used so that the stimuli could be given as close a 3cm apart, and the intensity of the anterior, conditioning stimulus was set to be lower than the active motor threshold for the motor cortex. Two points (A and B) were found to produce suppression of the response to the test shock, with a time course that reached a peak at an interstimulus interval of 6ms. Point A was 4-6 cm anterior to the hand area of motor cortex; point B was in the midline and 6cm anterior to the vertex. Since stimuli at these sites had no effect on the size of H-reflexes in arm muscles, nor on the EMG responses evoked by transcranial electrical stimulation of the motor cortex, we presume that the interaction we observed was due to activity in cortico-cortical connections between A or B and the motor cortex. In the second and third studies we used repetitive TMS given at 1Hz for up to 25min to manipulate the excitability of the connection for a short period outlasting the end of the rTMS. Stimuli that were subthreshold for stimulating motor cortex in active subjects were used, and motor cortex excitability was again tested before and after the train by measuring the EMG response to a standard test shock. Stimuli given over a point 3cm anterior to the hand area could reduce motor cortical excitability in the resting state for up to 30min after the end of the train. There was no effect from stimulation over motor cortex itself. We conclude that it is possible to produce effects at distant sites by repetitive stimulation of one area of cortex. Finally, we examined the nature of this effect in more detail by testing how a subthreshold rTMS of premotor cortex affected paired pulse testing of motor cortex excitability. The train reduced the amount of cortico-cortical inhibition in motor cortex for up to 30min after the train, but only at a small range of interstimulus intervals (6, 7ms). This implies that the train over premotor areas affected the excitability of intrinsic circuits in the motor cortex. Thus conditioning one part of the cortex with TMS can affect the way another cortical area processes incoming data. Tracking developmental change in the human motor system with TMS. Janet Eyre University of Newcastle Medical School Abstract not yet availablenot yet available Visual awareness studied by transcranial magnetic stimulation in blindsight and in retinal blindness Alan Cowey University of Oxford Subject G.Y. is a much studied hemianope whose left striate cortex is almost totally destroyed. He has blindsight (good visual detection and discrimination without visual awareness) for many visual stimuli presented within his blind-hemifield but sometimes reports faint conscious visual percepts especially for swiftly moving stimuli of high luminance contrast. To determine whether the latter reflect residual visual processing in extra-striate visual area MT/V5 we used repetitive transcranial magnetic stimulation (rTMS) over MT/V5 in his normal and his damaged hemisphere. On the normal side, ""silvery swirling"" visual phosphenes could be elicited by rTMS applied at an area over the position of MT/V5 as previously shown in this subject by fMRI.; but not by stimulating MT/V5 in the damaged hemisphere even following dense sampling of the adjacent regions. However, phosphenes were occasionally produced in the hemianopic field by TMS close to the midline which probably stimulated V3 of the damaged hemisphere. The size and position of the phosphenes was determined by asking the subject to fixate the centre of radial graph paper while rTMS was delivered and to mark the centre and edges of any phosphene on the paper. In contrast, phosphenes could be elicited in a peripherally blinded subject by stimulating MT/V5 in either hemisphere by rTMS. The phosphenes were achromatic and within 20 degree of the visual axis, the latter estimated by asking the blind subject to ""fixate"" the forefinger of one hand held at the centre of the graph paper while he used the other hand to mark any phosphene. We also studied the effects of rTMS in the blind subject when delivered over the sagittal midline. ""Bright"", often coloured, but stationary phosphenes were readily elicited with increasing eccentricity in the lower visual field the more rostral the rTMS. We conclude that even years after the eyes have been severed from the brain, striate cortex and visual area MT/V5 remain excitable and can generate visual percepts. But in the absence of V1, magnetic stimulation of area V5 on that side no longer yields conscious visual percepts, at least with the stimulation conditions we used. Unravelling movement and action with TMS Patrick Haggard Institute of Cognitive Neuroscience, University College London The information-processing underlying human action is characterised by a hierarchy, extending from very high-level intentions (e.g., ""I want a drink"") to the lowest level of motorneuronal firing and muscle contraction. The psychology of action has greatly suffered from an inability to control the input to, and to selectively interfere with, the higher and lower levels of control. TMS over the motor cortex can produce involuntary movements, which correspond in some ways to normal operation of the lower (movement) part of the hierarchy, without operation of the higher (action) part. As such, they provide an important method for psychologists wishing to individuate the two kinds of processing. I shall report a number of behavioural and psychophysical studies which have taken this approach. First, I shall report data showing that the conscious awareness of (TMSinduced) movement and the conscious awareness of intentional action differ fundamentally. Second, I shall report data on attempts to use TMS in a completely differen way, in which, rather than evoking movements at the lower level of the hieararchy, it is used to try to intervene on the higher processes in the hierarchy directly. Using TMS to study attention Matthew Rushworth University of Oxford Transcranial magnetic stimulation (TMS) can be used to transiently disrupt the normal patterns of neuronal activity in association cortex. While TMS is being applied , a brain area will be unable to function normally. TMS, like the investigation of permanent brain lesions, can provide evidence about whether a brain area is critical for a cognitive process. In addition TMS, because it only induces a very brief disruption, can be used to investigate when a brain area is playing its most critical role. In a recent experiment we used fMRI to identify areas of blood oxygen level dependent (BOLD) signal increase in the dorsomedial frontal cortex during two set switching tasks. One task required subjects to switch between two different attentional sets involving the allocation of attention to different stimulus features. In the second task subjects switched between two different intentional or response sets. Although there were BOLD signals increases in the dorsomedial frontal cortex in both experiments, TMS only disrupted performance in the second case of intentional set switching. Further experiments demonstrated that medial frontal TMS only disrupted task performance when it was delivered at the time of actual set-switching, not when it was delivered at the time that subjects selected individual task responses. Comparing the effects of medial frontal and dorsal premotor TMS revealed a temporal double dissociation of function. The time course of false recognition memory Evan Heit, Noellie Brockdorff and Koen Lamberts University of Warwick In the Deese/Roediger-McDermott false memory paradigm (e.g., Roediger & McDermott, 1995), after subjects study a list of inter-related words, they falsely recognise semantically similar lures nearly to the extent that they correctly recognise studied items. This result seems to persist even when subjects are forewarned about the nature of the illusion. In two experiments we examined the time course of false recognition judgments. Using an old-new recognition task, we elicited responses at different time lags from presentation of stimulus through a response-signal procedure. We investigated the effects of study list length and forewarning on the time course of false recognition judgments and examined the results in terms of changes in accuracy and changes in response bias over time. We report cha"
b12d7f382274291e9243259ff85ff69e744c8a02,
c169d4bf11b0255583c19cd8c76221ef3ef414d5,"Recent research has demonstrated that, when instructed to prioritize a serial position in visual working memory (WM), adults are able to boost performance for this selected item, at a cost to nonprioritized items (e.g., Hu, Hitch, Baddeley, Zhang, & Allen, 2014). While executive control appears to play an important role in this ability, the increased likelihood of recalling the most recently presented item (i.e., the recency effect) is relatively automatic, possibly driven by perceptual mechanisms. In 3 Experiments 7 to 10 year-old’s ability to prioritize items in WM was investigated using a sequential visual task (total N = 208). The relationship between individual differences in WM and performance on the experimental task was also explored. Participants were unable to prioritize the first (Experiments 1 and 2) or final (Experiment 3) item in a 3-item sequence, while large recency effects for the final item were consistently observed across all experiments. The absence of a priority boost across 3 experiments indicates that children may not have the necessary executive resources to prioritize an item within a visual sequence, when directed to do so. In contrast, the consistent recency boosts for the final item indicate that children show automatic memory benefits for the most recently encountered stimulus. Finally, for the baseline condition in which children were instructed to remember all 3 items equally, additional WM measures predicted performance at the first and second but not the third serial position, further supporting the proposed automaticity of the recency effect in visual WM."
e6f58cf042bee635eea2ff5559f2d479741f9e43,
e7ea9aa6941872b967cc784b06cf8156547b0ad9,
f083d79e9fac15553b4ca081e76ae06c22ff9a27,
1b12b9f96e23b05f44a4dc150a7efcd7999cde68,
944e94fe4905d0da60ca185237e91e8566399a9c,"The study of human long-term memory has for over 50 years been dominated by research on words. This is partly due to lack of suitable nonverbal materials. Experience in developing a clinical test suggested that door scenes can provide an ecologically relevant and sensitive alternative to the faces and geometrical figures traditionally used to study visual memory. In pursuing this line of research, we have accumulated over 2000 door scenes providing a database that is categorized on a range of variables including building type, colour, age, condition, glazing, and a range of other physical characteristics. We describe an illustrative study of recognition memory for 100 doors tested by yes/no, two-alternative, or four-alternative forced-choice paradigms. These stimuli, together with the full categorized database, are available through a dedicated website. We suggest that door scenes provide an ecologically relevant and participant-friendly source of material for studying the comparatively neglected field of visual long-term memory."
b3da68ed82630759a9ac5ace48d61c245136c760,
01d58a6fee031117cf3b36101bce10a3351a57c7,
0c020cecc9157ccaabf60834eb96c6b9cda4af8e,"Testes rapidos (screenings) de avaliacao de leitura sao instrumentos essenciais para professores e profissionais no âmbito educacional. Este estudo adaptou para o portugues o subteste de leitura de palavras doReading Decision Test (RDT), o qual tem duas formas equivalentes, compostas por palavras e pseudopalavras. O processo de adaptacao seguiu as regras recomendadas pela literatura internacional e foi aplicado em uma amostra de 230 escolares, do 1o ao 5o ano do ensino fundamental para analisar as propriedades psicometricas iniciais. Observou-se que as analises de itens e da velocidade de leitura mostraram efeito significativo do ano escolar (p< 0,05). Verificou-se tambem forte correlacao entre as duas formas do subteste. Conclui-se que o subteste de palavras do RDT apresenta propriedades psicometricas iniciais adequadas."
33f7dd0eb0b2e60e5a97e98f7175fb461c68e8f3,
48f0b901a4fde04463aae7ce3012d2e4e768ce78,
4ea1c37895e369430eabdb96e45248c8bc057826,
80fa0e1bc025f0c8649d191742a93d36e0d68a18,
837b8bfe0b69053bac788d9f7a66ec4267f1edf6,"The concreteness effect in verbal short-term memory (STM) tasks is assumed to be a consequence of semantic encoding in STM, with immediate recall of concrete words benefiting from richer semantic representations. We used the concreteness effect to test the hypothesis that semantic encoding in standard verbal STM tasks is a consequence of controlled, attention-demanding mechanisms of strategic semantic retrieval and encoding. Experiment 1 analysed the effect of presentation rate, with slow presentations being assumed to benefit strategic, time-dependent semantic encoding. Experiments 2 and 3 provided a more direct test of the strategic hypothesis by introducing three different concurrent attention-demanding tasks. Although Experiment 1 showed a larger concreteness effect with slow presentations, the following two experiments yielded strong evidence against the strategic hypothesis. Limiting available attention resources by concurrent tasks reduced global memory performance, but the concreteness effect was equivalent to that found in control conditions. We conclude that semantic effects in STM result from automatic semantic encoding and provide tentative explanations for the interaction between the concreteness effect and the presentation rate."
9b80513eda717fa28d6eeda0839667feaec80e66,
de168ccaaa217ed70140f7256080f277ea144983,"Humans have an astonishing ability to remember with high fidelity previously viewed scenes with robust memory for visual detail (Konkle et al., 2010). To better understand the mechanism that affords us this massive memory we investigated the automaticity of encoding into visual long-term memory. We studied this in two ways across three experiments. First, measuring the effect of limiting the time of encoding by varying the allotted time to encode each image while keeping overall time of study constant. Second, measuring the effect of an attentionally-demanding concurrent task on subsequent retention by systematically varying the levels of demand imposed by the concurrent executive task. If encoding is automatic neither shorter exposure nor concurrent demand should influence subsequent recognition. If executive attention is required than memory performance should decline as load is increased and encoding time decreases. We tested scene memory using a standard massive memory paradigm with a heterogeneous (452 real complex scenes) and a homogenous (304 doors) image set examining if time of encoding and concurrent tasks affects scene memorability. Even when encoding a very rich heterogeneous set of images the encoding time mattered, with a significant reduction in performance from 3 seconds (d'= 1.11) to 1 second (d'=.60) study time. Interestingly, further reduction in encoding time to 0.5 seconds shows no significant decrement suggesting that encoding might follow a two-step process. Further, high fidelity encoding is reduced when during encoding there is a competing working memory task both for heterogeneous (d'=.91 for no load to d'=.41 for high load) as well as homogenous (d'=.65 for no load to d'=.21 for high load) image sets with amplified reduction when there is little idiosyncratic detail in the image. Results suggest that visual recognition memory encoding is the outcome of a two-stage rather than an automatic process. Meeting abstract presented at VSS 2015."
3077d8f1a83c995ee0230df2e65b0832544aaf5e,
31acba72c13229d1ccda65238ee6c3dce07384c5,
3fa313577203c921e49ee4a56dab25fe52fc8985,
64c4911b6298c2cb79c6f328f631a0f02f2a0162,
69b0a37009924c02905fde5cd0ab62ae5d0792c3,
723cb5b66ce48e56bb611066237dfe513a6a5c0d,
9d2d496fc9147a21aedf5f2f38c05b724b835ba6,"Purpose Closing the eyes during recall can help witnesses remember more about a witnessed event. This study examined the effectiveness of eye-closure in a repeated recall paradigm with immediate free recall followed 1 week later by both free and cued recall. We examined whether eye-closure was more or less effective during the second free-recall attempt compared with the first, whether eye-closure during the first recall attempt had an impact on subsequent free- and cued-recall performance, and whether eye-closure during the second free recall could facilitate the recall of new, previously unreported, information (reminiscence). Method Participants witnessed a videotaped event and participated in a first free-recall attempt (with eyes open or closed) a few minutes later. After a week, they provided another free recall, followed by a cued-recall interview (with eyes open or closed). Results Eye-closure during the first free-recall attempt had no significant effect on performance during any of the recall attempts. However, eye-closure during the second session increased the amount of correct visual information reported in that session by 36.7% in free recall and by 35.3% in cued recall, without harming testimonial accuracy. Crucially, eye-closure also facilitated the recall of new, previously unreported visual information. Conclusions The findings extend previous research in showing that the eye-closure instruction can still be effective when witnesses are interviewed repeatedly, and that it can facilitate the elicitation of new information. Thus, the eye-closure instruction constitutes a simple and time-efficient interview tool for police interviewers. © 2013 The British Psychological Society."
9fdf7e47587e1d2723f7f721d8dfbff91e6e4de7,"From vocabulary learning to imitating sequences of motor actions, the ability to plan, represent, and recall a novel sequence of items in the correct order is fundamental for many verbal and nonverbal higher level cognitive activities. Here we review phenomena of serial order documented across the verbal, visual, and spatial short-term memory domains and interpret them with reference to the principles of serial order and ancillary assumptions instantiated in contemporary computational theories of memory for serial order. We propose that functional similarities across domains buttress the notion that verbal, visual, and spatial sequences are planned and controlled by a competitive queuing (CQ) mechanism in which items are simultaneously active in parallel and the strongest item is chosen for output. Within the verbal short-term memory CQ system, evidence suggests that serial order is represented via a primacy gradient, position marking, response suppression, and cumulative matching. Evidence further indicates that output interference operates during recall and that item similarity effects manifest during both serial order encoding and retrieval. By contrast, the principles underlying the representation of serial order in the visual and spatial CQ systems are unclear, largely because the relevant studies have yet to be performed. In the spatial domain, there is some evidence for a primacy gradient and position marking, whereas in the visual domain there is no direct evidence for either of the principles of serial order. We conclude by proposing some directions for future research designed to bridge this and other theoretical gaps in the literature."
a0df20e125de8fbdf1b198ba9c82292abc4e3419,
a16f639d203b06e660cae5bc1537af273e6ee7ff,
a4f133041531dcebe4b46ffdac31f63b75a19802,"It has become increasingly clear that some patients with apparently normal memory may subsequently show accelerated long-term forgetting (ALF), with dramatic loss when retested. We describe a constrained prose recall task that attempts to lay the foundations for a test suitable for detecting ALF sensitively and economically. Instead of the usual narrative structure of prose recall tests, it employs a matrix structure involving four episodes, each describing a minor crime, with each crime involving the binding into a coherent episode of a specified range of features, involving the victim, the crime, the criminal and the location, allowing a total of 80 different probed recall questions to be generated. These are used to create four equivalent 20-item tests, three of which are used in the study. After a single verbal presentation, young and elderly participants were tested on three occasions, immediately, and by telephone after a delay of 6 weeks, and at one of a varied range of intermediate points. The groups were approximately matched on immediate test; both showed systematic forgetting which was particularly marked in the elderly. We suggest that constrained prose recall has considerable potential for the study of long-term forgetting."
b98724607451b0867633f0d901f26db4ca409a5f,"Four experiments studied the interfering effects of a to-be-ignored ""stimulus suffix"" on cued recall of feature bindings for a series of objects. When each object was given equal weight (Experiment 1) or rewards favored recent items (Experiments 2 and 4), a recency effect emerged that was selectively reduced by a suffix. The reduction was greater for a ""plausible"" suffix with features drawn from the same set as the memory items, in which case a feature of the suffix was frequently recalled as an intrusion error. Changing payoffs to reward recall of early items led to a primacy effect alongside recency (Experiments 3 and 4). Primacy, like recency, was reduced by a suffix and the reduction was greater for a suffix with plausible features, such features often being recalled as intrusion errors. Experiment 4 revealed a tradeoff such that increased primacy came at the cost of a reduction in recency. These observations show that priority instructions and recency combine to determine a limited number of items that are the most accessible for immediate recall and yet at the same time the most vulnerable to interference. We interpret this outcome in terms of a labile, limited capacity ""privileged state"" controlled by both central executive processes and perceptual attention. We suggest further that this privileged state can be usefully interpreted as the focus of attention in the episodic buffer."
bfd9c6bdfefb9c673e2cbd2224f2018adfd7bb8c,"How does executive attentional control contribute to memory for sequences of visual objects, and what does this reveal about storage and processing in working memory? Three experiments examined the impact of a concurrent executive load (backward counting) on memory for sequences of individually presented visual objects. Experiments 1 and 2 found disruptive concurrent load effects of equivalent magnitude on memory for shapes, colors, and colored shape conjunctions (as measured by single-probe recognition). These effects were present only for Items 1 and 2 in a 3-item sequence; the final item was always impervious to this disruption. This pattern of findings was precisely replicated in Experiment 3 when using a cued verbal recall measure of shape-color binding, with error analysis providing additional insights concerning attention-related loss of early-sequence items. These findings indicate an important role for executive processes in maintaining representations of earlier encountered stimuli in an active form alongside privileged storage of the most recent stimulus."
05af83c268e0e2164ed3815e07732e14bc1f8cae,
06aa49b01c63ad33ea930bd6bea9296b5a03947f,
1999566407e8b76d68f6785d2c2a5d5e3aa971e1,
2206a5f70104b52f47727fc7f596f835f2775255,
2725b3175746978a23b07a7db3cdaf78d9b7ce9a,
2fb1e729155227417d15e3b116fb24287ee5eaae,
35e0f20b6331ebb55dc591e1bf1e4781c4bb7483,
51549569d514dc4e618477d07d9e239e39c9aa48,
69020225b9cd9341f7d4d96490f24cad1a02ac19,"Recent attempts to assess the practical impact of scientific research prompted my own reflections on over 40 years worth of combining basic and applied cognitive psychology. Examples are drawn principally from the study of memory disorders, but also include applications to the assessment of attention, reading, and intelligence. The most striking conclusion concerns the many years it typically takes to go from an initial study, to the final practical outcome. Although the complexity and sheer timescale involved make external evaluation problematic, the combination of practical satisfaction and theoretical stimulation make the attempt to combine basic and applied research very rewarding."
96ef4b82ba10a44cadc74467a6e767f34c863bfb,"Raven's Matrices Test was developed as a “pure” measure of Spearman's concept of general intelligence, g. Subsequent research has attempted to specify the processes underpinning performance, some relating it to the concept of working memory and proposing a crucial role for the central executive, with the nature of other components currently unclear. Up to this point, virtually all work has been based on correlational analysis of number of correct solutions, sometimes related to possible strategies. We explore the application to this problem of the concurrent task methodology used widely in developing the concept of multicomponent working memory. Participants attempted to solve problems from the matrices under baseline conditions, or accompanied by backward counting or verbal repetition tasks, assumed to disrupt the central executive and phonological loop components of working memory, respectively. As in other uses of this method, number of items correct showed little effect, while solution time measures gave very clear evidence of an important role for the central executive, but no evidence for phonological loop involvement. We conclude that this and related concurrent task techniques hold considerable promise for the analysis of Raven's matrices and potentially for other established psychometric tests."
9c241fd0d24be89fd4fb05871459a8fb66d860ef,
c1beaff3e994ff3a6321c0bb34d3bf0fd53d4f15,"A broad theory of depression is proposed based on a blend of Hume's (D. Hume, 1978, A Treatise of Human Nature. Oxford, England: Oxford University Press. Original work published 1739) concept of a valenced world and Damasio's (A. R. Damasio, 1994, Descartes' Error: Emotion, Reason, and the Human Brain. New York, NY: Putnam) somatic marker hypothesis. It is proposed that behavior is governed by a hedonic detector system coupled to working memory. Depression is assumed to reflect a malfunction of the hedonic detector, involving the inappropriate setting of its positive−negative neutral point. This simple assumption allows a coherent account of depression that links the neurobiological, behavioral, and psychological evidence with a plausible evolutionary hypothesis. Alternative hypothesis are discussed, and methods of investigating the hedonic detector are suggested."
d1a257ca9c79fdb014355c4ed443f3908eefc0db,
f474c32428725e82ff962f3d95ed2513715b5ed2,"The spatiotemporal profile of activation of the prefrontal cortex in verbal and non-verbal recognition memory was examined using magnetoencephalography (MEG). Sixteen neurologically healthy right-handed participants were scanned whilst carrying out a modified version of the Doors and People Test of recognition memory. A pattern of significant prefrontal activity was found for non-verbal and verbal encoding and recognition. During the encoding, verbal stimuli activated an area in the left ventromedial prefrontal cortex, and non-verbal stimuli activated an area in the right. A region in the left dorsolateral prefrontal cortex also showed significant activation during the encoding of non-verbal stimuli. Both verbal and non-verbal stimuli significantly activated an area in the right dorsomedial prefrontal cortex and the right anterior prefrontal cortex during successful recognition, however these areas showed temporally distinct activation dependent on material, with non-verbal showing activation earlier than verbal stimuli. Additionally, non-verbal material activated an area in the left anterior prefrontal cortex during recognition. These findings suggest a material-specific laterality in the ventromedial prefrontal cortex during encoding for verbal and non-verbal but also support the HERA model for verbal material. The discovery of two process dependent areas during recognition that showed patterns of temporal activation dependent on material demonstrates the need for the application of more temporally sensitive techniques to the involvement of the prefrontal cortex in recognition memory."
faece903162440c0487026dc7149014aa8e85ef9,
1e6654a5593b48c3304537c45e7044e620a6a947,
1fc73edafa4c1d4ea3bfbae73029a5d250eec202,
395b25316e511d35e124840ce6c7e314fbed8cf3,"Previous research has shown that closing the eyes can facilitate recall of semantic and episodic information. Here, two experiments are presented which investigate the theoretical underpinnings of the eye-closure effect and its auditory equivalent, the “ear-closure” effect. In Experiment 1, participants viewed a violent videotaped event and were subsequently interviewed about the event with eyes open or eyes closed. Eye-closure was found to have modality-general benefits on coarse-grain correct responses, but modality-specific effects on fine-grain correct recall and incorrect recall (increasing the former and decreasing the latter). In Experiment 2, participants viewed the same event and were subsequently interviewed about it, either in quiet conditions or while hearing irrelevant speech. Contrary to expectations, irrelevant speech did not significantly impair recall performance. This null finding might be explained by the absence of social interaction during the interview in Experiment 2. In conclusion, eye-closure seems to involve both general and modality-specific processes. The practical implications of the findings are discussed."
4e181572e500562fe2e854c10b386d83bfde7684,"We aimed to resolve an apparent contradiction between previous experiments from different laboratories, using dual-task methodology to compare effects of a concurrent executive load on immediate recognition memory for colours or shapes of items or their colour–shape combinations. Results of two experiments confirmed previous evidence that an irrelevant attentional load interferes equally with memory for features and memory for feature bindings. Detailed analyses suggested that previous contradictory evidence arose from limitations in the way recognition memory was measured. The present findings are inconsistent with an earlier suggestion that feature binding takes place within a multimodal episodic buffer Baddeley, (2000) and support a subsequent account in which binding takes place automatically prior to information entering the episodic buffer Baddeley, Allen, & Hitch, (2011). Methodologically, the results suggest that different measures of recognition memory performance (A′, d′, corrected recognition) give a converging picture of main effects, but are less consistent in detecting interactions. We suggest that this limitation on the reliability of measuring recognition should be taken into account in future research so as to avoid problems of replication that turn out to be more apparent than real."
51fbd550929b79ef9a6509d88c8df393a0cf2b05,
6da7dbf4b0a434cf28c13b9300fe78ed5b798d78,"In an attempt to account for the impact of emotion on cognition, Baddeley (2007) proposed the existence of a hedonic detection system. Malfunctioning of this system was assumed to play a crucial role in depression. Exploring this hypothesis requires a simple and rapid way of assessing the neutral point of proposed hedonic detector. We describe two experiments that aim to develop such a method of investigating this system. Both are based on the assumption that the hedonic judgement of simple stimuli will be influenced by the valence of an induced mood. Experiment 1 showed that a negative mood leads to the more negative evaluation of words than the positive mood. Experiment 2 also includes a neutral condition and the evaluation of words, pictures, and faces. In each case the negative mood led to lower hedonic ratings, whereas no difference was found between neutral and positive moods. Implications for further investigating the hypothetical hedonic detector are discussed."
9bf3676600273fdcda4f6f842db144abe19d270a,"BACKGROUND
Recent studies have shown that individuals with Down syndrome (DS) are poorer than controls in performing verbal and visuospatial dual tasks. The present study aims at better investigating the dual task deficit in working memory in individuals with DS.


METHOD
Forty-five individuals with DS and 45 typically developing children matched for verbal mental age completed a series of verbal and visuospatial working memory tasks, involving conditions that either required the combination of two tasks in the same modality (verbal or visual) or of cross-modality pairs of tasks.


RESULTS AND CONCLUSIONS
Two distinct deficits were found in individuals with DS: impairment in verbal tasks and further impairment in all dual task conditions. The results confirm the hypothesis of a central executive impairment in individuals with DS."
cd7321b9aac84fc6414a60499ee6b4b67fe00a8d,
d4a6dd8cff822d7db3f3ca1719084264d66ed886,
e1c4cf51955452fa9a5df71fab2fac2d74b001b2,"I present an account of the origins and development of the multicomponent approach to working memory, making a distinction between the overall theoretical framework, which has remained relatively stable, and the attempts to build more specific models within this framework. I follow this with a brief discussion of alternative models and their relationship to the framework. I conclude with speculations on further developments and a comment on the value of attempting to apply models and theories beyond the laboratory studies on which they are typically based."
181ac5934d7712938b74488b3e7808c14c2b3c70,
5d2e8cc4ff49766f089d3d0c1491cb9c1a8714fa,"A larger normative sample has been collected than previous versions of the test It was thought by the authors that some of the subtests in the currently used versions can prove to be a little too difficult (RBMT-E) or a little too easy (RBMT) for certain patients. The RBMT-3 sought to make adjustments in item difficulty to meet this need. On the Face Recognition subtests of the original RBMT the ethnic diversity of the local population was not adequately represented. Therefore, the new version includes more pictures of people of African-Caribbean and Asian origin to ensure the test is appropriate for a multi-racial society such as the United Kingdom and the United States of America. The stories used to assess a person’s ability to absorb verbal information have also been updated. A new subtest – the Novel Task has been added. This novel task assesses the ability of a person to learn a new skill, an accomplishment critical for everyday functioning. An intervention chapter has been added to improve the clinical utility of the tool. Finally, given that the RBMT-E was published in 1999, a revision of the normative data is required. We stay close to the original structure of the RBMT as this has proved both valid and sensitive to everyday memory problems in people with brain injury. It is hoped that this new version of the RBMT will enhance its function as both a clinical and a research tool."
709d39e28524de976823b008a9b234fb02e95dc2,
94ef44f8c4165d18fba840159ea98c3576c5acb6,"A number of studies suggest an important role for the hippocampus in tasks involving visuospatial or relational working memory. We test the generality of this proposal across tasks using a battery designed to investigate the various components of working memory, studying the working memory performance of Jon, who shows a bilateral reduction in hippocampal volume of approximately 50%, comparing him to a group of 48 college students. We measure performance on four complex working memory span measures based on combining visuospatial and verbal storage with visuospatial or verbal concurrent processing as well as measuring Jon's ability to carry out the component storage and processing aspects of these tasks. Jon performed at a consistently high level across our range of tasks. Possible reasons for the apparent disparity between our own findings and earlier studies showing a hippocampal deficit are discussed in terms of both the potential differences in the demands placed on relational memory and of the proposed distinction between egocentric and allocentric visuospatial processing."
b7622aedb9f1c04f3d6e234adc9a9f366e33132e,"Three experiments investigated the contribution of phonological short-term memory (STM) to grammar learning by manipulating rehearsal during study of an auditory artificial grammar made up from a vocabulary of spoken Mandarin syllables. Experiment 1 showed that concurrent, irrelevant articulation impaired grammar learning compared with a nonverbal control task. Experiment 2 replicated and extended this finding, showing that repeating the grammatical strings at study improved grammar learning compared with suppressing rehearsal or remaining silent during learning. Experiment 3 found no effects of rehearsal on grammar learning once participants had learned the component syllables. The findings suggest that phonological STM aids artificial grammar learning via effects on vocabulary learning."
bcbe14bcc35f7e4444d3baacaffa8338a7c885d4,"The purpose of this chapter is to apply insights from cognitive psychology on working memory to everyday self-regulation. We will first introduce contemporary views surrounding the multi-component-view of working memory. Our emphasis will be on the central executive as the component that is assumed to orchestrate perceptual, cognitive, and motor processes in the service of goal pursuit. We will then spell out in more detail how working memory may benefit self-regulation. Research pertaining to momentary fluctuations in working memory capacity will be reviewed, as well as research highlighting the role of working memory capacity in the control of attention, thought, emotion, and action. We conclude by discussing why in our view there is no simple mapping of working memory operations on the distinction between conscious and non-conscious processing."
c8670e6f2fce9d11ffe379ecdbf81ac05d7f75ef,
ce9db34f9a0efe5348306960cf8a3bb2b58f80cf,"Two studies that examine whether the forgetting caused by the processing demands of working memory tasks is domain-general or domain-specific are presented. In each, separate groups of adult participants were asked to carry out either verbal or nonverbal operations on exactly the same processing materials while maintaining verbal storage items. The imposition of verbal processing tended to produce greater forgetting even though verbal processing operations took no longer to complete than did nonverbal processing operations. However, nonverbal processing did cause forgetting relative to baseline control conditions, and evidence from the timing of individuals' processing responses suggests that individuals in both processing groups slowed their responses in order to ""refresh"" the memoranda. Taken together the data suggest that processing has a domain-general effect on working memory performance by impeding refreshment of memoranda but can also cause effects that appear domain-specific and that result from either blocking of rehearsal or interference."
d16ccad385ae0df6cf8c317bec961842d2995efd,
e720f16910bf2cf4d3cf7e0d1616586b25815a3b,
eca3ef87c42c814909ad3f09c4e28610f7eda5b0,
04629f2ad30c96fc47fa73abdb9b20572d9cb5b6,
104ab03ca0f5b16d6e17ed344bfcec1612b4b6f2,"Studying short-term memory within the framework of the working memory model and its associated paradigms (Baddeley, 2000; Baddeley & Hitch, 1974) offers the chance to compare similarities and differences between the way that verbal and tonal materials are processed. This study examined amateur musicians’ short-term memory using a newly adapted version of the visual-auditory (V-A) recognition method (Schendel & Palmer, 2007) within the framework of an irrelevant sound paradigm. We report evidence for a modality specific irrelevant sound effect: irrelevant tones disrupted memory for sequences of tones, whilst only irrelevant speech disrupted memory for sequences of letters. These preliminary results suggest that the adapted V-A recognition method will be useful for future parallel investigations of short-term memory for verbal and tonal materials."
247dc5c6e48bbcd3882e2b529bad149ca63acc43,
35ab61222742927f539b06acaf65692d6f221656,
3809ba6e2f1cb3bd96237be5800081d09e846a59,"Abstract: Psychological and neuropsychological evidence indicates that human memory can be seen as comprising a number of separable but interacting memory systems. Information from the environment is first processed through a series of brief sensory memory systems that can best be regarded as part of the processes of perception. Information then flows directly into long-term memory, and in parallel into working memory, a system for keeping information “in mind”, while performing complex cognitive activities such as reasoning, learning and comprehending. Working memory can be decomposed into four subsystems comprising at the central executive, an attentionally limited control system, two modality-based storage systems one for acoustic-verbal information, the phonological loop, and the other for visuo-spatial information. A fourth component the episodic buffer provides a multidimensional temporary store that links these components with long-term memory and perception. Long-term memory can be split into two broad categories, explicit and implicit. Explicit memory comprises episodic memory, our capacity to recollect specific experiences; it is this aspect of memory that is particularly vulnerable to disease or brain damage. The second component is semantic memory which stores our knowledge of the world. There is a range of implicit systems which accumulate information that can later be used, but do not require conscious retrieval. Examples include acquiring motor skills, classical conditioning, perceptual priming and the general acquisition of habits."
534533442e64c5af31cb6f69111e6282a88794be,"A brief account is presented of the three-component working memory model proposed by Baddeley and Hitch. This is followed by an account of some of the problems it encountered in explaining how information from different subsystems with different codes could be combined, and how it was capable of communicating with long-term memory. In order to account for these, a fourth component was proposed, the episodic buffer. This was assumed to be a multidimensional store of limited capacity that can be accessed through conscious awareness. In an attempt to test and develop the concept, a series of experiments have explored the role of working memory in the binding of visual features into objects and verbal sequences into remembered sentences. The experiments use a dual task paradigm to investigate the role of the various subcomponents of working memory in binding. In contrast to our initial assumption, the episodic buffer appears to be a passive store, capable of storing bound features and making them available to conscious awareness, but not itself responsible for the process of binding."
5b043f3a4d2511832e635a8f739fd35c365f5c23,
97269108fc677064baa86a8459aef58bc8984295,
a97a3c44fe5bde195e710092acb36995e15609b8,
b58c90b06e000f05b90d7d74e6bb3b8f765cdb36,
028b20ca4a60da7bf87fa974f0b8b03ba359b9a4,
525a06861fec069e083ab38ed436878ab9390e14,"In a re-examination of the recognition memory of Jon, a young adult with developmental amnesia due to perinatal hippocampal damage, we used a test procedure that provides estimates of the separate contributions to recognition of recollection and familiarity. Comparison between Jon and his controls revealed that, whereas he was unimpaired in the familiarity process, he showed abnormally low levels of recollection, supporting the view that the hippocampus mediates the latter process selectively."
5e2baa0e2b7e9f6df25c3f82edf1a7d8b2a43e7a,
628b22f2ae4b233c4afabc533c57a2878f420d08,"We present a new analysis of our previously published corpus of handwriting errors (slips) using the proportional allocation algorithm of Machtynger and Shallice (2009). As previously, the proportion of slips is greater in the middle of the word than at the ends, however, in contrast to before, the proportion is greater at the end than at the beginning of the word. The findings are consistent with the hypothesis of memory effects in a graphemic output buffer."
90b3a4b563873f8790a14a550c6a9c9daa26eed9,
956ac2f881c30498e0f3693074ae28c0d169ebc7,"Using positron emission tomography (PET), we explored the neural correlates of an executive function, dual tasking, in patients with amnestic mild cognitive impairment (aMCI) and in elderly controls. The experiment employed simple auditory and visual tasks that were presented both in isolation and simultaneously to create a task condition requiring enhanced attentional control. Behaviorally, both groups performed well, albeit the patients made more errors on the visual task. The PET analysis focused at prefrontal regions where group differences in task-related activation patterns were expected. During dual task performance, the patients showed attenuated activity in the left inferior frontal region when compared to the controls. This suggests abnormalities in the neural processes underlying attentional control in aMCI."
d12da7397739a781e9b22dab568d3a623d5f3e19,"We examine the role of general attention in the binding of colour and shape across the visual and verbal modalities. Three experiments studied the effects of concurrent tasks on the binding and retention of either unified visual stimuli, namely coloured shapes, or cross-modal stimuli in which one feature involved visual and the other auditory presentation. Performance accuracy was broadly equivalent across conditions, and was unimpaired by spatial tapping but impaired by backward counting. The decrement was however, no greater for the cross-modal binding conditions, suggesting that the act of binding is not itself attention demanding. Implications for this unexpected finding are discussed."
e2b4596094136af612b87ef06ac22220d990f337,
306a27fe373de1876ffb66af67b71ee8be5d6c94,
6120de55be4411a15005cb8b4366ecb609b4b3ee,"Working Memory and DyslexiaRoderick I. Nicolson*, Angela J. Fawcett* and Alan D. Baddeley†*Department of Psychology † MRC Applied Psychology UnitUniversity of Sheffield CambridgeAbstractRecent research has demonstrated that dyslexic children suffer impairments inboth phonological skills and in working memory performance. In principleeither a phonological deficit or a working memory deficit could underlie bothsets of symptoms. This issue was addressed via a series of experimentsdesigned to explore the relationships between dyslexia, age, phonologicalperformance and working memory. Two groups of dyslexic childrenparticipated, with mean ages 15 and 11 years respectively, together with threegroups of non-dyslexic children (mean ages 15, 11 and 8 years) matched for IQwith the dyslexic children and selected so as to provide both chronological andreading age controls for the two dyslexic groups. The performance of the olderdyslexics across the range of tasks was slightly but not significantly worse thantheir chronological age controls and indistinguishable from that of their readingage controls. By contrast, the performance of the younger dyslexics wassignificantly worse than that of their chronological age controls on phonologicaldiscrimination, articulation rate, and nonword repetition, and was significantlyworse even that of their reading age controls on repetition of longer nonwords.On tests of memory span, all five groups showed the normal phonologicalsimilarity effect and the normal word length effect. When memory span wasregressed as a function of articulation rate, there was no evidence of impairedslope or intercept for the dyslexic children. It is concluded that the 11 year olddyslexic children show residual problems on phonological processing,especially for tasks involving unfamiliar stimuli, but that by their mid-teensdyslexic children have overcome these problems. The major remaining problemfor the dyslexic children appears to be a continuing lack of fluency inarticulation, a factor which is sufficient to account for the slight deficits onmemory span. Furthermore, it is argued that the articulation rate deficitprovides a parsimonious explanation of the range of deficits shown by theyounger dyslexic children. It is concluded that neither impaired phonologicalskills nor impaired working memory is sufficient in itself to explain the deficits,but that some deeper explanation must be sought."
bfe39bf023448f13014f22203673991c440ebac3,"The attentional blink paradigm was used to examine whether emotional stimuli always capture attention. The processing requirement for emotional stimuli in a rapid sequential visual presentation stream was manipulated to investigate the circumstances under which emotional distractors capture attention, as reflected in an enhanced attentional blink effect. Emotional distractors did not cause more interference than neutral distractors on target identification when perceptual or phonological processing of stimuli was required, showing that emotional processing is not as automatic as previously hypothesized. Only when semantic processing of stimuli was required did emotional distractors capture more attention than neutral distractors and increase attentional blink magnitude. Combining the results from 5 experiments, the authors conclude that semantic processing can modulate the attentional capture effect of emotional stimuli."
c8edf636cd6e35d972ac72859784c1e98293db4e,
3d193f507a3902a0feacb5903c8c532d39da32f2,
d3cb1568fcbb5bae752c863fbbdef6def4bc07c3,
e34cbb62c45464d91b7205c9e4dd859bf09bff27,
e4559ce71e57774e008eb47231bd0adab37e3839,"It has been suggested that certain theoretically important anomalous results in the area of verbal short-term memory could be attributable to differences in strategy. However there are relatively few studies that investigate strategy directly. We describe four experiments, each involving the immediate serial recall of word sequences under baseline control conditions, or preceded by instruction to use a phonological or semantic strategy. Two experiments varied phonological similarity at a presentation rate of one item every 1 or 2 seconds. Both the control and the phonologically instructed group showed clear effects of similarity at both presentation rates, whereas these were largely absent under semantic encoding conditions. Two further experiments manipulated word length at the same two rates. The phonologically instructed groups showed clear effects at both rates, the control group showed a clear effect at the rapid rate which diminished with the slower presentation, while the semantically instructed group showed a relatively weak effect at the rate of one item per second, and a significant reverse effect with slower presentation. The latter finding is interpreted in terms of fortuitous differences in inter-item rated associability between the two otherwise matched word pools, reinforcing our conclusion that the semantically instructed group were indeed encoding semantically. Implications for controlling strategy by instruction are discussed."
024ebc0ef801890b2e144452cc15030a2c415d27,
07b176af461d8d8aa6834bbf74b087b8c9a91651,
2946da33c4a7db4b187d9ab010f67d5f6beedd6f,
3c7832ca0bb7d12ada4f8009112395d4ea89dde4,"Jones, Hughes, and Macken (2007) claim that their data and our own are inconsistent with a multicomponent working-memory model. We explain in greater detail how the model can account for the data and can address their more specific criticisms. Both sides accept that data relating to the presence of a phonological similarity effect throughout the list depend on list length. We accept that, at this point, all explanations of their interaction are speculative and require further empirical investigation. We examine J, H, & M's interpretation of their and our results in terms of an auditory modality effect, observing that their interpretation of this effect is not well supported by the literature. We suggest that their account assumes a very narrow basis for a general theory of short-term retention, in contrast to a phonological loop interpretation, which forms part of a well-developed and articulated model of working memory."
6486fcc30584a969cbf225c390eba94e0b1e1c24,
64efbde83faf95a14d222d6df664619d594acf5e,
7f1ffe5c5e5421f32313799e722c9a5963275084,"A brief account is given of the evolution of the concept of working memory from a unitary store into a multicomponent system. Four components are distinguished, the phonological loop which is responsible for maintaining speech-based information, the visuospatial sketchpad performing a similar function for visual information, the central executive which acts as an attentional control system, and finally a new component, the episodic buffer. The buffer comprises a temporary multidimensional store which is assumed to form an interface between the various subsystems of working memory, long-term memory, and perception. The operation of the model is then illustrated through an account of a research programme concerned with the analysis of working memory in Down syndrome."
87f9fbf8d2ac90b8200a1f5eeed9b6aa8c14425d,
8ce346c2828762980cf58eda89d4bd24be074e57,
a201c183a9d4fcb09b2eae7adecb9090651a1bbd,
a9e694164513391b56968613517a5b132bccae06,
b27fe81e91849e5f5b2b95dbd2b758a8af243c67,
b623e572a177337772914ca6ca658354a40f4a1b,
c96c19c1810e3a4eb8420d4e4373b6700c25f438,
e4e80411054e9921b6e7eb562ffb7fd5e5d5492b,
f020359ff64933e8c8d574be48847a2107ac202f,
fbb0aa52c0be173553e0cecea4c77a5457079049,"1. Introduction and overview 2. Why do we need a phonological loop? 3. The phonological loop: challenges and growing points 4. Visuospatial short-term memory 5. Imagery and the visuospatial sketchpad 6. Recency retrieval and the constant ratio rule 7. Fractionating the central executive 8. Long-term memory and the episodic buffer 9. Exploring the episodic buffer 10. Individual differences in working memory 11. What limits working memory span 12. Neuroimaging working memory 13. Working memory and social behaviour 14. Working memory and emotion I: fear and craving 15. Working memory and emotion II: depression and the well-springs of action 16. Working memory and consciousness 17. Multilevel control of action 18. Working memory in context: life, the universe and everything"
fd1f484db32696e972e2956c434c24703d2a36d3,"Jones et al. (Jones, Hughes, & Macken, 2006; Jones, Macken, & Nicholls, 2004) identify the interaction between phonological similarity, articulatory suppression, and stimulus presentation mode in verbal short-term memory as potentially providing important support for the phonological loop hypothesis. They find such an interaction but attribute it to “perceptual organization masquerading as phonological storage”. We present data using shorter letter sequences and find clear evidence of the interaction predicted by the phonological loop hypothesis, which, unlike the evidence of Jones et al., is not limited to recency, and which provides continued support for the phonological loop hypothesis."
fea85e5aca5ad617c90215e5e637906efee6ece8,
179692ca926694dca6a221cdfd88bc5a2f378b2e,
1c32b8c528df8b0adc1530da5cbe74c9b9a11975,"Working memory is the temporary storage system that is assumed to underpin our capacity for coherent thought. One working memory model (WMM) assumes an attentional control component, the central executive, together with two subsystems, the visuo-spatial sketchpad that is capable of storing visual and spatial information, and the phonological loop which holds and manipulates speech-like information. Although the WMM has been applied across a wide range of situations, there is little work on its application to music. The present study attempts to apply to music one of the major phenomena of the phonological loop, the observation that immediate recall of sequences of words or letters is impaired when they are similar in sound. (e.g. PCVTD vs. XKWYR). Two experiments were performed, in both of which subjects heard and attempted to reproduce sequences of notes that were either close together in pitch height (proximal) or far apart (distant). Memory for proximal sequences was poorer than for distant in both experiments, lending support to the possibility that the phonological loop may also be capable of holding musical sequences."
2e7f6db0f099acb7ac026d1d2f6097f72845e46d,"We report two experiments that investigated factors that might boost ‘episodic’ recall for Jon, a developmental amnesic whose episodic memory is gravely impaired but whose semantic memory seems relatively normal. Experiment 1 showed that Jon's recall improved following a semantic study task compared with a non-semantic study task, as well as following four repeated study trials compared with only one. Experiment 2 additionally revealed that Jon's recall improved after acting compared with reading action phrases at study, but only if the phrases were well integrated semantically. The results provide some support for the hypothesis that Jon's ‘episodic’ recall depends on the extent to which he is able to retrieve events using semantic memory."
2b512fe29d991c289d59f05d1677e901ddf7ee4b,
5095a10bace4ebf52f93bd53c15d57c8933f3fe0,"In the preface to this book, the editors reflect that the original Handbook of Memory Disorders (Baddeley et al., 1995) was aimed at a clinically oriented readership. Following publication of the original text, it was noted that neuroscientists expressed enthusiasm for the work, thus the second edition of the handbook (Baddeley et al., 2002) expanded significantly on scientific issues that were perhaps of less immediate clinical relevance. The current text, which the editors have titledThe Essential Handbook of Memory Disorders for Cliniciansis composed of a series of chapters from the 2002 text. Similar to the 1995 first edition, the goal of this new book is to provide an accessible text aimed at clinicians. Core clinical issues of assessment, nomenclature, phenomenology, etiology, and management are examined as they relate to disorders of memory and associated underlying diseases. The editors went about this task by selecting what they consider to be the more clinically relevant chapters from the second (2002) edition of The Handbook of Memory Disorders. The resulting text consists of a total of 15 chapters, authored for the most part by world-class clinicians and scientists. Although the book is not formally divided into sections, readers are likely to find themselves deriving four broad themes including contributions on the foundations of memory and amnesia, assessment, diseases underlying memory disorders, and rehabilitation and management. Although these broader themes are not covered in detail, the text does, in fact, generally address issues frequently confronted in practice. One could disagree with the inclusion or exclusion of selected chapters from the 2002 book to the current text, but in any case it is clear that the 15 chapters offered will be of value to clinicians. Appropriately, the book begins with two chapters that lay the foundation for subsequent topics in the book. Studies of the psychology of memory are reviewed, including familiar topics such as types of memory (e.g., short-term memory, long-term memory), and stages of memory (encoding, storage, retrieval). Neurologic conditions associated with amnesia (such as cerebral anoxia 0 schemia, Wernicke-Korsakoff syndrome, cerebral vascular accidents, and others) and subtypes of amnesia and underlying neural circuitry are examined. In this context, the chapter by O’Connor and Verfaellie entitled “The amnesic syndrome: Overview and subtypes” should be considered mandatory reading for its elegant review of some of the principal neurologic conditions underlying amnesia. A series of chapters dedicated to neurological conditions with associated memory disorders begins with an examination by Levin and Hanten of closed head injury as a cause for posttraumatic amnesia and residual memory deficits. The chapter is notable for its careful and commanding overview of the underlying empirical literature in this vast field. This is followed by well-informed chapters on psychogenic amnesia, acquired and congenital developmental amnesias, Alzheimer’s disease, and memory dysfunction in an array of conditions associated with subcortical dementia (e.g., Huntington’s disease, multiple sclerosis, Parkinson’s disease). A final series of chapters is aimed at clinically practical questions such as assessment of memory disorders, distinguishing disorders of memory from other cognitive impairments, and remediation and management of both children and adults with memory disorders. Noteworthy in this series are two chapters on assessment, one by Howeison and Lezak and another by Wilson. In combination, these chapters address some of the most clinically relevant issues in assessment of memory, and will be appreciated by readers as authoritative references. The book closes with a welcome chapter on the emotional and social consequences of memory disorders. The editors of this book aim squarely at the needs of practicing clinicians by addressing some of the core questions and conditions faced by neuropsychologists. Although the book would benefit from more explicit conceptual “grouping” of chapters into themes, in the big picture this is Journal of the International Neuropsychological Society (2005),11, 339–343. Copyright © 2005 INS. Published by Cambridge University Press. Printed in the USA."
5c702e171fc9b3dacf652c19c8507565eeb0b77c,"This experiment addresses the question of what makes a working memory measure a good predictor of higher‐level abilities. Verbal and visuospatial processing episodes were interleaved with distinct verbal and visuospatial storage episodes to form four complex span tasks. Although these measures were reliable predictors of reading and mathematics ability in children, they were no more predictive of these abilities than corresponding simple span tasks involving storage alone. However, when individual differences in storage ability and processing capacity were controlled for, residual variance in complex span performance was related to academic ability in some cases. These findings indicate that complex span tasks are multiply determined, and that differences in task structure can dramatically influence the relative importance of these multiple constraints and the predictive power of a complex span measure."
7a2846605888fe68df211a0c20dcf5f0196b3180,"Phonological similarity effects were used to assess the role of acoustic coding in verbal complex span, a processing-plus-storage measure found to correlate significantly with aspects of complex cognition. Three experiments demonstrated consistent effects of phonological similarity on listening span. These effects appeared relatively insensitive to manipulations of task materials (Experiment 1) and differences in processing task demands (Experiments 2 and 3). The results were interpreted as reflecting a significant role for the phonological loop in supporting verbal complex span and a multicomponent view of working memory, as tapped by these tests. Phonological similarity did not significantly interact with aspects of the tasks varied across Experiments 1 to 3, suggesting a relative robustness of the effect. However, variation in the phonological similarity effect sizes across Experiments 1 to 3 supports the suggestion that task demands and characteristics have the potential to disrupt the phonological similarity effect and, by implication, the reliance on a phonological code."
d67db7a5c172570832d3a73020a78bcf60c7c5d4,"This study investigated the constraints underlying developmental improvements in complex working memory span performance among 120 children of between 6 and 10 years of age. Independent measures of processing efficiency, storage capacity, rehearsal speed, and basic speed of processing were assessed to determine their contribution to age-related variance in complex span. Results showed that developmental improvements in complex span were driven by 2 age-related but separable factors: 1 associated with general speed of processing and 1 associated with storage ability. In addition, there was an age-related contribution shared between working memory, processing speed, and storage ability that was important for higher level cognition. These results pose a challenge for models of complex span performance that emphasize the importance of processing speed alone."
0b8f2dfb0c98126689a1a13caed27c5ec722de87,
4af06bf2eae509ba8e89fd1d1ce8984eef39ad31,
8c9ec73cc39f1937a092331074813b3d1b680cce,"To explore the relationship between short-term memory and speech production, we developed a speech error induction technique. The technique, which was adapted from a Japanese word game, exposed participants to an auditory distractor word immediately before the utterance of a target word. In Experiment 1, the distractor words that were phonologically similar to the target word led to a greater number of errors in speaking the target than did the dissimilar distractor words. Furthermore, the speech error scores were significantly correlated with memory span scores. In Experiment 2, memory span scores were again correlated with the rate of the speech errors that were induced from the task-irrelevant speech sounds. Experiment 3 showed a strong irrelevant-sound effect in the serial recall of nonwords. The magnitude of the irrelevant-sound effects was not affected by phonological similarity between the to-be-remembered nonwords and the irrelevant-sound materials. Analysis of recall errors in Experiment 3 also suggested that there were no essential differences in recall error patterns between the dissimilar and similar irrelevant-sound conditions. Weproposed two different underlying mechanisms in immediate memory, one operating via the phonological short-term memory store and the other via the processes underpinning speech production."
9e666b56ee6e4215cb7bbe60402bf9762b55b6a6,"BACKGROUND
The Neale Analysis of Reading Ability (NARA) (Neale, 1997) is widely used in education and research. It provides measures of reading accuracy (decoding) and comprehension, which are frequently interpreted separately.


AIMS
Three studies were conducted to investigate the degree to which the NARA measures could be separated.


SAMPLES
British 7- and 8-year-olds participated in Study 1 (N=114) and Study 2 (N=212). In Study 3, 16 skilled and less-skilled comprehenders were identified from the Study 2 sample.


METHODS
Study 1: By investigating their contribution to silent reading comprehension, the independence of NARA decoding and comprehension scores was determined. Study 2: Decoding groups matched for listening comprehension were compared on the NARA comprehension measure, and population performance was compared across listening comprehension and NARA reading comprehension. Study 3: Comprehension groups were compared on ability to answer open-ended and forced-choice questions.


RESULTS
Firstly, NARA comprehension performance depended on decoding, to the extent that children with high listening comprehension ability but low decoding ability attained low NARA comprehension scores. Secondly, 32% of children who attained low NARA comprehension scores exhibited high listening comprehension. Thirdly, comprehension groups differed when assessed with open-ended questions but not when assessed with forced-choice questions.


CONCLUSIONS
The NARA can underestimate the comprehension ability of children with weak decoding skills and children who have some difficulty with open-ended questions. The decoding and comprehension measures of the NARA cannot be separated. These findings have important implications for the interpretation of the measures provided by the NARA, in education and research."
a001c5ce80b9c2421d8117e647400da131b9b214,"The authors report 5 serial-recall experiments. In 4 of the 5 experiments, they show that irrelevant sound (IS) has a retroactive effect on material already in memory. In Experiment 1, IS presented during a filled retention interval had a reliable effect on list recall. Four further experiments, 3 of which used retroactive IS, showed that IS continued to-have an effect on recall following a long, filled retention interval. Articulatory suppression during visual input was found to abolish the long-lasting, retroactive effect of IS, supporting the idea that IS affects the phonological-loop component of short-term memory. IS also, therefore, seems to affect a longer term memory system with which the loop interacts."
ab6dd8743f3efd13f0d81a4738606bf415148fde,
d2a74048f2d8a85e75532132e628ec532934b855,"About the Editors. List of Contributors. Preface. Chapter 1: The Psychology of Memory (Alan D. Baddeley). Chapter 2: The Amnesic Syndrome: Overview and Subtypes (Margaret O' Connor and Mieke Verfaellie). Chapter 3: Posttraumatic Amnesia and Residual Memory Deficit after Closed Head Injury (Harvey S. Levin and Gerri Hanten). Chapter 4: Psychogenic Amnesia (Michael D. Kopelman). Chapter 5: Developmental Amnesias and Acquired Amnesias of Childhood (Christine M. Temple). Chapter 6: The Memory Deficit in Alzheimer's Disease (James T. Becker and Amy A. Overman). Chapter 7: Memory Disorders in Subcortical Dementia (Jason Brandt and Cynthia A. Munro). Chapter 8: Assessment of Memory Disorders (Barbara A. Wilson). Chapter 9: Separating Memory from Other Cognitive Disorders (Diane B. Howieson and Muriel D. Lezak). Chapter 10: Management and Remediation of Memory Problems in Brain-injured Adults (Barbara A. Wilson). Chapter 11: Assessment and Management of Memory Problems in Children (Judith A. Middleton). Chapter 12: Assessment and Intervention in Dementia of Alzheimer Type (Linda Clare). Chapter 13: Reducing the Impact of Cognitive Impairment in Dementia (Bob Woods). Chapter 14: External Memory Aids and Computers in Memory Rehabilitation (Narinder Kapur, Elizabeth L. Glisky and Barbara A. Wilson). Chapter 15: Emotional and Social Consequences of Memory Disorders (Robyn L. Tate). Author Index. Subject Index."
e80c84c9f7822f06796b9330357ef3f21d8c17ae,
ebe8dd9283a9c1e6572bbd66ecad6a3e7a16ebc7,"Three experiments compared groups of Alzheimer's disease (AD) patients and healthy older and younger participants on visuospatial tracking and digit sequence recall, as single tasks and performed concurrently. In Experiment 1, tasks were performed concurrently with very low demand relative to span. Only the AD patients showed a dual task deficit. In Experiment 2, single task demand was manipulated on each task from below span to above span for each individual. All groups showed the same performance reductions with increasing demand. In Experiment 3, demand on 1 task was constant, whereas demand on the concurrent task was varied. AD patients showed a clear dual task deficit but were no more sensitive than control groups to varying demand. Results suggest an identifiable cognitive resource for dual task coordination within a multiple component working memory system."
35bc8f0a3eb753ccbee78b28bff1004a9c67ba74,
5ec04ff463c5e966480e5db19a20d99d35deceaa,"Under appropriate conditions, immediate serial verbal recall is impaired by irrelevant speech, articulatory suppression, and syncopated tapping. Interpretation of these variables in terms of the phonological loop component of working memory assumes separate phonological storage and articulatory rehearsal processes. In contrast, the Object-Oriented Episodic Record (O-OER) of Jones and the feature theory of Neath interpret these and other phenomena in terms of a unitary multimodal system. Three experiments investigate these disrupting tasks, with each experiment emphasizing one parameter. In each case, recall of phonologically similar and dissimilar letter sequences is compared as a marker of the presence or absence of phonological coding. In Experiment 1, subjects heard or articulated a single item, or tapped a single key at equal intervals. Only articulatory suppression impaired performance; it also abolished the effects of phonological similarity. Experiment 2 was identical, except that items were heard, or generated in a syncopated rhythm. Both suppression and tapping impaired performance to an equivalent extent and obliterated the effect of phonological similarity. Syncopated irrelevant speech caused a modest but significant impairment in performance. Experiment 3 was identical to Experiment 1, except that six tokens were used. Irrelevant speech and tapping had a clear impact on recall, but neither removed the phonological similarity effect. Again articulatory suppression had a major impact on performance and removed the effect of phonological similarity. We conclude that the pattern of results readily fits the phonological loop hypothesis, provided one accepts Saito's proposal that generating syncopated sequences uses common processes with speech production. It is not clear how the results can be explained by either the O-OER or the feature hypothesis."
6273bce0ebfe541afb3d9ed51a79504f5eee62aa,
5982722c2a536b0d960adbc973fd696365f046f4,
6b6286afa356728d73479a93331673b110916c90,
6ee076290c9b542d9bd913c8faa4080fc4d58ecb,"Ruchkin et al.'s theoretical conclusions reflect two venerable fallacies. They confound an experimental paradigm with a theoretical concept, and they assume that features of the paradigm that are most readily detected by their methods provide an adequate account of the operation of the theoretical system. This results in a simplistic theory that does not do justice to the richness of the available data."
75277f9ff9ca06a4c2262ff03432097307bdc07e,"We welcome the discussion prompted by our data (Larsen & Baddeley, this issue 2003). In the case of Macken and Jones (this issue 2003), we note that much of it concerns inconsistency between their findings and those of ourselves and/or others, emphasizing the need for further replication. We welcome the emphasis that Neath, Farley, and Surprenant (this issue 2003) place on the importance of strategy. This is likely to be an issue of increasing importance in the field, although we have doubts about the correlational approach adopted by Neath et al. Finally, we welcome the demonstration by Page and Norris (this issue 2003) that their primacy model is able to give a computationally explicit account of the irrelevant speech effect within a broad phonological loop framework."
a60504a57aa08d7b05c6ef8a904e112eac2c04ba,
c759237aa84f203548bbf22cf1399a82105fef81,
c9617ec24f9d54b92ea72441bdb88d37d7c2cda5,
cedd9077b3cfa81c630a74122b793784635fc3ae,
dad72fd01329a374a87aa910cd2a21ab638d820d,
f8ad8fd6480f15f26a8d1cc254d5463ac8c9a077,"Two studies are presented that investigated the constraints underlying working memory performance in children and adults. In each case, independent measures of processing efficiency and storage capacity are assessed to determine their relative importance in predicting performance on complex span tasks,which measure working memory capacity. Results show that complex span performance was independently constrained by individual differences in domain-general processing efficiency and domain-specific storage capacity. Residual variance, which may reflect the ability to coordinate storage and processing, also predicted academic achievement. These results challenge the view that complex span taps a limited-capacity resource pool shared between processing and storage operations. Rather, they are consistent with a multiple-component model in which separate resource pools support the processing and storage functions of working memory."
02ccd771d46932046006d1407b177b0eec26f9cb,
587b831bb87c53d0d696e301ae61e18d11921f0c,
0779346ccbb03302a3f3da7ae5f2c4fcb056e388,
1019cf65d58a6584dfd45ef5012f31d0a13adee5,
224c603d506b3a5108bc186ff26aea2d63d40f53,"The current state of A.D. Baddeley and G.J. Hitch’s (1974) multicomponent working memory model is reviewed. The phonological and visuospatial subsystems have been extensively investigated, leading both to challenges over interpretation of individual phenomena and to more detailed attempts to model the processes underlying the subsystems. Analysis of the controlling central executive has proved more challenging, leading to a proposed clarification in which the executive is assumed to be a limited capacity attentional system, aided by a newly postulated fourth system, the episodic buffer. Current interest focuses most strongly on the link between working memory and long-term memory and on the processes allowing the integration of information from the component subsystems. The model has proved valuable in accounting for data from a wide range of participant groups under a rich array of task conditions. Working memory does still appear to be working. The term working memory appears to have been first proposed by Miller, Galanter, and Pribram (1960) in their classic book Plans and the Structure of Behavior. The term has subsequently been used in computational modeling approaches (Newell & Simon, 1972) and in animal learn"
307fdc2d7da43de46b1081cd11673702b9bdedab,"Cowan's revisiting of the magic number is very timely and the case he makes for a more moderate number than seven is persuasive. It is also appropriate to frame his case within a theoretical context, since this will influence what evidence to include and how to interpret it. He presents his model however, as a contrast to the working memory model of Baddeley (1986). I suggest that this reflects a misinterpretation of our model resulting in a danger of focusing attention on pseudo-problems rather than genuine disparities between his approach and my own."
349cd752cc84c9168c33c3f1d284d54223ff49a1,"This paper is divided into three sections. The first reviews the evidence for a verbal short-term memory deficit in Down syndrome. Existing research suggests that short-term memory for verbal information tends to be impaired in Down syndrome, in contrast to short-term memory for visual and spatial material. In addition, problems of hearing or speech do not appear to be a major cause of difficulties on tests of verbal short-term memory. This suggests that Down syndrome is associated with a specific memory problem, which we link to a potential deficit in the functioning of the 'phonological loop' of Baddeley's (1986) model of working memory. The second section considers the implications of a phonological loop problem. Because a reasonable amount is known about the normal functioning of the phonological loop, and of its role in language acquisition in typical development, we can make firm predictions as to the likely nature of the short-term memory problem in Down syndrome, and its consequences for language learning. However, we note that the existing evidence from studies with individuals with Down syndrome does not fit well with these predictions. This leads to the third section of the paper, in which we consider key questions to be addressed in future research. We suggest that there are two questions to be answered, which follow directly from the contradictory results outlined in the previous section. These are 'What is the precise nature of the verbal short-term memory deficit in Down syndrome', and 'What are the consequences of this deficit for learning'. We discuss ways in which these questions might be addressed in future work."
5d48406ed30078f6a0b2e97b3871438e90e75ce3,"Over the last half century, the experimental study of human memory has departed from the earlier concept of a unitary faculty, with the increase in knowledge leading to differentiation between subsystems of memory, often based on the study of neuropsychological patients. Although foreshadowed by the classic work of William James (1890), the current approach to the fractionation of memory probably began with Hebb's (1949) proposal of a distinction between short-term memory (STM), based on temporary electrical activity within the brain, and longterm memory (LTM), based on the development of more permanent neurochemical changes. He even proposed a learning mechanism, a concept that continues to be influential in neurobiological theorizing (see Burgess et al. 2001). Experimental evidence for a distinction between STM and LTM began to appear a decade later with the demonstration by Brown (1958) and Peterson & Peterson (1959) of the rapid forgetting of small amounts of material when ongoing rehearsal was prevented. They proposed that this forgetting reflected the decay of a short-term trace, a process they distinguished from long-term forgetting, which was attributed to interference among longterm memory representations. This view was resisted, with the counter claim made that all forgetting could be interpreted within a single stimulus-response association framework (Melton 1963). The question of whether shortterm forgetting reflects trace decay or interference remains unresolved (Cowan et al. 2000; Service 1998). During the 1960s, however, experimental evidence from a range of sources seemed to point increasingly strongly to the need to distinguish between STM and LTM on grounds other than type of forgetting. Neuropsychological evidence was particularly influential, with patients suffering from the classic amnesic syndrome showing grossly impaired LTM, coupled with total preservation of performance on a range of tasks associated with STM (Baddeley & Warrington 1970). Anatomically, the amnesic syndrome has most strongly been associated with damage to the hippocampus (Milner 1966), although it could result from damage to a series of structures that broadly make up the Papez circuit (see Aggleton & Pearce 2001). The STM-LTM distinction was further supported by patients showing the opposite dissociation, with STM performance impaired and LTM preserved (Shallice & Warrington 1970). By the late 1960s, a range of two-component models was being proposed, of which the most influential was that of Atkinson & Shiffrin (1968). In this model, information was assumed to come in from the environment, be processed by a short-term storage system and then fed into LTM. Probability of learning was assumed to depend on time held within the short-term store. STM was also"
6241214ede0725a99ee704f862aae66c961d98f9,"This study presents normative data for the Speed and Capacity of Language Processing (SCOLP) test from an older American sample. The SCOLP comprises 2 subtests: Spot-the-Word, a lexical decision task, providing an estimate of premorbid intelligence, and Speed of Comprehension, providing a measure of information processing speed. Slowed performance may result from normal aging, brain damage (e.g., head injury), or dementing disorders or may represent the intact performance of someone who always performed at the low end of normal. The SCOLP enables the clinician to differentiate between these possibilities. Adequate age-appropriate norms to differentiate dementia from normal aging do not exist. We present data from 424 older community-dwelling Americans (75-94 years old). The results confirm that information processing speed slows with increasing age. By contrast, increasing age has little effect on lexical decision. Thus, our data suggest that the SCOLP shows promise as a tool to help distinguish between normal aging and the early stages of dementia."
72a727d0fab48acbbb8c7b3e9844b82e845f6bb0,
8828e9ae6eac8c1061e46fbb32f865580c8e037d,
cf36bf9c331d4c497de898a105fc7e596347f6bd,"Attentional control of executive function declines during the early stages of Alzheimer's disease. Controversy exists as to whether this decline results from a single global deficit or whether attentional control can be fractionated, with some aspects being more vulnerable than others. We investigated three proposed domains of attention, namely (i) focal attention, based on simple and choice reaction times; (ii) the capacity to resist distraction in a visual search task; and (iii) the capacity to divide attention between two simultaneous tasks. For each domain, two levels of difficulty were used to study Alzheimer's disease patients, who were compared with elderly and young control subjects. The unitary attentional hypothesis predicted that the impacts of level of difficulty, age and disease would be qualitatively similar across the three attentional domains. In fact we observed different patterns for each domain. We obtained no differential impairment for patients in the focal attentional task, whereas patients were somewhat more susceptible than control subjects to the similarity of the distractor items in visual search. Finally, we observed marked impairment in the capacity of Alzheimer's disease patients to combine performance on two simultaneous tasks, in contrast to preserved dual-task performance in the normal elderly group. These results suggest a need to fractionate executive processes, and reinforce earlier evidence for a specific dual-task processing deficit in Alzheimer's disease."
d47564a8f6101e88c84f9a1717c01bd6aa6b7b18,"A series of 7 experiments used dual-task methodology to investigate the role of working memory in the operation of a simple action-control plan or program involving regular switching between addition and subtraction. Lists requiring switching were slower than blocked lists and showed 2 concurrent task effects. Demanding executive tasks impaired performance on both blocked and switched lists, whereas articulatory suppression impaired principally the switched condition. Implications for models of task switching and working memory and for the Vygotskian concept of verbal control of action are discussed."
e5a605476d68655d077587a5b97b8b27aa9a5906,
8d340c0c5dc84acddcb188e5e2e047f17e4038bd,
9a9ff45a9d1be8b5f1f4401f10baaedd3b0853d2,"Previous research has indicated that 2 processing rates may constrain verbal short-term memory performance. These have been linked to individual differences in (a) the time taken to articulate spoken words and (b) the duration of pauses that occur between words in the output responses to memory tasks. Two experiments examined whether evidence for these effects on memory can be obtained for measures taken from a single speech sample. Children articulated pairs of words as rapidly as possible. In both experiments, the spoken duration of words and the length of the pauses between them predicted significant variance in verbal short-term memory performance. It is argued that the duration of words is linked to memory performance through the processes underlying time-based forgetting in short-term memory. In contrast, the duration of pauses in speeded articulation may index individual differences in speech planning processes."
9c983c4c99451e34fa45b65a3363ca278bb56109,"Measuring recovery of function may mean testing the same individual many times, a procedure that is inevitably open to improvement due to learning on the specific tests rather than recovery per se. This is particularly likely to be an issue with measures of memory performance. We therefore studied the performance of normal and brain-injured people across 20 successive test sessions on measures of orientation, simple reaction time, forward and backward digit span, visual and verbal recognition, word list learning and forgetting, and on three semantic memory measures, namely, letter and category fluency and speed of semantic processing. Differences in overall performances between the two groups occurred for all tests other than orientation, digit span forward, and simple reaction time, although the tests differed in their degree of sensitivity. The tests varied in the presence or absence of practice effects and in the extent to which these differed between the two groups. Data are presented that should allow investigators to select measures that are likely to optimize sensitivity while minimizing possible confounding due to practice effects. (JINS, 2000, 6, 469–479.)"
b0ac08fdf62625beed0dd280e3c21331176ffa93,
d24d422d478d29424155dd82f00b8df4e4ba9c04,
e4b4798bf9493af4ab2a482a5124fded22c37d07,
f27e7a8d45da76c61e63519ae8404f2688926900,"Individuals with Down syndrome suffer from relatively poor verbal short-term memory. Previous explanations of this deficit have been framed in terms of inefficient or absent rehearsal of verbal material in Down syndrome within the phonological loop component of Baddeley and Hitch's (1974) working memory model. Two experiments are presented which test this explanation by looking for the markers of rehearsal in children with Down syndrome and verbal mental age matched controls. Both experiments confirm that individuals with Down syndrome show poorer verbal short-term memory performance than controls. However, they rule out rehearsal as an explanation of these deficits because the evidence suggests that neither individuals with Down syndrome nor matched controls are engaging in spontaneous subvocal rehearsal. Other explanations of poor verbal short-term memory performance in Down syndrome, in terms of impairments both within and outside of the phonological loop system, are discussed. Practical implications for intervention strategies aimed at improving verbal short-term memory skills in Down syndrome are also outlined."
f42b3465df12e3bfa2a02b4bd639642445d0d207,"The working memory framework was used to investigate the factors determining the phenomenological vividness of images. Participants rated the vividness of visual or auditory images under control conditions or while performing tasks that differentially disrupted the visuospatial sketchpad and phonological loop subsystems of working memory. In Experiments 1, 2, and 6, participants imaged recently presented novel visual patterns and sequences of tones; ratings of vividness showed the predicted interaction between stimulus modality and concurrent task. The images in experiments 3, 4, 5, and 6 were based on long-term memory (LTM). They also showed an image modality by task interaction, with a clear effect of LTM variables (meaningfulness, activity, bizarreness, and stimulus familiarity), implicating both working memory and LTM in the experience of vividness."
f6919e2674c7bc0009d1239865d06ec9b5cb5441,"The role of visual working memory in temporary serial retention of verbal information was examined in four experiments on immediate serial recall of words that varied in visual similarity and letters that varied in the visual consistency between upper and lower case. Experiments 1 and 2 involved words that were either visually similar (e.g. fly, cry, dry; hew, new, few) or were visually distinct (e.g. guy, sigh, lie; who, blue, ewe). Experiments 3 and 4 involved serial recall of both letter and case from sequences of letters chosen such that the upper- and lower-case versions were visually similar, for example Kk, Cc, Zz, Ww, or were visually dissimilar, for example Dd, Hh, Rr, Qq. Hence in the latter set, case information was encoded in terms of both the shape and the size of the letters. With both words and letters, the visually similar items resulted in poorer recall both with and without concurrent articulatory suppression. This visual similarity effect was robust and was replicated across the four experiments. The effect was not restricted to any particular serial position and was particularly salient in the recall of letter case. These data suggest the presence of a visual code for retention of visually presented verbal sequences in addition to a phonological code, and they are consistent with the use of a visual temporary memory, or visual “cache”, in verbal serial recall tasks."
08ba213f2ef6b500f04059989182498691a788ee,"Individuals with Down syndrome are thought to perform poorly on tests of verbal short-term memory, such as measures of word span or digit span. This review critically examines the evidence for a specific deficit in verbal short-term memory in Down syndrome, and outlines a range of possible explanations for such a deficit. The potential implications of a verbal short-term memory impairment for broader aspects of development are outlined, in particular with respect to vocabulary development. Possible intervention strategies, which might improve verbal short-term memory performance in Down syndrome are also considered. However, we argue that further research is needed to fully clarify the nature of a verbal short-term memory deficit in Down syndrome, before the merits of these various intervention approaches can be properly evaluated."
1a9a9618959fa4d5b7a38649ade4a4d86a3e6df2,
21ca76cdb828ce8e4059c5da88045151da19cb95,"Three groups of participants were assessed. Each participant was tested on 20 occasions. The groups comprised people (i) in post traumatic amnesia (PTA) following severe head injury (n=9), (it) with severe head injury but not in PTA (n=10), and (iii) with no history of head injury or other neurological condition (n=13). Subjects were given several tests of memory, attention and learning in order to determine which tests were good at (a) distinguishing people in PTA from those not in PTA, and (b) monitoring recovery over time. The results indicate that people in PTA have a wide range of deficits and their cognitive recovery is a gradual process rather than an all-or-none phenomenon. In terms of measurement, the study suggests that a good test of PTA should include orientation questions, together with a reaction time measure, a visual recognition test and a speed of information processing measure. Most of the tests administered were good at distinguishing between brain-injured and nonbrain-injured people, although only two tests distinguished between the two brain-injured groups, i.e. those in PTA and those out of PTA. Almost all tests were good at monitoring recovery from PTA."
28658afb71456002bfd162f5237e26e2e4ddf13a,
336851176ba248d95d547399401e09d000c01020,
4242fa1e5cccc98ba08933cadffe36c38a32b1b0,"Abstract We report the case of LE, a sculptress with an autoimmune disorder, systemic lupus erythematosus, that caused her to have an impaired visual short-term memory together with Image generatlon problems. This dramatically affected her sculpting style. LE was a puzzle both clinically and sclentlflcally. Cllnlcally, her problems were nearly missed because she scored within the normal range on tests of vlsuo-spatial immediate memory. However, on a test of immediate visual pattern memory, i.e. with much less of a spatial component, LE's performance was severely compromised, Illustrating a dissociation between spatial span and pattern span. Scientifically, she was a puzzle because she was able to compensate well for her difficulties and use alternative routes to solve her difficulties, e.g. kinaesthetic solutions were employed to answer questions depending on visual imagery. This clouded the picture with regard to her performance on visual imagery tasks. We discuss the results in terms of Baddeley and Hit..."
9fe3293c7cf42c73fb453869f4a574887cf62eab,"Caplan, Rochon, and Waters (1992) report a failure to observe the poorer immediate serial recall for words of longer spoken duration obtained by Baddeley, Thomson, and Buchanan (1975) and subsequently replicated by others. Indeed, they find a significant reversal of this effect. We present evidence that the material used by Caplan et al. differs only minimally in spoken duration under speeded articulation conditions (Exp. 1 = 1.9%, Exp 2 = 2.31%), in contrast to a clear difference in the case of the original Baddeley et al. material (24.5%). It is further suggested that the reversal of the word-length effect may result from differences in acoustic similarity between the “long” and “short” word sets used by Caplan et al. We conclude that the evidence continues to indicate that longer spoken duration is associated with reduced memory span."
a4600e07335bf19195f04e571ea7c53ac413a828,
c405a27e626a4ad255e9018e619c7f0ba1c83b3d,"The authors summarize developments in the concept of working memory as a multicomponent system, beginning by contrasting this approach with alternative uses of the term working memory. According to a 3-component model, working memory comprises a phonological loop for manipulating and storing speech-based information and a visuospatial sketchpad that performs a similar function for visual and spatial information. Both are supervised by a central executive, which functions as an attentional control system. A simple trace-decay model of the phonological loop provides a coherent account of the effects of word length, phonemic similarity, irrelevant speech, and articulatory suppression in verbal short-term memory tasks. This model of the loop has also proved useful in the analysis of neuropsychological, developmental and, cross-cultural data. The notion of the sketchpad is supported by selective interference with imagery in normal adults and by specific neuropsychological impairment. Analysis of the central executive is illustrated by work on deficits in the ability to coordinate subproccesses in Alzheimer's disease (AD). (PsycINFO Database Record (c) 2008 APA, all rights reserved)"
d3c38e3582189d88f62d871062e1506e8c025998,"The Rivermead Behavioural Memory Test provides a well-validated instrument for detecting everyday memory problems in patient groups. It was however designed as a screening test, and thus is insufficiently sensitive to detect mild deficits, whether due to brain damage or to the introduction of a drug or stressor. The Extended Rivermead Behavioural Memory Test (ERBMT) increases the level of difficulty by doubling the amount of material to be remembered, by combining material from Forms A and B, and Forms C and D of the original test to produce two parallel versions of the new extended test. The sensitivity of the ERBMT was assessed by comparing the performance of a middle-aged and an elderly group of normal subjects, who would be expected to show modest differences in memory performance. The subtests varied in their sensitivity to this small age difference, but when performance was assessed in terms of scaled scores that allow an overall combined measure of memory performance to be calculated, the test proved sensitive (t = 4.87, P < 0.0001), and free of ceiling and floor effects. We suggest that the ERBMT provides a promising measure of everyday memory in normal adults."
536ddf0f3f6c39a853bc3ccff862b139f428da29,
6d5ac854be2596813a3d20fb2cc1d4838aa3e8d0,Introduction. What is Memory? Short-term Memory. Working Memory. Learning. Organising and Remembering. Forgetting. Repression. Storing Knowledge. Retrieval. Eyewitness Testimony. Amnesia. Memory in Childhood. Memory and Ageing. Improving Your Memory. What's Next in the Study of Memory?
b1f88abf1afff77bf8bde8f8737d29ae56627cc8,"FIVE CENTRAL FEATURES OF THE MODEL (1) According to our view, working memory comprises multiple specialized components of cognition that allow humans to comprehend and mentally represent their immediate environment, to retain information about their immediate past experience, to support the acquisition of new knowledge, to solve problems, and to formulate, relate, and act on current goals. (2) These specialized components include both a supervisory system (the central executive) and specialized temporary memory systems, including a phonologically based store (the phonological loop) and a visuospatial store (the visuospatial sketchpad). (3) The two specialized, temporary memory systems are used to actively maintain memory traces that overlap with those involved in perception via rehearsal mechanisms involved in speech production for the phonological loop and, possibly, preparations for action or image generation for the visuospatial sketchpad. (4) The central executive is involved in the control and regulation of the working memory system. It is considered to play various executive functions, such as coordinating the two slave systems, focusing and switching attention, and activating representations within longterm memory, but it is not involved in temporary storage. The central executive in principle may not be a unitary construct, and this issue is a main focus of current research within this framework. (5) This model is derived empirically from studies of healthy adults and children and of brain-damaged individuals, using a range of experimental methodologies. The model offers a useful framework to account for a wide range of empirical findings on working memory."
b7b7129661bdc0f0bd323a58363205f288715af9,"Both single unit recording and neuroradiological studies suggest that frontal and executive processes are necessary for visual maintenance rehearsal. This observation is linked to the classic vigilance literature by the proposal that vigilance decrement is found when the subject is required to maintain a representation over a brief delay. Vigilance performance was therefore studied in a sample of elderly subjects who were tested over a 40-min period involving perceptual or memory-based tasks which were matched for initial level of performance. There was a significant interaction between task and delay, with only the memory-based task showing decrement. A second study used the same two tasks to investigate vigilance performance in patients suffering from probable Alzheimer's Disease. Over a 15-min delay period, an equivalent interaction effect occurred, again indicating substantially greater decrement for the memory-based task. The results are interpreted as consistent with a role for the executive processes of working memory in both visual rehearsal and vigilance performance."
0096ec7a55eff33bd47d70f5890a2ce6a5ac3ccd,
173620e74efd23876f43c31848ab7d14d54ec941,"A relatively simple model of the phonological loop (A. D. Baddeley, 1986), a component of working memory, has proved capable of accommodating a great deal of experimental evidence from normal adult participants, children, and neuropsychological patients. Until recently, however, the role of this subsystem in everyday cognitive activities was unclear. In this article the authors review studies of word learning by normal adults and children, neuropsychological patients, and special developmental populations, which provide evidence that the phonological loop plays a crucial role in learning the novel phonological forms of new words. The authors propose that the primary purpose for which the phonological loop evolved is to store unfamiliar sound patterns while more permanent memory records are being constructed. Its use in retaining sequences of familiar words is, it is argued, secondary."
3c5ea4081a243fef19407b4157aa9293a011871a,"We describe the memory functioning of C, a professional musician who became amnesic following herpes simplex encephalitis in 1985. Although transient amnesia in a professional musician has previously been described, this is the first reported case of chronic amnesia in a highly talented professional musician. C is unusual in three respects. First, his amnesia is particularly severe. Second, his amnesia includes semantic as well as episodic memory deficits. Third, he believes he has just woken up and his preoccupation with this state of 'just wakening' has persisted for over 9 years. This appears to be the result of a delusion rather than the consequence of his amnesia."
8d4c4db44bc78d7d119cea99322ac41fc96d6cad,
ac482277ace1690d680f077a0f2c4e6233838efb,"One commonly cited feature of Williams syndrome is a characteristic dissociation between relatively spared language skills and severely impaired nonverbal abilities. However, the actual evidence for a dissociation between verbal and nonverbal abilities in Williams syndrome is equivocal. In two separate studies we examined these abilities in 16 individuals showing the Williams syndrome phenotype. When considered as a whole, the group did have significantly superior verbal abilities, but this difference was caused by a large discrepancy in abilities in only a small number of individuals. In both studies there was a clear, linear relation between individuals' verbal ability, and the magnitude of their verbal-nonverbal discrepancy. We suggest that these results are best explained in terms of verbal ability developing at a faster rate than nonverbal ability in this disorder. We discuss how this model of differential rates of development has the potential to reconcile the apparently inconsistent findings in this area."
d255e967b09bc5b66c1a7812065be12840f94d18,
e1857673be62062bbb87efb64a51284be2b5bfeb,"Parkin's criticisms of the central executive are based on a series of misconceptions. The central executive is not an organ that might or might not exist, but a scientific concept. Part of its function is to separate the analysis of executive processes from the question of their anatomical location. Like other components of working memory, it is fractionable into subsystems. How the subsystems interrelate and how they map onto the anatomical substrate are empirical questions under active current investigation. (JINS, 1998, 4, 523–526.)"
e833b2ec101efd6876be873c27254c3f942c3e44,"ABSTRACT This study tested the hypothesis that individual differences in immediate verbal memory span would predict success in second language vocabulary acquisition. The subjects learned 56 English–Finnish translations during two sessions using a method in which they were encouraged to distribute their learning and to use semantic encoding strategies where appropriate. Verbal, but not visuo-spatial, memory span was correlated with the rate of vocabulary learning, a result that could not have occurred because of immediate retrieval from a short-term buffer. When tested one week later, the subjects were less likely to remember those words they had had difficulty learning, even though they had studied these items more often. The theoretical and practical implications of the findings for vocabulary learning are discussed."
f2c714252bbbcf1010d8490359152f7719862e59,"A series of experiments explores the capacity for generating sequences of random responses, relating it to the central executive component of working memory. Experiment 1 shows a broadly similar pattern of redundancy increasing with speed of generation for both the verbal generation of digits and the manual pressing of keys. In both cases deviations from randomness are shown to reflect the increasing use of a limited number of stereotyped response sets. The remaining experiments use keyboard generation. Experiment 2 demonstrates that concurrent immediate serial recall decreases randomness, and that longer recall sequences produce less random output. Experiments 3 and 4 show that whereas simple counting has no effect on randomness, serial recall, semantic category generation, and concurrent digit generation have substantial effects, and a concurrent fluid intelligence test has the greatest influence on the randomness of key pressing. It is suggested that the task of random generation resembles that of category fluency because it requires the subject to switch retrieval plans and inhibit repetition. On this basis it is predicted that a task involving repeated switching of categories will interfere with generation, despite being predictable and having a low memory load. Experiments 5 and 6 confirm this prediction. Strengths and limitations of the switching hypothesis are discussed, as are the implications of our results for the analysis of executive processes."
2cdd1855d48543332df3b842f5129c89489e1879,
46ce6fd2eaef3560c91e5ce5750d95a24258d7e5,
51e9a6caed851f8feffc5e4406395787307c0e98,"Patients with defined frontal lobe lesions were assigned to 1 of 2 groups based on whether they showed a behaviorally assessed dysexecutive syndrome or were behaviorally normal. All participants were tested on dual-task performance and on 2 tasks assumed to measure frontal lobe function, the Wisconsin Card Sorting Test and verbal fluency. The dysexecutive group differed significantly from the nondysexecutive in showing impaired capacity for dual-task coordination, but there were no significant differences on the Wisconsin Card Sorting Test and verbal fluency. Results are interpreted in terms of a multicomponent central executive, whose function is linked to, but not coterminous with, the operation of the frontal lobes."
65eeabc0dce284419200140162a49dd027d16743,
83c861e725e169ee245427669e7cef7f077927cb,
9a20a7766f93feb60109b51d8956cbf45791caa2,"It has been claimed that the symptoms of post-traumatic stress disorder (PTSD) can be ameliorated by eye-movement desensitization-reprocessing therapy (EMD-R), a procedure that involves the individual making saccadic eye-movements while imagining the traumatic event. We hypothesized that these eye-movements reduce the vividness of distressing images by disrupting the function of the visuospatial sketchpad (VSSP) of working memory, and that by doing so they reduce the intensity of the emotion associated with the image. This hypothesis was tested by asking non-PTSD participants to form images of neutral and negative pictures under dual task conditions. Their images were less vivid with concurrent eye-movements and with a concurrent spatial tapping task that did not involve eye-movements. In the first three experiments, these secondary tasks did not consistently affect participants' emotional responses to the images. However, Expt 4 used personal recollections as stimuli for the imagery task, and demonstrated a significant reduction in emotional response under the same dual task conditions. These results suggest that, if EMD-R works, it does so by reducing the vividness and emotiveness of traumatic images via the VSSP of working memory. Other visuospatial tasks may also be of therapeutic value."
9cf28efa5741591bb5de2f0692b3e2f3de6bfcfc,
9eadd5ebb76a6870f894c1b2127df8324f41c92e,"Short-term memory for verbal and visuospatial information was examined in a group of children and teenagers with Down's syndrome. Performance on the verbal task was impaired relative to matched control groups, but there were no group differences on the visuospatial task. Relatedly, the Down's syndrome group showed inferior short-term memory for verbal as opposed to visuospatial information, whereas controls showed the opposite pattern. These findings did not appear to result from a general superiority of nonverbal abilities in the Down's syndrome group, or from hearing difficulties that might have impacted on the verbal short-term memory task, in which material was presented auditorily. The results are consistent with the suggestion that Down's syndrome is associated with a selective impairment of the phonological loop component of Baddeley and Hitch's (1974) working memory model."
9f19de01c65bad4c3764aaf829124e7a4a0f91cd,"A brief account of the concept of working memory is presented, followed by a more detailed description of one sub-component of the system, namely the phonological loop. The question of the functional significance of this component of working memory is discussed. Evidence suggests a minor role in language comprehension, together with a much more substantial role in the capacity to acquire novel phonological, and possibly grammatical forms. It is suggested that the phonological loop has evolved as a mechanism for language acquisition."
d64911e66a802ed913ff3dadea6789b50ec91214,
96880b74a9bc3d28cd0d92ac68d4eacecfc043e9,"A major problem in analysing the executive processes that seem to depend upon the prefrontal cortex stems from the absence of a well developed cognitive model of such processes. It is suggested that the central executive component of an earlier model of working memory might provide a suitable framework for such an analysis. The approach is illustrated using one proposed component of executive control, namely the capacity to combine two concurrent tasks. The application of the approach to patients suffering from Alzheimer's disease, and patients with acquired brain damage is discussed. Finally, a study is described in which the dual task performance of patients with known frontal lesions is shown to be associated with observed behavioural problems. The paper concludes with the discussion of the prospects for extending the approach to include a range of other executive processes, and to the way in which such an analysis may subsequently lead to a more integrated model of the central executive, and a better understanding of its relationship to the prefrontal cortex."
ab095d14ffd106003ad3934456ac69a3433f546a,
b18fbb32c20d4e6a0b5403612cebe90b66d42698,
c769ad65b89c06b984f1b47a54b7c81cb4a6cae5,
cc031b2edacd3a16b5eccafa815def7e732aa757,"Part 1 Perception - selection and attention: the perception of features and objects, Anne Treisman on the output of a visual fixation, Andries Sanders selection of input and goal in the control of behaviour, John Duncan filtering and physiology in visual search - a convergence of behavioural and neurophysical measures, Peter McLeod and Jon Driver objects, streams and threads of auditory attention, Dylan Jones. Part 2 Attentional control of complex tasks: designing for attention, Neville Moray motor programs and musical performance, L. Henry Shaffer working memory or working attention? Alan Baddeley supervisory control of action and thought selection, Tim Shallice and Paul Burgess crystal quest - a search for the basis of maintenance of practised skills into old age, Patrick Rabbitt. Part 3 Conscious awareness: search for the unseen, Larry Weiskrantz implicit learning - reflections and prospects, Dianne Berry redefining automaticity - unconscious influences, awareness and control, Larry Jacoby et al varieties of consciousness and levels of awareness in memory, Endel Tulving. Part 4 Attention, arousal and stress: viral illness and performance, Andrew Smith cognitive-energetical control of mechanisms in the management of work demands and psychological health, G. Robert J. Hockey individual differences in personality and motivation - ""non-cognitive"" determinants of cognitive performace, William Revelle selective effects of emotion on information-processing, John D. Teasdale interaction of arousal and selection in the posterior attention network, Michael I. Posner self-report questionnaires in cognitive psychology - have they delivered the goods?, James Reason."
d38febca79044642f89d4fbf726bb152c08fd593,"Cognitive neuropsychiatry occupies the comparatively neglected research region that lies between neurology, psychiatry, and cognitive psychology. Reasons for this neglect are discussed, together with arguments as to why it may be timely to focus on this intellectual no man's land."
e630ae04cc6580e8fecf0cd9e1d001757221b0f8,
f21efe3433615fcdb464efec676338977314fa90,"In performing many complex tasks, it is necessary to hold information in temporary storage to complete the task. The system used for this is referred to as working memory. Evidence for the need to postulate separable memory systems is summarized, and one particular model of working memory is described, together with its fractionation into three principal subsystems. The model has proved durable and useful and, with the development of electrophysiological and positive emission tomography scanning measures, is proving to map readily onto recent neuroanatomical developments."
f912908407c68d1f1d287e65377abb7c34732e48,"In recent years, the study of normal cognitive function has gained substantially from the study of patients with cognitive deficits. Research on autobiographical memory is no exception, and research on retrograde amnesia and on confabulation in patients suffering from brain damage featured prominently in an earlier volume on this topic (Rubin, 1986). However, while the contribution of neuropsychology to the understanding of normal function is well established, neuropsychiatry has so far been less influential. One of the reasons is that psychiatry itself has been far from unanimous in its approach to the most appropriate conceptualization of its subject matter. There has, for example, been prolonged controversy as to whether schizophrenia should be regarded as a physical disorder resulting from some form of dysfunction of the brain, or as others would claim, a psychological response to psychosocial stress. While there is still little agreement as to what, if any, physical changes in the brain are associated with schizophrenia (see Waddington, 1993, for an overview), the application of neuropsychological techniques to the study of schizophrenic patients makes it clear that cognitive deficits are characteristic, and in particular tend to produce an impairment in memory (see McKenna, Clare, & Baddeley, 1995, for a review), and in executive functions (see Frith, 1992). These deficits do not appear to be a result of drug treatment, nor can they readily be explained in terms of problems of motivation, or disruption by positive symptoms such as hallucinations and delusions."
fbdab392a1253fe0d87cfba3294ba47ce3ca219b,"Many models of serial recall assume a chaining mechanism whereby each item associatively evokes the next in sequence. Chaining predicts that, when sequences comprise alternating confusable and non-confusable items, confusable items should increase the probability of errors in recall of following non-confusable items. Two experiments using visual presentation and one using vocalized presentation test this prediction and demonstrate that: (1) more errors occur in recall of confusable than alternated non-confusable items, revealing a “sawtooth” in serial position curves; (2) the presence of confusable items often has no influence on recall of the non-confusable items; and (3) the confusability of items does not affect the type of errors that follow them. These results are inconsistent with the chaining hypothesis. Further analysis of errors shows that most transpositions occur over short distances (the locality constraint), confusable items tend to interchange (the similarity constraint), and repeated responses are rare and far apart (the repetition constraint). The complete pattern of errors presents problems for most current models of serial recall, whether or not they employ chaining. An alternative model is described that is consistent with these constraints and that simulates the detailed pattern of errors observed."
fe48087f29b88fa93db11b31af6776a87bf98d7e,"The central executive component of working memory is a poorly specified and very powerful system that could be criticized as little more than a homunculus. A research strategy is outlined that attempts to specify and analyse its component functions and is illustrated with four lines of research. The first concerns the study of the capacity to coordinate performance on two separate tasks. A second involves the capacity to switch retrieval strategies as reflected in random generation. The capacity to attend selectively to one stimulus and inhibit the disrupting effect of others comprises the third line of research, and the fourth involves the capacity to hold and manipulate information in long-term memory, as reflected in measures of working memory span. It is suggested that this multifaceted approach is a fruitful one that leaves open the question of whether it will ultimately prove more appropriate to regard the executive as a unified system with multiple functions, or simply as an agglomeration of independent though interacting control processes. In the meantime, it seems useful to continue to use the concept of a central executive as a reminder of the crucially important control functions of working memory."
142d53931878eb8975051da4876b6a9d1ed72485,
2f1187152f5364a27810a110907b217e4cd90d04,About the Editors. List of Contributors. Preface. Preface to the First Edition. SECTION I: THEORETICAL BACKGROUND. The Psychology of Memory (A. Baddeley). Neurobiological Foundations of Human Memory (D. Tranel and A. Damasio). Functional Neuroimaging of Memory (M. Rugg). The Medial Temporal Lobe and Memory for Facts and Events (J. Manns and L. Squire). Connectionist Models for Memory Disorders (J. Murre). Psychopharmacology of Human Memory (H. Curran and H. Weingartner). SECTION II: VARIETIES OF MEMORY DISORDER. The Amnesic Syndrome: Overview and Subtypes (M. O'Connor and M. Verfaillie). Theories of Anterograde Amnesia (A. Mayes). Retrograde Amnesia (M. Kopelman). Transient Global Amnesia (G. Goldenberg). Recovery of Memory Function in Neurological Disease (N. Kapur and K. Graham). Neuropsychological Impairments of Verbal Short-term Memory (G. Vallar and C. Papagano). Neuropsychological Impairments of Visual and Spatial Working Memory (S. Della Sala and R. Logie). Disorders of Semantic Memory (J. Snowden). The Cognitive Neuroscience of Confabulation: A Review and a Model (A. Gilboa and M. Moscovitch). Frontal Lobes and Memory (J. Baldo and A. Shimamura). Posttraumatic Amnesia and Residual Memory Deficit after Closed Head Injury (H. Levin and G. Hanten). Schizophrenia (P. McKenna et al.). Memory and Emotional Disorder (T. Dalgleish and S. Cox). Psychogenic Amnesia (M. Kopelman). SECTION III: DEVELOPMENT AND MEMORY. Memory Development During the Childhood Years (S. Gathercole). Children with Intellectual Disabilities (S. Vicari and G. Carlesimo). Developmental Amnesias and Accquired Amnesias of Childhood (C. Temple). Memory in Elderly People (J. Kester et al.). The Memory Deficit in Alzheimer's Disease (J. Becker and A. Overman) Memory Disorders in Subcortical Dementia (J. Brandt and C. Munro). SECTION IV: ASSESSMENT AND MANAGEMENT OF MEMORY PROBLEMS. Assessment of Memory Disorders (B. Wilson). Separating Memory from Other Cognitive Disorders (D. Howieson and M. Lezak). Management and Remediation of Memory Problems in Brain-injured Adults (B. Wilson). Assessment and Management of Memory Problems in Children (J. Middleton). Assessment and Intervention in Dementia of Alzheimer Type (L. Clare). Reducing the Impact of Cognitive Impairment in Dementia (B. Woods). External Memory Aids and Computers in Memory Rehabilitation (N. Kapur et al.). Emotional and Social Consequences of Memory Disorders (R. Tate). Author Index. Subject Index.
4681df03dbdb768c92ad86d51f88cf72495bd3ff,
47a715f3467a64e4c06a979d982831423f503235,
4eb6a77964b8522f992ca0cb0b899c6c56433882,
a1227257402a241ef6e8c5a2608af5b5db8f74c2,
cfe6fce034dc09184a1ee358f9d5e119cd3acbf3,
d02bd44316e8c02a6b4dd20aefbd57d1aa7e845a,
88e0d13f8a5b6e43adbc8101d5f421e4ea8b1157,
d5a4e46e38c8ca624c0c229aa75f08159bd42ee1,
dec5a97e27baacaabe15788b984deaaf19a2f62e,"Abstract Individuals infected with Human Immunodeficiency Virus (HIV) and having cognitive impairment have been described as having slow mentation. Data supporting this proposition come from a variety of sources, including Sternberg's (1966) item recognition memory task. The procedure nominally provides an index of speed of mental operations, independent from input/output demands. However, since the original use of this procedure in the 1960s, advances in cognitive psychology have revealed many of its limitations. The purpose of the present study was to examine the psychometric characteristics of this task. Each participant performed the Sternberg item recognition task twice, 6 mo apart. The stability of the estimate of the slope of regression equations and for zero intercept ranged from excellent (r = .87) to poor (r = .30), and the data from many individual subjects could not be reliably modelled using multiple linear regression techniques. These data, as well as those from previous research, demonstrate the limited practical use of this task in clinical samples. Furthermore, as cognitive psychological theory has advanced in the past 30 yr, the conceptual underpinnings of the procedure have essentially evaporated. (JINS, 1995, 1, 3–9)."
e25eb0ca5dcd4a0384d64013fe02f776068b538d,"The problems of adapting measures of cognitive performance to Third World conditions are described, and three novel adaptations are proposed, one based on speed of sentence comprehension, one on vocabulary acquisition, and a third on speed of visual search using pictorial material. These and other existing tests are applied to studying the cognitive performance of Jamaican children as part of an investigation into the effects on cognition of infection by the parasitic worm Trichuris trichiura. We demonstrate that the tests are usable under Third World field conditions, and give reliable results. The validity of our proposed tests is indicated by their capacity to predict scholastic performance. Despite their brevity and avoidance of any demand on literacy, they yielded substantial correlations with the reading, spelling and arithmetic scales of the Wide Range Achievement Test."
e4c7c5fe08cdc1d4726a2e65d11b94d7cce0bfa0,
f26dac875a1b15eb7cec76a6de421889fa1b61b4,
f6d23039d0257dbd047075d59d5afe204267a757,"We describe the memory functioning of C, a professional musician who became amnesic following herpes simplex encephalitis in 1985. Although transient amnesia in a professional musician has previously been described, this is the first reported case of chronic amnesia in a highly talented professional musician. C is unusual in three respects. First, his amnesia is particularly severe. Second, his amnesia includes semantic as well as episodic memory deficits. Third, he believes he has just woken up and his preoccupation with this state of 'just wakening' has persisted for over 9 years. This appears to be the result of a delusion rather than the consequence of his amnesia."
fcb1e291377426b5128723859436a3382052599b,
0a649ea97445cd5b74d1a396ad0498e985870df3,"Abstract This paper explores the changes in cognitive function which occur as someone ""loses consciousness"" under anesthesia. Seven volunteers attempted a categorization task and a within-list recognition test while inhaling air, 0.2% isoflurane, and 0.4% isoflurane. In general, performance on these tests declined as the dose of anesthetic was increased and returned to baseline after 10 min of breathing air. A measure of auditory evoked responding termed ""coherent frequency"" showed parallel changes. At 0.2% isoflurane, subjects could still identify and respond to category exemplars but showed impaired short-term memory function. Electrical stimulation at 0.4% isoflurane, intended to mimic the arousing effects of surgery, had a small, beneficial effect on performance. A mean of 63% of category exemplars was identified at this stage, but recognition memory for those exemplars was at chance on recovery. There was no evidence for learning of words presented at 0.8% isoflurane."
3ddd9295f734c5653dcd818704075cd70c35031e,"Abstract We report six experiments comparing errorful and errorless learning in the teaching of new information to neurologically impaired adults with severe memory problems. The first experiment is a group study in which amnesic subjects, young controls, and older controls were required to learn two lists of words under two conditions. One of these required subjects to generate guesses that produced incorrect responses, and the other prevented guessing—permitting only correct responses. Conditions and lists were counterbalanced across subjects. People with amnesia scored significantly higher under the errorless condition. We further explored the principle of errorless learning in five single case studies in which severely memory impaired people were required to learn information analogous to that needed in everyday life. Tasks included learning names of objects and people, learning how to programme an electronic aid, remembering orientation items, and learning new items of general knowledge. In each case..."
7b659ea4b3802f85a91640a42127c459dbc942c7,"This article presents findings from the Children's Test of Nonword Repetition (CNRep). Normative data based on its administration to over 600 children aged between four and nine years are reported. Close developmental links are established between CNRep scores and vocabulary, reading, and comprehensive skills in children during the early school years. The links between nonword repetition and language skills are shown to be consistently higher and more specific than those obtained between language skills and another simple verbal task with a significant phonological memory component, auditory digit span. The psychological mechanisms underpinning these distinctive developmental relationships between nonword repetition and language development are considered."
d51e3b50183ce0004f368ae600888df0156cdcef,
ef465c112df43485e2d370431d744cfeecc61f73,
053b8120f5dfa2a27d4ebd2419f16e648d069e7e,
0b7ffdeead4cffb116cde43c78100c969838d34b,"Five experiments tested the prediction, from a simple chaining model, that interleaving irrelevant material will substantially disrupt immediate serial recall. Experiment 1 interpolated long or short words between items in an auditory digit span test. These two ""sandwich"" conditions disrupted recall to an equal but moderate extent. Experiment 2 presented mixed lists of digits and words, cuing one or the other before or after presentation. Precuing led to substantially better recall. Experiment 3 used articulatory suppression to rule out the hypothesis that recall was protected from the sandwich effect by subvocal rehearsal. Experiment 4 combined the sandwich effect with a concurrent task, finding clear effects of both but no interaction. Experiment 5 showed that the predictability of interpolated material did not influence recall. These results can be explained by adding an attentional preprocessor to standard chaining models. Immediate memory span, like speech itself, requires that order information be preserved. This has presented a major problem both to models of immediate serial recall and to models of language behavior. Although the range of models of serial order is wide, they tend to fall into two broad categories, one assuming that order is imposed by means of an external structure and one assuming a form of chaining whereby associative links are formed between successive items (see Lewandowsky & Murdock, 1989, for a more detailed discussion of existing models)."
2c97b40a528777d84ce235bb0e44e92d693b3f45,"Abstract When not engaged in demanding tasks, we commonly experience streams of thoughts and images quite unrelated to immediate sensory input. Such stimulus-independent (SI) thoughts may be troublesome, as in worry, insomnia and depression. Previous research within a working memory paradigm suggested that SI thought production depended on central executive control resources. To explore this hypothesis further, we examined the interference with SI thought production resulting from shadowing auditorily presented digits compared to remembering them. Effects of stimulus presentation rate and size of memory load were also examined. At slow presentation rates, remembering produced more interference than shadowing. For shadowing, faster presentation produced greater interference than slow presentation. In remembering, interference was not substantially affected by size of memory load, was greater when subjects reported greater awareness of task stimuli, and was restricted to thoughts forming parts of connected ..."
3aeafbf1b39b9069b010a67fa35fa7284eab40f6,"QU, an eight-year-old boy, was identified from a large scale normative study on the basis of his greatly reduced digit span, combined with normal long-term memory and non-verbal intelligence. Further investigation indicated that his visual STM was normal, but that he was clearly impaired on two verbal STM tests, nonword repetition, and memory span for words. His span showed clear effects of phonological similarity and word-length, suggesting qualitatively normal functioning of the phonological loop component of working memory, despite a quantitative impairment in level of performance. This pattern resembles that found in an earlier study of children with a specific language disorder. We tested QU on measures of vocabulary, syntax, and reading, and found him to be substantially below the age norms on all three. The implications of these findings are discussed for the role of the phonological loop in language development."
3e5a384e79840a0cca5385c7a1955e1b5b715f44,"The development of a test aimed at estimating premorbid intelligence is described. The test, Spot-the-Word, involves presenting the subject with pairs of items comprising one word and one non-word, and requiring the subject to identify the word. Data show that performance correlates highly with verbal intelligence as estimated by Mill Hill Vocabulary score and by performance on the National Adult Reading Test (NART). Performance does not decline with age, in contrast to an associated test of verbal recognition memory. A second study attempted to test the effect of intellectual deterioration due to age on Spot-the-Word performance. Elderly subjects who had high vocabulary scores scored well on the Spot-the-Word regardless of whether fluid intelligence as measured by the AH4 test was well preserved, or was low, implying intellectual deterioration. A final study collected normative data on a sample of 224 subjects stratified by age and socio-economic status, with each subject performing two parallel forms of the test, A and B, together with the NART. Correlation between the two forms was .884, while correlation with NART was .831 for Form A and .859 for Form B, suggesting adequate reliability and validity. It is concluded that the test provides a potentially useful additional method of estimating premorbid intelligence."
4102e25a54d9480d36ede343f4d9574905e00993,
46ef36950c9917375d23c505f7a39dc54bb0f0dd,
55564f0007ca35189821add7ef7785032ec0729e,
58519800a8317b00479b7b4d68cccf80f8563c16,
74168cb05f82121fa0762776b138cfe37e1f8473,
89afa95904c11cc1e7ddb1f15058b589618e4cd8,
950683242fb183cbd4493cfe41524b1dc4c394da,"The coherent frequency (CF) of the auditory evoked response (AER) is derived using auditory clicks presented at frequencies in the range 5-47 Hz. CF and psychological performance were measured while seven subjects breathed isoflurane in doses increasing from 0% to 0.2%, 0.4% and 0.8% end-tidal concentration and then decreasing to 0%. With increasing doses of isoflurane, CF decreased and there was a decrease in within-list recognition (WLR) and category recognition (CR) scores. There was a correlation between changes in CF and WLR (P < 0.05) and between CF and category recognition (CR) (P < 0.05). A painful stimulus given in conjunction with 0.4% isoflurane caused an increase in CF, WLR and CR in some subjects. This did not reach statistical significance for the group as a whole, apart from the short word interval scores in the WLR which indicated an increase in attention (P < 0.01). Subjects did not respond with 0.8% isoflurane, either before or after painful stimulation. Reduction of end-tidal isoflurane from 0.8% to 0% caused an increase in the CF and improved performance on the psychological tests. A category generation task on recovery showed no evidence of implicit learning of words presented in conjunction with 0.8% isoflurane.(ABSTRACT TRUNCATED AT 250 WORDS)"
9747aec533f6e1fa0f3f625f29bb6b50484a266f,"Abstract While the search for a theory of rehabilitation by Caramazza and Hillis is welcomed, the model they propose is criticised on the grounds that it offers little more than a recommendation that we should wait until rehabilitation has occurred, and then attempt to understand it. The theme of this comment is to propose that it is necessary not only to understand the systems being treated, but also to have a model of how they can be changed by experience. Examples are given of ways in which recent developments in the cognitive neuropsychology of learning and memory may contribute to improved rehabilitation methods. It is suggested that an adequate theory of rehabilitation must combine a knowledge of the systems undergoing treatment with an understanding of how the principles of learning may be applied to their modification."
a9b568a1b6fc8cd1d1469a42e0e6abd30393e330,"Abstract The study concerns S.R., a graduate student with a specific deficit in short-term phonological memory, whose performance on a range of memory and learning tasks is compared to a group of six fellow students. S.R. performed at a consistently lower level than the controls on a range of tasks involving short-term verbal memory, including digit span, nonword repetition, memory for dissimilar and similar consonants and words, and memory for words differing in length. He nevertheless showed normal effects of phonological similarity and word length, suggesting that his phonological loop was qualitatively normal. Short-term memory for visual patterns was normal. Long-term memory was tested using the Doors and People Test. This showed excellent visual recognition and pattern learning memory, coupled with poor performance on recognition or recall of names. By analogy with a patient with an acquired deficit in short-term phonological memory, it was predicted that S.R. would show normal verbal learning of pa..."
b10aaf169e2307e2f22cd1af47c16b60c6f92395,
db53faa9b35be39f08692e530ff1967eb87502c5,"The term ‘working memory’ refers to the system respon- sible for the temporary maintenance of information~nec- essary for performing such cognitive tasks as reasoning, understanding and learning. Evidence from a range of studies based on the functioning of working memory in normal and brain d,amaged patients, led myself and G. Hitch to propose a model for working memory that assumes it has three components. The first is an at- tentional controller, a ‘central executive’ that is respon- sible for strategy selection and cognitive control. This central system is aided by two subsidiary slave systems, the ‘phonological loop’ that is responsible for maintain- ing and manipulating speech-based information, and the ‘visuo-spatial sketchpad’ that is responsible for holding and manipulating visual images [ 1~1."
ddb1bfcc57bcfd98a0afcb2c8eaa14ef200a0c2f,
ae098ed09d4dba3dbb6ecc5f71f4f943e2bc37b3,"This paper describes a new technique for assessing ""autobiographical"" and ""personal semantic"" memory in amnesic patients and healthy controls. It provides evidence of the reliability and validity of the procedure, and reports an age-related temporal gradient in amnesic patients. The results are considered in terms of the severity, the rate of onset, and the duration of the amnesia; and a preliminary analysis is given of the findings in different diagnostic groups. The findings indicate that autobiographical and personal semantic memory show a consistent pattern of impairment, when a comparison is made which controls for the age of the memories and the subject's own past experience."
fe317f9534a36ecfff9a5a568c55cd987714c72d,
ff583ae5de13a0016bad2a9630b5fd5f0a6fa7a2,
1d08eac3953fb88e08075e725ac23e3252affc4f,
1ef133a1050efffe0432798962ae5946df66af21,
53ddbdb8fe2b8289a23b2212899fefdc7bc2ddad,"Working memory may be defined as the system for the temporary maintenance and manipulation of information, necessary for the performance of such complex cognitive activities as comprehension, learning, and reasoning. Used in this sense, the term refers to an area of research that may or may not prove to be dependent on a single coherent system. Such a system is proposed within a broad and relatively speculative overview of human memory that emphasizes the putative role of working memory. This is followed by a brief account of a particular model of working memory, and a more detailed discussion of the way in which the various subcomponents of the model relate to other aspects of memory and cognition."
61c4ae2e105f9bb19c2a8e148bf9763f50e50ad1,"Abstract This study compared memory and attention functioning in three groups of patients: those in post-traumatic amnesia (PTA), those with the amnesic syndrome (AS) and those with chronic memory impairment (CMI) following severe head injury, but who were no longer in PTA. We also tested a control group, most of whom had sustained orthopaedic injuries. Subjects were assessed on tests of immediate and delayed memory, semantic and episodic memory, procedural learning, speed of comprehension, and attention. Patients in PTA differed from all other groups on semantic processing (errors), verbal fluency (animals) and simple reaction time. The best measures for discriminating PTA were accuracy of comprehension, verbal fluency, delayed logical memory, simple reaction time, and backward digit span."
61fd0cac80c1281510860b143b4afc75e40dfcb1,
6f3e7a0d7c9989ac48fc1752d62ac2d768bb65d7,
789339b376663549e51ca44d6e62a47b7f1ef15b,
81697caa90b99b2245a03de7a9359c2b862c788c,
983a2c19a9d82c79a93cc5b6016ebeac484dc1b7,"I am honoured and pleased to have been invited to give the Bartlett Lecture. I feel a particular debt of gratitude to Bartlett as he was instrumental in founding the Unit in which I have spent most of my professional life, a Unit that embodied many of Sir Frederic’s strengths, and which I hope continues to do so. When I arrived at the Unit as a young graduate research worker from University College London, Sir Frederic had already retired as Professor of Psychology and Director of the Unit. However, he still had a room at the Unit and maintained an interest in its activities, often bringing round visitors from abroad. In my own case my research stemmed from the attempt to design a postal code, and involved a series of very un-Bartlettian experiments on the learning of nonsense syllables, a perversion that Sir Frederic seemed to tolerate with a rather grandfatherly indulgence. Were he here today, I trust that he would at least approve of the fact that my lecture will contain relatively little work on nonsense material, while noting that I have not yet managed to kick the nonsense habit completely. The topic of my lecture is a direct continuation of research on short-term memory that, although not pursued by Bartlett himself, owed much of it’s early development to Bartlett’s students, Broadbent, Brown, and Conrad. I no longer use the term “short-term memory”, but I certainly regard my research on working memory as a continuation and elaboration of an existing tradition, rather than a rejection of old models for new, such as is occasionally suggested by colleagues who refer to the “demise of short-term memory” (Crowder, 1982). Although disposable concepts may have their attractions, as will become clear, my own inclinations are towards attempting to build in durability and ease of maintenance."
9c21257b1e4b1ea178b38582968297582bc8c062,"Earlier research suggests that AD patients tend to be particularly impaired in the central executive component of working memory, leading to problems in coordinating information from different sources. This suggests that they may be particularly handicapped in keeping track of conversations involving several people. This was studied in 19 AD and 19 matched control subjects. The patients were screened to minimize problems of face recognition and language comprehension and were then shown videotapes of conversations involving from two to five characters. After each tape, a statement made by one of the characters was presented and the subject required to point to the person who had made that statement. Performance was at ceiling for normals, except when the speakers had changed location, when some errors occured. AD patients showed a clear tendency for performance to deteriorate as number of speakers increased, and to show higher error rates when participants changed location. Implications are discussed for AD patients attempting to cope with everyday social situations."
9c7c0c21a3deb33faf33a6433b621dea05c765ca,
a21355d53c2844af973fd88050d37a9f3baf08b4,"SYNOPSIS In a sample of 60 schizophrenic patients encompassing all grades of severity and chronicity memory impairment was found to be prevalent, often substantial, and disproportionate to the overall level of intellectual impairment. The deficits were not easily attributable to poor cooperation, attention or motivation; nor were they related to neuroleptic or anticholinergic medication. Memory impairment was significantly associated with severity and chronicity of illness and also with negative symptoms and formal thought disorder. There was evidence from the sample as a whole, and from a more detailed examination of five patients with relatively isolated deficits, that schizophrenic memory impairment conformed to the pattern seen in the classical amnesic syndrome. Additionally, there was preliminary evidence for a marked deficit in semantic memory."
a7069c41d9a9cfefb56c83903e8feda8a92deadb,
cd1597c0b6c31a3eebcc41dfb0811079e6582116,"The nature of the developmental association between phonological memory and vocabulary knowledge was explored in a longitudinal study. At each of 4 waves (at ages 4, 5, 6, and 8 yrs), measures of vocabulary, phonological memory, nonverbal intelligence, and reading were taken from 80 children. Comparisons of cross-lagged partial correlations revealed a significant shift in the causal underpinnings of the relationship between phonological memory and vocabulary development before and after 5 yrs of age. Between 4 and 5 yrs, phonological memory skills appeared to exert a direct causal influence on vocabulary acquisition. Subsequently, though, vocabulary knowledge became the major pacemaker in the developmental relationship, with the earlier influence of phonological memory on vocabulary development subsiding to a nonsignificant level. (PsycINFO Database Record (c) 2008 APA, all rights reserved)"
1911585d693fb1092dc4b4bf37cfbcdc71a749b4,"Becker (1988) has argued that Alzheimer's disease is particularly characterised by a combination of amnesic and dysexecutive deficits. He has supported this hypothesis by identifying patients who represent a relatively pure example of each of these. We describe a search for similarly pure patients in a sample of 55 carefully selected Alzheimer cases. We succeed in identifying one case each of relatively pure amnesia and relatively pure dysexecutive syndrome. We also, however, find cases of predominant STM deficit, as well as cases with defective visual but not verbal memory, and cases of the converse pattern. These cases do not seem to reflect simple random variation in the data, since less theoretically coherent patterns of symptoms are not found in this pure form. We conclude that AD can give rise to relatively specific cognitive deficits during its early stages, but that these do not necessarily argue for Becker's two-component interpretation of the cognitive deficit in Alzheimer's disease."
3c27632a572a71c9803a52333b13a5d23bf77d84,
677a11c7e0daca0b8c2b05667396ed8cdd417be4,"The opportunity taken by Snowling, Chiat, and Hulme to step into the debate concerning the nature of the relationship between nonword repetition abilities and vocabulary acquisition in young children should be welcomed. Vocabulary size is strongly associated with a range of abilities, including general intelligence scores, reading ability, reading comprehension, and school success (e.g., Anderson & Freebody, 1981) and as a consequence, vocabulary knowledge provides the major index of verbal intelligence in many standardized ability tests used with both children and adults. Given the weight attached by psychologists to vocabulary knowledge, it seems surprising that until recently the cognitive processes underpinning word learning had been largely neglected. Any progress in understanding the psychological constraints in vocabulary development, whether it takes the form of informed theoretical debate or further empirical work, should therefore be encouraged."
768a72d62aae2173289dbf6b7ba82ca326b8d287,"ABSTRACT It has recently been suggested that the developmental association between nonword repetition performance and vocabulary knowledge reflects the contribution of phonological memory processes to vocabulary acquisition (e.g., Gathercole & Baddeley, 1989). An alternative account of the association is that the child uses existing vocabulary knowledge to support memory for nonwords. The present article tests between these two alternative accounts by evaluating the role of phonological memory and linguistic factors in nonword repetition. In a longitudinal database, repetition accuracy in 4-, 5-, and 6-year-olds was found to be sensitive to two independent factors: a phonological memory factor, nonword length, and a linguistic factor, wordlikeness. To explain these combined influences, it is suggested that repeating nonwords involves temporary phonological memory storage which may be supported by either a specific lexical analogy or by an appropriate abstract phonological frame generated from structurally similar vocabulary items."
b2a87c638c106e7eedabd854c02b02b65378c301,"A previous study (Baddeley et al., 1986) explored the hypothesis that patients suffering from dementia of the Alzheimer type (AD) are particularly impaired in the functioning of the central executive component of working memory. It showed that, when patients are required to perform 2 concurrent tasks simultaneously, the AD patients are particularly impaired, even when level of performance on the individual tasks is equated with that of age-matched controls. Although the results were clear, interpretation was still complicated by 2 issues: first, the question of comparability of performance on the separate tests between AD and control patients; secondly, the question of whether our results could be interpreted simply in terms of a limited general processing capacity being more taxed by more difficult dual tasks than by the individual tasks performed alone. The present study followed up the AD and control patients after 6 and 12 mths. We were able to allow for the problem of comparability of performance by using patients as their own control. Under these conditions, there is a very clear tendency for dual task performance to deteriorate while single task performance is maintained. A second experiment varied difficulty within a single task in which patients and controls were required to categorize words as belonging to 1, 2 or 4 semantic categories. There was a clear effect of number of categories on performance and a systematic decline in performance over time. There was, however, no interaction between task difficulty as measured by number of alternatives and rate of deterioration, suggesting that the progressive deterioration in performance shown by AD patients is a function of whether single or dual task performance is required, and is not dependent on simple level of task difficulty. Implications for the analysis of the central executive component of working memory are discussed."
b2f23812c51a93ea15f050aad7ccfa8c9073217c,"Abstract : A distinction between voluntary/controlled and stimulus-driven/ automatic behavior has been separately applied to the effects of frontal lobe lesions, individual differences in general intelligence or Spearman's g, and interference between dissimilar, concurrent tasks. We suggest that these three problems are indeed closely linked, all concerning a process of selection between alternative goals or abstract requirements on behavior, especially under conditions of novelty and /or weak environmental cues to action. Among our findings are: (1) Executive deficits following frontal lesions are specifically associated with losses in fluid intelligence. (2) Conventional frontal tests have little in common besides g. (3) Across a wide range of spatial and verbal tasks, dual task interference is closely related to both g correlations and frontal lobe involvement. This may only be true, however, when the secondary task is random sequence generation, designed to avoid stereotype. (4) Frontal patients and people from the lower part of the g distribution share a tendency to goal neglect, or disregard of a task requirement even though that requirement has been understood. Neglect is confined to novel behavior, eliminated by verbal prompts, and sensitive to the number of concurrent goals. (5) In speeded stimulus classification, switching classification rules produces high g correlations. Correlations rapidly decrease, however, with practice on a fixed rule. The results begin to clarify the role of executive control in the organization of behavior. Working memory, Central executive, Frontal lobes, Intelligence."
caedeaa310c97d2e507429f5f88bcbecc0c5df7d,
edfb41585d71cc30e2f69d680bafa044d599cb3e,"Abstract Three experiments studied the long-term retention of parking locations. In Experiment 1, members of the Applied Psychology Unit (MU) attempted to recall where they had parked during the morning and afternoon of each of the previous 12 working days. A marked recency effect was observed. In Experiment 2, members of the APU Subject Panel were invited for a single test session, and asked where they had parked after a delay of 2 hours, 1 week or 1 month. Recall was excellent and did not differ as a function of delay, allowing a simple trace decay interpretation to be ruled out. A third experiment invited subjects to attend on two occasions separated by a 2-week interval. The subjects were then required to recall their parking locations some 4 weeks after either their first visit or their second visit. Performance in both groups was inferior to that observed in Experiment 2, and declined over time. A temporal discrimination model, based on laboratory studies of both long-term and short-term recency in ..."
fae6f44ffb271a1d3c49066122005a56b9efe512,"A study of 4-and 5-year-old children investigated whether measures of phonological memory and rhyme awareness reflect a common phonological processing skill or differentiable phonological abilities. Tests of phonological memory, rhyme oddity detection, reading, vocabulary and non-verbal intelligence were given to the children in each age group. Factor analyses performed on the measures showed that phonological memory and rhyme awareness measures did indeed share a common phonological processing component, but other analyses established that the two types of phonological processing task were nonetheless differentially linked with reading and vocabulary development. The two phonological memory measures taken in the study–non-word repetition and digit span–were significantly related to vocabulary knowledge in both the ages 4 and 5 groups, and to reading achievement at age 5, but not age 4. These findings replicate and extend our earlier findings. In contrast, rhyme awareness scores were not significantly associated with vocabulary knowledge at either age, but were strongly related to scores on one of the reading tests, a multiple-choice measure, at both ages 4 and 5. The pattern of findings indicates that, although there appears to be a common phonological processing component underpinning phonological memory and phonological awareness tasks, the tasks also reflect separate cognitive skills which make differential contributions to reading and vocabulary development."
22617da1859d0f2921c0638d4174bec072c76bb1,"The multidimensional nature of the deficits presented by patients referred for neurological rehabilitation poses problems for therapists in selecting appropriate assessments. Although many patients will exhibit memory problems on admission, most will also show signs of other impairment, such as visuoperceptual deficit; but few tests exist that take into account the effect of such influences. The Rivermead Behavioural Memory Test, which was developed to measure and monitor everyday memory problems in patients with acquired brain damage, has been modified to provide a subscale of items that are sensitive to memory impairment but insensitive to perceptual deficit. Revised norms are presented, based on a sample of 100 brain-injured subjects. Results indicate that use of the subscale reduces the likelihood of overestimating the severity of memory deficit in perceptually impaired patients."
3b7c0ade6d6a5409e4984b5765fd6334207ed192,
3c730da61f1dc2fdd4a320a2a387eda6f5e66c55,"Why Do We Need Memory? Perceiving and Remembering. How Many Kinds of Memory? The Evidence for STM. The Role of Memory in Cognition - Working Memory. Visual Memory and the Visuo-spatial Sketchpad. Attention and the Control of Memory. When Practice Makes Perfect. Organizing and Learning. Acquiring Habits. When Memory Fails. Retrieval. Recollection and Autobiographical Memory. Where Next? Connectionism Rides Again. Knowledge. Memory, Emotion and Cognition. Understanding Amnesia. Treating Memory Problems. Consciousness. Implicit Knowledge and Learning. Implicit Memory and Recollection."
3fb8490d4c96fa66d2d5d8a177a1eb7662fbe8be,"Synopsis Memory impairment is not usually considered to form part of the clinical picture of schizophrenia, except perhaps in severely deteriorated patients. In a survey of 60 patients encompassing all grades of severity and chronicity poor memory performance was found to be common, sometimes substantial, and disproportionately pronounced compared to the degree of general intellectual impairment. Although associated with severity and chronicity of illness, impaired memory was by no means confined to old, institutionalized, or markedly deteriorated patients. The pattern of deficit appeared to resemble that of the classic amnesic syndrome rather than that seen in Alzheimer-type dementia."
7c6a814cede38250a1671b00aa0e7d7b0f4c920d,"Correlational studies have suggested that immediate phonological memory, as measured by the capacity to repeat back non-words varying in length, is associated with level of vocabulary in young children. The present study explores the possibility of a causal relationship between phonological memory and vocabulary acquisition by testing the abilities of children high and low in repetition performance to learn labels for unfamiliar toy animals. The low repetition children were found to be slower at learning phonologically unfamiliar names such as ‘Pimas’ for the toys, although there was no difference in learning speed for familiar names such as ‘Thomas’. The two groups also differed one day later in their retention of the labels that had initially been learned. These results suggest that immediate memory processes are directly involved in the learning of new vocabulary items in young children. The possible nature of the contribution of temporary phonological memory to vocabulary acquisition is discussed."
9f4bb332a21557a306fee149dfa6e5e7785b0ade,J. Rasmussen: Human error and the problem of causality in the analysis of accidents D. Dorner: The logic of failure J. Reason: The contribution of latent human failures to the breakdown of complex systems R. Patterson: Auditory warning sounds in the work environment D.E. Broadbent: Effective decisions and their verbal justification R. Green: Human error on the flight deck A.N. Nicholson: Medication and skilled work A.P. Smith: Respiratory virus infections and performance M.F. Allnut: Sustained performance and some effects on the design and operation of complex systems S. Folkard: Circadian performance rhythms: some practical and theoretical implications J. Fox: Automating assistance for safety critical decisions V. de Keyser: Temporal decision making in complex environments N. Moray: A lattice theory approach to the structure of mental models D.A. Norman: The 'problem' with automation: inappropriate feedback and interaction not 'over-automation' Index.
b457c62c291364acd87b8c2f48d8c3f3deb06edd,
da3ecfa9ccfb7e16a87fd4415cf3b524c14d463d,"Synopsis Levinger & Clarke (1961) found that subjects tended to forget more word associations to emotional rather than neutral words. Kline (1981) regarded this study as providing “irrefutable evidence for the Freudian concept of repression”. On the other hand, results from Kleinsmith & Kaplan (1964) suggested that this effect may reverse after a delay. The present study attempted to replicate Levinger & Clark's result using stimulus words balanced for concreteness and frequency. In addition, recall was tested immediately and after a delay. Associations to emotional material were more poorly recalled immediately than associations to material which was low in emotional valence, but tended to be better recalled a month later. There were no differences in recall of pleasant versus unpleasant material. Results do not support the repression hypothesis, in that emotional material becomes more memorable over time. Moreover, repression should differentially apply to unpleasant rather than pleasant material but no such differences were found."
e448c51e1b7f4b7186a78e4c1ac058b346f6b737,
e6a37f6f351304314ae3101812cb68aa9a303740,
e7fc2a7e9778e2bcece8165e0c484e280812f81c,
e8d0bb83e49cbe7badfb527a69841ff9df0d9e1b,
e96a6f36c08e168de62688d9717e4dbd07fb49ab,
eedd9496abcb4c63b6d230310d8122a2a86bb992,"An assessment of the current memory status of patients referred for speech therapy is rarely provided, but may be useful in indicating whether there is memory loss in addition to language deficits. Failure to progress during treatment could be due to failure to remember the content of the previous sessions. Most existing memory tests are unsuitable for assessing subjects with dysphasia. For that reason we sought to modify the Rivermead Behavioural Memory Test (RBMT: Wilson, Cockburn & Baddeley, 1985), which was developed to measure the existence and extent of everyday memory problems and which contains both verbally and non-verbally mediated items. The study described here, based on a sample of 176 brain-injured patients, showed that a mild to moderate language deficit did impair performance on the RBMT. The influence was found to be limited to a subset of verbal memory items. A shortened version of the test was developed and shown to be sensitive to memory deficits and insensitive to the effects of dysphasia. Revised norms for this version of the test are provided. These should enable an estimate to be made of the presence and magnitude of memory deficits in dysphasic patients, allowing treatment to be optimized."
f85d54170ffbb2a016cefd3794c76e75e41a24da,
181040048fc9322329bb486ccc4bb3ff47f5b40f,
1fcccc1bbc92ae0becd0730babf41f49acae8ea7,
421e28e7b11b268eff3b9c26bf71019384ba218f,
4e402e06abb140dd973f7110949ec2520ceb91ee,
51b92491db9d597c8c588130dba0e45bef403b08,
71ff062b164cec96282742a1c83f2cafb7421ac7,
77669a859fc4420ceb9db9a7563d199b2ace2601,
82530a2eb9874339b7345b8b0de78fa04f43cf98,
b7a1ed77089d616885ebf216e7577aff439d4202,
edec6fdd106caad8be3f40a8e93f22665e7f4e0b,"This paper describes the Rivermead Behavioural Memory Test (RMBT)-a short test of everyday memory problems with four parallel forms. It was administered to 118 control subjects aged between 16-69 years with a mean IQ of 106 (range 68-136). The limit of normal performance was established on this group and cut-off points were determined for individual components of the test. The test was also given to 176 brain-damaged people and its validity assessed both by correlating RMBT scores to performance on existing tests, to subjective ratings from patients and carers and to observation by therapists of memory lapses. Validity, parallel form and interrater reliability all proved to be high. It is concluded that the RBMT is a short, reliable, and valid test of everyday memory problems."
f8b2cc69ddad89c8b90b09c1a24cd1f7298e31bd,"Immediate memory for visually presented verbal material is disrupted by concurrent speech, even when the speech is unattended and in a foreign language. Unattended noise does not produce a reliable decrement. These results have been interpreted in terms of a phonological short-term store that excludes non-speechlike sounds. The characteristics of this exclusion process were explored by studying the effects of music on the serial recall of sequences of nine digits presented visually. Experiment 1 compared the effects of unattended vocal or instrumental music with quiet and showed that both types of music disrupted STM performance, with vocal music being more disruptive than instrumental music. Experiment 2 attempted to replicate this result using more highly trained subjects. Vocal music caused significantly more disruption than instrumental music, which was not significantly worse than the silent control condition. Experiment 3 compared instrumental music with unattended speech and with noise modulated in amplitude, the degree of modulation being the same as in speech. The results showed that the noise condition did not differ from silence; both of these proved less disruptive than instrumental music, which was in turn less disruptive than the unattended speech condition. Theoretical interpretation of these results and their potential practical implications for the disruption of cognitive performance by background music are discussed."
0bd2ebb828ba8cc88f3bbc5725210ae0f34c16d6,
11525aeb3911fd60b173ee62d01ff798660f5f89,
18fa819bbb5a34a48d5a8f53c57968b83beea4d3,
1ca45d38f2b4200dd66f63db4ca692c854a3e0d8,
27a3e7821e4c696126b7c2bb99fe66f5134a3ed7,
4c0402644278a10b804cb954d56b852c1cab8a48,
4ceafb780e3a170c0a2d3ffdf2d774d15869ea81,
6af6786d4f57ca750b21d56e788d28723e40fbe4,
75bfd15696332846137a6544f139e2567f464275,
863594b5c3793a54115ec7bdffadbe21d2f8b638,
ad56ac0e2d9d8ce707f89f39eafc32a58c82f1be,
ad8c9a454c3d65132c07710d71e50af5f288ca74,"This paper reviews recent neuropsychological studies of primary-memory functioning in early Alzheimer-type dementia (AD) with specific reference to recent research on working memory. The deficits in primary memory follow a fairly distinct pattern with a comparatively unimpaired recency effect in free recall, a moderate impairment in memory span, and a more substantial impairment in short-term retention following distraction. The nature of these deficits is considered in relation to the Working Memory Model (Baddeley & Hitch, 1974). It is suggested that the Articulatory Loop System is functioning normally but that there is an impairment in the control processes of working memory."
cff2da737428f97e519a4f0fbe8e2d9deba6c1be,"Abstract The memory performance of patients suffering from senile dementia of the Alzheimer type (SDAT) (N = 29), normal subjects of equivalent age and education (N = 58), and young normal controls (N = 42) was tested using free recall and verbal and nonverbal span. Three measures were derived from the free recall task: primacy based on the first item, secondary memory based on the middle serial positions, and primary memory based on recency and the Waugh-Norman correction factor. The SDAT patients differed from the normal elderly on all free recall and span measures except for primary memory. The elderly were clearly inferior to the young on secondary memory, and were marginally poorer on primary memory and the two span measures. Three possible explanations of this pattern of results are considered, based on the dichotomous modal model of memory, levels of processing, and working memory. It is suggested that the assumption that SDAT patients suffer from a deficit in the central executive component of wor..."
d9ce6b37e34fd580511392751af16ba823f717f0,
10cc0c1ea4f45efd9246ce2872cb1482d5e611c7,"Trainee parachutists were tested on two occasions during a two-day training course. The first test was always on day 1 when no jumping occurred, while the second test either occurred on the afternoon of day 1 (control group) or occurred immediately prior to entering the aircraft for their first ever jump on the afternoon of day 2 (fear group). Level of anxiety was assessed both subjectively and by ambulatory heart-rate monitoring, and a range of performance tests were given. Subjects proved to be significantly more anxious in the pre-jump fear condition on both subjective and heart-rate measures. A significant decrement in performance was observed for digit span, logical reasoning test accuracy (though not speed), speed of letter search and speed of performing the visuo-spatial Manikin test. Stroop task performance was not influenced by anxiety. There was no correlation between rated jump performance and either anxiety or personality as measured by the Eysenck Personality Questionnaire."
2042f245fe67e8b5e781c8099640aef3f27cff4c,"Abstract The grammatical and semantic processing of auditorily presented sentences and passages of prose was investigated in a left brain-damaged patient (PV), who has a reduced auditory memory span, interpreted in terms of a selective deficit of the phonological short-term store component of working memory. In the case of short sentences the patient's performance is well within the normal range, whether tested by sentence-picture matching or by the detection of syntactic or semantic anomalies. She retains an intact capacity to detect semantic anomalies whether tested using short sentences, long sentences, or prose passages. She retains some capacity for detecting syntactic anomalies even in long sentences, provided these are tested under conditions where such mismatches are very frequent; when they are embedded in more varied material, however, her performance deteriorates. Finally, when the syntactic anomaly involves an anaphoric mismatch across sentences, her performance drops to chance level. These re..."
2269bd5949d6e3ad9a5b019f44a78adc0f99149b,
68f49c34a78e457566d25a2e3f48cacaea441dda,"Three-quarter views of faces promote better recognition memory for previously unfamiliar faces than do full-face views. This paper reports experiments which examine the possible basis of the effect, and, in particular, examine whether the effect reflects some ‘canonical’ role for the 3/4 view of a face. Experiment 1 showed no advantage of 3/4 views over full-face views when the task was to decide whether or not each of a series of faces was that of a highly familiar colleague. In Experiment 2 a sequential matching task was used, where subjects had to respond positively if both members of a pair of faces were of the same person. When the faces used were highly familiar to the subjects, there was no evidence of an advantage for a 3/4 view in the matching task. Three-quarter views and full-face views led to equivalent performance, though profiles produced decrements in performance. When the same faces were shown to subjects who were unfamiliar with the faces, 3/4 views did lead to increased speeds in same trials, compared with full-face, though profiles again proved difficult. Thus a 3/4 view advantage appeared only where the faces were unfamiliar, and the task had to be performed at the level of visual matching. It appears that the 3/4 view advantage may be obtained only when the task involves explicit matching between test views and remembered target photographs, rather than reflecting any more fundamental properties of the representations used to recognize highly familiar faces."
a27acc976e4ca0fdc3f0cebf6cc922213d1bdd6d,
ccf8b8df2582dbe681557467bb73c9f31fcdf910,"Studies of ‘noise pollution’ have typically used unpatterned white noise. The present study compares the effect of noise with that of unattended speech. Three experiments required the immediate serial recall of sequences of nine visually presented digits accompanied by silence, noise or unattended speech in an unfamiliar language. Experiments 1 and 2 showed a clear effect of unattended speech at both 75dB(A) and 95dB(A), while unattended noise had no effect in either study. Experiment 3 used a separate groups design combining 95 dB(A) noise and quiet with instructions either to remain silent or to rehearse overtly. Overt rehearsal enhanced recall, while unattended noise again had no effect. It is suggested that noise does not interfere with short-term memory but that unattended speech does impair performance by disrupting the articulatory loop component of working memory. Implications for studies of ‘noise pollution’ are discussed."
d92e047834a14692743219a694b5c6dcf649867a,
e81f93d61d1af0126dee6a48da1168be02c7c940,
eb31c0cae73400ab3c3eab20592eea2104f773c8,"This paper describes five experiments with two main aims; first, to assess the effects of variations in photographic facial pose on recognizability. A second, more fundamental aim was to examine the reliability of laboratory findings relevant to face recognition, in more ecologically valid settings, and to develop a methodology to further this aim. In Experiment 1 best performance was achieved with provision of three facial poses, namely full-face, three-quarter and profile views. With a single pose a three-quarter view was the most helpful. Experiments 2 and 3 attempted to evaluate these findings in a real-life setting using the general public or paid volunteers who attempted to identify a live target in a town centre from a previously presented photograph. The general public produced a very poor response, and the volunteers produced a low detection and high false recognition rate. Experiment 4 increased constraints on the experimental environment and produced a reasonable hit rate, but no effects of pose. Experiment 5 was a laboratory study where the presentation of a live target preceded photographic recognition. Effects of pose reappeared in line with results of Experiment 1. These results underline the danger involved in making practical recommendations arising from purely laboratory based research."
037c6eac8783646443d7bcfacdbd20b91443eaf5,"This study explored the hypothesis that patients suffering from dementia of the Alzheimer type (DAT) are particularly impaired in the functioning of the Central Executive component of working memory, and that this will be reflected in the capacity of patients to perform simultaneously two concurrent tasks. DAT patients, age-matched controls and young controls were required to combine performance on a tracking task with each of three concurrent tasks, articulatory suppression, simple reaction time to a tone and auditory digit span. The difficulty of the tracking task and length of digit sequence were both adjusted so as to equate performance across the three groups when the tasks were performed alone. When digit span or concurrent RT were combined with tracking, the deterioration in performance shown by the DAT patients was particularly marked."
25d87bf4d8ad7a61668e7a85bc8342d0d6839d03,
2fad7fa4eca3043b9910726bb8b657bf56470514,"One approach to the understanding of normal cognitive function is to investigate its breakdown following brain damage. In the area of memory this has proved a very profitable strategy; typically, concepts and techniques developed in the psychological laboratory have been used to analyze memory deficits in the clinic, and this in turn has frequently led to further development and modification of the initial model of normal memory. This chapter is concerned with the early stages of an attempt to approach autobiographical memory within this tradition. Because the study of autobiographical memory in normal subjects is still at a very early stage of development, conclusions are likely to be tentative. We shall be concentrating, however, on a phenomenon that we believe to be sufficiently striking and unequivocal to allow some conclusions, even though our data on normal control subjects are still sparse. We believe that our results, although preliminary, raise interesting questions both for the understanding of normal autobiographical memory and for its breakdown in amnesic patients. We shall begin by describing our current technique for investigating autobiographical memory, together with its application to a range of patients, most of whom were suffering from long-term memory problems. After a general overview of the autobiographical memory performance of these patients, we shall concentrate on four patients with frontal lobe damage, two of whom show clear evidence of confabulation. These latter patients will be described in more detail, an explanation of their autobiographical memory defect will be proposed, and its implications for autobiographical memory in normal subjects will be discussed."
348242c6b7c8999893bf0492411bcd73620856ee,"Broadbent (1983) has suggested that the influence of unattended speech on immediate serial recall is a perceptual phenomenon rather than a memory phenomenon. In order to test this, subjects were required to classify visually presented pairs of consonants on the basis of either case or rhyme. They were tested both in silence and against a background of continuous spoken Arabic presented at 75 dB(A). No effect of unattended speech was observed on either the speed or accuracy of processing. A further study required subjects to decide whether visually presented nonwords were homophonous with real words. Again, performance was not impaired by unattended speech, although a clear effect was observed on an immediate serial memory task. Our results give no support to the perceptual interpretation of the unattended speech effect."
4878937df643a0c1996cdc461363fb6ab869aadf,"The present special issue is the result of a call for papers on human memory, preferably with a biological perspective, so as to make it an appropriate companion issue to the animal section issue on memory. When invited to write an editorial, I accepted, provided I was to be allowed the traditional prerogative of writers of editorials, namely licence to substitute opinion for evidence and speculation for expertise. The topic on which I want to offer my ill-formed speculation is the traditional question of the extent to which human memory is modular, on the one hand, and to what extent Lashley’s concept of mass-action can be applied to those parts of the brain responsible for memory. I speculate with some trepidation, as a cognitive psychologist dabbling in a classical area of physiological psychology. I would suggest, however, that this is an issue of direct current interest both to cognitive psychologists and to those interested in memory in animals, and as such is perhaps an appropriate editorial topic for this particular issue of the journal. Over the last decade or so, the combination of neuropsychological single case techniques with the development of cognitive approaches to human memory have produced a considerable advance in our functional understanding of human memory. Theoretical development has typically occurred by means of a series of dissociations established between different aspects of memory, allowing the previous unitary concept of memory to be fractionated into a number of interrelated subsystems. The first example of this was the distinction between short-term and long-term memory strongly supported by studies of the classical amnesic patient H M (Milner, 1966), followed by the demonstration that this neuropsychological dissociation could be tied in with the theoretical distinctions proposed by cognitive psychologists working with normal"
5945adb24e43efe20f31fbc92918143eec52cd7e,"The everyday memory of a group of elderly adults was assessed using techniques developed for use with younger head-injured people (Sunderland et al., 1983). The participants completed a memory questionnaire and a daily checklist; their spouses gave their assessment using a separate questionnaire. These subjective methods showed only moderate agreement, and the questionnaire had low test-retest reliability. It appears that these methods of subjective memory assessment have little validity when used with normal elderly adults. Two positive findings did emerge: As in previous studies, a story recall test was the strongest predictor of reported memory performance; and despite a universal belief among elderly adults that their memory had deteriorated with age, very few of them felt that they were at all handicapped by forgetfulness in everyday life."
6afcee7556b36a2ee7f3833603fbbd9fd264880a,
8836975114f1ebcde6682f1fcef5b1623cf8fdc1,
a880767578ce1164257a437dd4a6534b6ce36d37,
cd1fc110e4742a62ce2f4c138b1e5573537e9956,
ef5978e62ab40de7b0c2054e61828bb843cc4962,
10ebbec733eaef0ba16bebb650f38aadc484dd4b,
152b74b1d7db8de61a80b6dbc7d42fcbf07d9b23,"This paper reports psychological observations on men during a number of simulated (pressure chamber) dives. The first investigated cognitive performance during a dive to 540 m of sea water (MSW) in oxyhelium (HeO2( and allowed a direct comparison with earlier results in trimix (HeN202). Impairments were detected at depths exceeding 300 MSW and these increased with depth. However, the decrements were not as great as those obtained at similar depths in trimix. The second dive to 61 MSW used a mixture of oxygen and nitrogen to determine the role of nitrogen in the trimix dive. Impairments were small, suggesting that effects with trimix were due to an interaction between the presence of nitrogen and overall pressure. This dive also provided an assessment of the sensitivity of our tests at the maximum recommended depth for diving in air (50 MSW). The absence of effects at depths greater than this recommended maximum suggested that results with the deeper dives could not be attributed to our use of an oversensi..."
1648bcc2ba093802ddb1679822916b5f9c35ffc7,
3b277b14adc2c0e918b83ec058decd882911da30,"Abstract The open-sea performance of divers at depth may be substantially lower than would be predicted from dry pressure-chamber simulation studies. This has been attributed to the effects of anxiety. This hypothesis was explored using a land-based study in which the manual dexterity of 32 novice divers was tested using a screwplate test on two occasions, immediately before a potentially stressful open-sea dive, and at a time when diving was not imminent. Both pulse rate and subjective ratings suggested that the pre-dive condition was associated with anxiety. Speed of completing the screwplate test deteriorated by approximately 6%, a significant but not large impairment. Implications for the anxiety hypothesis of open-sea decrement are discussed."
86b7a6b5a49cd01f6c9762e17875b0e286ad8f2b,
9c07737f22520565496b476858c00967b1489bc9,
1397a93f38aa9e563307258faa9834b30ae37775,
247e6489be365a1dbfac931cd6accb8fa825c0ff,"A recurrent theme in the study of human memory over the last 20 years has been the question of whether it should be regarded as a unitary system, or as a collection of two or more subsystems. After a few years of relative quiescence, this topic has begun to move back into the theoretical limelight and, as has often been the case in this area, the stimulus for theoretical development has come from the study of clinical evidence from patients with memory deficits. Before going on to discuss these recent developments, it would perhaps be helpful to outline the earlier background to the controversy. (A more detailed discussion is given in Baddeley, 1983a, chapters 9-11.) Up to and including the 1950s, the study of memory was largely concerned with the learning of lists of words or paragraphs of prose and their retention over periods ranging from a few minutes to a few years. Memory was, implicitly at least, regarded as a single unitary system. During the latter half of the 1950s, however, a number of investigators began to observe that even small amounts of material would show signs of being forgotten over a matter of seconds, provided that the subject was prevented from continually rehearsing it, and they postulated a separate short-term memory system to account for their results (e.g. Broadbent, 1958). The 1960s saw the development of a controversy as to whether it was, in fact, necessary to assume separate systems to account for these new results, or whether existing theories could explain both sets of data (Melton, 1963). This produced a great deal of experimental work attempting to look for clear empirical evidence for or against two separate memory systems. Many apparent differences were observed, with the following three being among the more theoretically cogent."
5d99174e5bad291614b26dc8ef5b4b0503c56701,
64e435bd13928ea40f77aac89fccf2ebd96318eb,"A series of five experiments explore the influence of articulatory suppression on immediate memory for auditorily presented items with a view to testing the revised concept of an articulatory loop. Experiments 1, 2 and 3 demonstrate that the phonological similarity effect is not abolished by articulatory suppression, whether this occurs only at input or at both input and recall. Experiments 4 and 5 show that the tendency for long words to be less well remembered than short is abolished by articulatory suppression, even when presentation is auditory, provided suppression occurs during both input and recall. These results are consistent with the concept of a loop comprising a phonological store, which is responsible for the phonological similarity effect, coupled with an articulatory rehearsal process that gives rise to the word length effect."
794f81d3feedde30b77c37cca504fdbe5d7d8c2a,
874178b3e5578e68fa20659dd364c2348c0d6557,"Abstract Phonological processing and speech comprehension were investigated in a patient with left hemisphere damage and a grossly reduced auditory verbal span, attributed to a selective impairment of a phonological short-term store (Vallar & Baddeley, in press). Her phonological processing perfomance was well within the normal range as measured by phonological discrimination, the assignment of stress to words, and rhyme judgement. Comprehension of individual words and of short sentences was unaffected. However, the patient's comprehension of long sentences was defective, whether presented visually or auditorily, when preservation of the specific wording was essential for understanding. These results suggest a dissociation between the processes that perform phonological analysis of verbal information and a phonological short-term store, the latter being selectively impaired in the present case. The pattern of deficit observed in this case suggests that such a system may be useful for comprehending sentenc..."
d165f98bd8b5a11af80b435a32b27b7026f547ce,
a5af6fd6c2cb288fc4a62f511d7ff00d1c536468,
a61c0e46cc62aa8699c1828140a83cb0e43d8a57,"A series of experiments explored the role of subvocalisation in fluent reading. Experiment I showed that when subjects were required to suppress articulation while reading, their ability to detect anomalous words or errors of word order in prose was markedly impaired although speed of reading was unaffected. Experiment II showed that this decrement was not a general effect due to performing a secondary task, since a concurrent tapping task did not impair detection accuracy. A third study explored the role of acoustic interference in reading by requiring subjects to detect errors in prose while attempting to ignore irrelevant speech, with or without articulatory suppression. Once again articulatory suppression led to a clear decrement in the subject's ability to detect errors, while unattended speech had no effect on performance. None of the manipulations influenced the speed with which the subjects performed the reading task. It is concluded that subvocalisation allows the creation of a supplementary articulatory code which is produced and utilised in parallel with other aspects of reading. Such a code seems particularly suitable for monitoring order information."
0e890020c76ce1347a382ea386ddc058c96c304a,
c83758dceacb0b2a802dba87d48a8846bbf199c0,"This paper reports psychological observations on men during a simulated (pressure chamber) dive to 660 m of sea-water (msw) using a gas mixture known as Trimix (He-O2-N2). Recent studies by Bennett (1981) have suggested that this mixture allows for faster compression with less impairment in performance than the mixture traditionally used (He-O2). Data were obtained from two divers on tests of cognitive performance, namely arithmetic ability, grammatical reasoning, perceptual speed, visuo-spatial manipulation and semantic processing. At maximum depth there was a severe blanket impairment of ability to perform any of the tests. However, at shallower depths the impairments were not as marked, with performance at 300 msw close to that measured at surface pressure. Subjects were also required to fill in two questionnaires, one concerned with the quality of their previous night's sleep, the other with their mood at the time. Sleep quality was disrupted throughout the dive, with one subject affected rather more ..."
f2332eeb4fb2c189fd5014a21ea82d0b81074f80,
f445f1f9c9d5ab765602314b90a9555630a4d91d,"Inexperienced speakers were tested shortly before they presented colloquium papers at a public forum. Increases in anxiety and arousal were found using subjective and ECG measures. Performance deteriorated in digit span and verbal fluency but not in logical reasoning, tick length or the Stroop test. However, degree of decrement was small and rate of accumulating data slow, suggesting this is not an ideal situation for studying the effects of apprehension on performance."
032ace4abe7d74e5e234f0a4ba0a0b940a7e6e84,"The relation between neuropsychology and the study of normal cognitive function is discussed in the context of recent research on human memory. It is suggested that neuropsychological evidence has clear implications for the fractionation of human memory into subsystems. The distinction between long-term and short-term memory, between semantic and episodic memory, and the further fractionation of short-term or working memory all offer examples of concepts that have been successfully applied within the neuropsychological domain, and where the neuropsychological evidence has led to a modification and development of the original concept. Attempts to offer a cognitive interpretation of the amnesic syndrome are discussed. While none of these is entirely satisfactory, such work has led to a potentially important distinction between autobiographical memory or recollection, which is defective in amnesic patients, and a more perceptual or procedural learning process, which appears to be intact in such patients. Recent research on normal subjects is beginning to reveal a similar distinction. It is concluded that the relation between neuropsychology and the study of normal cognitive function continues to be an extremely fruitful one."
190105e0446c3bc6914734694c92d462420833ef,"Two experiments explored the role of subvocal articulatory rehearsal in the Peterson short-term forgetting task. In the first of these, subjects recalled consonant trigrams after an interval of 0, 5 or 15 s during which they either counted backwards in threes, suppressed articulation by continuously uttering the word “the”, or in a third control condition continuously tapped on the table. While counting backwards caused the usual dramatic forgetting, tapping caused no forgetting, and articulatory suppression only minimal forgetting at the longest delay. A second study used the same procedure but included only two conditions, articulatory suppression during the retention interval and articulatory suppression during both input and retention. Neither showed evidence of forgetting over the 15 s delay. These results suggest that covert speech is not necessary for rehearsal in short-term verbal memory. As such they call for a re-evaluation of the nature and function of rehearsal."
52494a9dcd7c421b4aeebbb35f0792f75233f44d,
7eb802715113843e68300ec4313a5a3a7fd7a284,What is memory? short-term memory working memory learning organizing and remembering forgetting repression storing knowledge retieval eyewitness testimony amnesia memory in childhood memory and ageing improving your memory.
83d1382ce899035f21222d5180e7d334e7efd27c,
969af3f704f480b5845894fdf33ec026e85e1149,
a75ecd566234828176480c3317e4c31b8d9fae4d,
aa32e2d61a72aec4256da71b08144912d56c40c7,"Two experiments attempted to improve face recognition by ac- companying the faces with elaborative contextual information regarding the personality and background of the person depicted. In neither experi- ment did elaboration lead to better subsequent recognition of the face pre- sented without context. A third study explored the role of context as a potential retrieval cue. Faces were accompanied by descriptive phrases which might or might not accompany the appropriate face during recogni- tion. A previously presented context had a substantial effect on response bias, but little effect on the discriminability of the memory trace. Compara- ble data from verbal memory are discussed, and a distinction drawn be- tween independent context in which the context and stimulus are pro- cessed separately, and interactive context in which the context modifies the encoding of the stimulus."
c9354d04c61cf20e89db646f7391584ba9a0fc7b,
ff7b110bfdff92bb916842986335cfb6929ecb93,
1e24e81a1f0a740afffac655529bf10d4c709bb8,"This study forms part of a series of simulated saturation oxyhelium dives, examining physiological and psychological changes in man in high pressure conditions. A series of five dives are reported, lasting between 18 and 26 days, and reaching maximum depths of between 300 and 540msw. Tests of cognitive functioning, including associative and short term memory, arithmetic ability, perceptual speed, spatial manipulation, grammatical reasoning and semantic processing, were administered to well-practiced subjects prior to each dive, at maximum depth and again during decompression. Self-report measures of sleep and mood questionnaires were administered for a period extending from 1 week before each dive commenced until at least 1 week after the dive was completed. The results indicate significant impairments in all performance tasks at maximum depth, except for the tests of associative memory and grammatical reasoning. A significant reduction in sleep and decrease in the subjective level of alertness was found ..."
25bde8c4e8bbc3cbe0d32f9a1478aa6b3149b11b,"Over the last 40 years, experimental psychology has concentrated almost exclusively on studying behaviour in the laboratory. While laboratory-based research is likely to retain its importance, it is essential that the theories and concepts developed in this way be exposed to the more bracing conditions found outside the laboratory. Examples of the value of such research are given from the area of stress where work on alcohol and driver performance are discussed, and from studies of everyday memory, illustrated by work on saturation advertising and on absent-mindedness. Developments so far have been primarily in the area of cognitive psychology, but it is suggested that a willingness to move outside the laboratory is likely to be even more fruitful in studying the problems of conative and orectic psychology, the study of the will and the emotions."
70bb18ad9f50b3b8d60c624b1d5ede135c6816c9,
c5c8350b331b4aa100b033b2942345b5d1b93d9d,
53fbc3d84d304d7b615801bb7bfca4627793b0ed,"Hewitt (1977) has distinguished intrinsic context, which is directly involved in the encoding of material and extrinsic context comprising such arbitrary features of the learning situation as environment of learning. While both types of context influence recall, with better performance when the original context is reinstated, recognition effects have been observed only with intrinsic context. The present study uses the contrast between the land and underwater environments to explore this apparent discrepancy. Subjects learned lists of 36 words either on land or under water, and subsequently tried to recognize them from a list of 72 words presented in either the same or the alternative environment. In contrast to an earlier recall study, no trace of context dependency was observed. Implications for the distinction between intrinsic and extrinsic context are discussed."
fdf4c9d6e9bc78d80118f3bbd77cb08bb4611965,
59a951f7ccf32720655358ebafa313527ae46113,"Abstract Experiments were carried out in an attempt to validate the facial recognition element of a three-day training course in person recognition. In one experiment the subjects had to rely on memory. In two other experiments they were provided with an array of full-face photographs to match against different poses and expressions of the same faces. The performance of subjects who had received training was never reliably better than that of untrained subjects, and in one experiment was significantly worse. Ii is suggested that the emphasis on isolated facial features in the training course may be responsible for its lack of success, and that processing independent features is not a good basts for a model of facial recognition"
a3c50a97c5bf42bc6f58816bf6c4168f0430ad50,
cc55887c947e3accf3987f1c6d348d512b0521ed,
68545dc2e4613553d29e9bd54601993d1f664a66,"Four groups of postmen were trained to type alpha-numeric code material using a conventional typewriter keyboard. Training was based on sessions lasting for one or two hours occurring once or twice per day. Learning was most efficient in the group given one session of one hour per day, and least efficient in the group trained for two 2-hour sessions. Retention was tested after one, three or nine months, and indicated a loss in speed of about 30%. Again the group trained for two daily sessions of two hours performed most poorly. It is suggested that where operationally feasible, keyboard training should be distributed over time rather than massed."
7b05e21a06057554c76db7aac2c946951f1eca98,
7d29c22ea0ad6e8debe7d3a9c54ff5da081ce96d,
751aa7dee44c347f6308c9af5d43481a7177715d,
8f63996bdb629aaa2da29d6cb630369b0db8508c,"Two studies investigated recognition of pictures of faces, focusing on the effects of changes in appearance of the face from presentation to test and type of processing or encoding. Experiment 1 demonstrated that (a) previously seen faces changed in pose and facial expression were discriminated from ""new"" faces essentially as well as pictures identical at presentation and test; (b) major changes in the appearance of a face (""disguises"") reduced recognition almost to the level of chance; and (c) subjects encoding faces in terms of personality characteristics showed better recognition performance than subjects whose processing was based on physical, facial features. Experiment 2 expanded on result (b), utilizing photographs with systematic variations in pose and in the presence/absence of glasses, wig, and beard. The design required subjects to learn names for target faces and then to identify those targets in a series of test photographs. The manipulation of pose and disguising features produced effects on probability of identification that were orderly and dramatic in magnitude. Simple changes in appearance can effectively interfere with recognition of faces."
650157ba281c28ac397de59634e7858695ebc409,
d1c316f161b0cb8b8f99a5338b099f40a11879f8,"In this chapter I will try to provide a brief overview of the concepts and techniques that are most widely used in the psychology of memory. Although it may not appear to be the case from sampling the literature, there is in fact a great deal of agreement as to what constitutes the psychology of memory, much of it developed through the interaction of the study of normal memory in the laboratory and of its breakdown in brain-damaged patients. A somewhat more detailed account can be found in Parkin & Leng (1993) and Baddeley (1999), while a more extensive overview is given by Baddeley (1997), and within the various chapters comprising the Handbook of Memory (Tulving & Craik, 2000). THE FRACTIONATION OF MEMORY The concept of human memory as a unitary faculty began to be seriously eroded in the 1960s with the proposal that long-term memory (LTM) and short-term memory (STM) represent separate systems. Among the strongest evidence for this dissociation was the contrast between two types of neuropsychological patient. Patients with the classic amnesic syndrome, typically associated with damage to the temporal lobes and hippocampi, appeared to have a quite general problem in learning and remembering new material, whether verbal or visual (Milner, 1966). They did, however, appear to have normal short-term memory (STM), as measured for example by digit span, the capacity to hear and immediately repeat back a unfamiliar sequence of numbers. Shallice & Warrington (1970) identified an exactly opposite pattern of deficit in patients with damage to the perisylvian region of the left hemisphere. Such patients had a digit span limited to one or two, but apparently normal LTM. By the late 1960s, the evidence seemed to be pointing clearly to a two-component memory system. Figure 1.1 shows the representation of such a system from an influential model of the time, that of Atkinson & Shiffrin (1968). Information is assumed to flow from the environment through a series of very brief sensory memories, that are perhaps best regarded as part of the perceptual system, into a limited capacity short-term store. They proposed that the longer an item resides in this store, the greater the probability of its transfer to LTM. Amnesic patients were assumed to have a deficit in the LTM system, and STM patients in the short-term store."
2037dd7c16a0521698734c9039da70c350302283,"Fifteen divers performed five tasks in water of temperatures 20 degrees C and 5 degrees C, using standard scuba equipment. A significant deterioration of performance occurred under the colder condition in: simple arithmetic 13%; logical reasoning 17%; word recall 37%; word recognition 11%; and manual dexterity 17%. Throughout each dive, rectal and five skin temperatures were monitored. Average fall in rectal temperature was 0.5 degrees C during 20 degrees C dives and 1.1 degrees C during 5 degrees C dives. Average body surface temperature fell by 5 degrees C and 12.5 degrees C respectively. Average heat losses calculated from the data were 95 kcal.m(-2).hr(-1) (20 degrees C dives) and 245 kcal.m(-2).hr(-1) (5 degrees C dives). The impairment in word recognition was significantly correlated with the fall in rectal temperature for the 5 degrees C dives. For other tests, the deterioration did not appear to be correlated with body-temperature changes, but rather, occurred rapidly upon cold water immersion. The significance of these findings is discussed in relation to current understanding of the mechanisms by which cold is thought to influence performance underwater."
69788f24b90342fa5489f544089d26f78700a7aa,
2d0f1ca5a82d213603f5695bfc60e93225d4e8c0,
77094793cdf77f68d68dad2334945edf978da3a7,
8d80fb042d5e5f38bd0efcbd19d6a41db70029b3,"example, Wickens’ discussion of word meaning describes findings from two experimental situations which will quite possibly prove to be very important. However, the results on release from proactive inhibition have been discussed previously (though not in such detail), and they are open to many different interpretations; while the use of brief tachistoscopic presentation to study the analysis of word meaning prior to identification of the word itself provides only weak results, and Wickens is left scraping the barrel for significant effects. Hunt and Love analyse the abilities of a mnemonist, and the results lead them to emphasize the idea of recoding, especially that of an active, abstract, linguistic nature, as against Luria’s suggestion of imagery coding. However, their discussion does not in itself provide any new insights into normal psychological functioning, nor does it provide any conceptual tools for understanding abnormal processes. The third paper which is worthy of note is that by Miller, in which he continues his search for a paradigm to link psychology and linguistics. He proposes that one define semantic components for a given linguistic realm on an intuitive basis, and then refine this account by linguistic analysis. The validity of these components may then be tested experimentally. Unfortunately, his approach involves a rather rambling discussion of an exploratory nature, and it does not seem to be linked to any clear psychological issues. Perhaps Miller has been contaminated by those philosphers who believe in the description of ordinary language as an end in itself. I t was simply improbable that the symposium would give rise to any interesting and integrated developments, given that the participants possessed neither a common experimental background, a common theoretical background, nor a common methodology. T. E. RICHARDSON It is fairly obvious why this book fails to nail its expressed aims."
b470cbb6c7c235f670bb63601da7c9d853219718,
d71d381c6371f95b4b84baa2763f147709ab3d57,"In a free recall experiment, divers learnt lists of words in two natural environments: on dry land and underwater, and recalled the words in either the environment of original learning, or in the alternative environment. Lists learnt underwater were best recalled underwater, and vice versa. A subsequent experiment shows that the disruption of moving from one environment to the other was unlikely to be responsible for context-dependent memory."
f310744345086835879c317d330e19f5ea6aa646,"The cognitive efficiency of 14 divers was studied during 1-hour exposure to water of 40°F (4.4°C) and 78°F (25.6°C). Reasoning ability was tested using a sentence comprehension task presented at the beginning and end of each test session. Vigilance was tested by requiring subjects to detect the onset of a faint peripheral light during the performance of a two-man pipe assembly task. Memory was tested by requiring subjects to learn a number of “facts” during the dive, with retention tested by recall and recognition on land, after a 40-min delay. Despite a mean drop in rectal temperature of 1.3°F (0.72°C), neither reasoning nor vigilance was impaired. Memory performance did deteriorate, though it is suggested that this may reflect a peripheral context-dependent memory effect. It is concluded that a well-motivated subject may be cognitively unimpaired despite a marked drop in deep body temperature."
11234d5bbaea19ddadffdbce1ac186a58b758fdc,
1a581a5c11c5c90f74114948f4ac209b2e57cf21,"The last two sections of the book, concerned with human processes, are on the whole far less satisfactory both in terms of content and organization. The major variable of concern in most of the papers is the developmental one. Schaffer reviews his own and others’ work on habituation in infants and children and presents convincing evidence for developmental dissociation between different response measures. The importance of careful experimental design in developmental studies is illustrated by Bryant’s discussion of his own experiments on inference in children. In a number of experiments he demonstrates that 4-year-old children can perform certain logical inferences when the experimenter ensures that the child has access to the information upon which the inference is based. This contribution benefits from being set in the context of the summaries of Piagetian theory by Etienne and Sinclair. The major weakness of this volume, as of so many conference reports, is its failure to develop some coherent theme or context in which the contributions can be presented. The division of a relatively arbitrary set of papers into sections prefaced by editorial summaries and the inclusion of a certain amount of half-hearted cross referencing is not really sufficient. In many ways the title is a canny one for, of necessity, any consideration of learning processes is concerned with constraints and this seems to be the primary rationale for a number of the contributions. However, this excess baggage often provides moments of humour as when one contributor draws a parallel between Bateson’s suggestion that chicks might prefer variations in the standard stimulus to the standard itself in an imprinting situation and our own response to the “sonata form”. A. DICKINSON"
1aae7b293953d8aef1cf640e941735dacea3834e,
33de5db6d031a1c6fa4aeb370207e2d886af03c2,
85b5fee15dc26dc169232ef42e320939028df324,
87dc1bdc473cec82c9e98030e228434eed5656df,
91cdee14248dec00916fcece3fd7828d734dd158,
c3d990ecf3b7098a080b89d1ed059192e9e054a7,"The last two sections of the book, concerned with human processes, are on the whole far less satisfactory both in terms of content and organization. The major variable of concern in most of the papers is the developmental one. Schaffer reviews his own and others’ work on habituation in infants and children and presents convincing evidence for developmental dissociation between different response measures. The importance of careful experimental design in developmental studies is illustrated by Bryant’s discussion of his own experiments on inference in children. In a number of experiments he demonstrates that 4-year-old children can perform certain logical inferences when the experimenter ensures that the child has access to the information upon which the inference is based. This contribution benefits from being set in the context of the summaries of Piagetian theory by Etienne and Sinclair. The major weakness of this volume, as of so many conference reports, is its failure to develop some coherent theme or context in which the contributions can be presented. The division of a relatively arbitrary set of papers into sections prefaced by editorial summaries and the inclusion of a certain amount of half-hearted cross referencing is not really sufficient. In many ways the title is a canny one for, of necessity, any consideration of learning processes is concerned with constraints and this seems to be the primary rationale for a number of the contributions. However, this excess baggage often provides moments of humour as when one contributor draws a parallel between Bateson’s suggestion that chicks might prefer variations in the standard stimulus to the standard itself in an imprinting situation and our own response to the “sonata form”. A. DICKINSON"
e358a780095674ab203c2a4cac4a7e72ec8410b8,"It is an important book, both because of the quality of the contributions, and also because it marks a major change in direction of work on human memory in North America. In the late 19609, a good deal of American work was still concerned with the serial and pairedassociate learning of lists of unrelated words, interpreted in terms of associative interference theory. At the same time there was a growing interest in the role of organization in memory, an approach owing something to both Gestalt psychology and to the more recent developments in the information processing approach. The late 1960s saw a gradual shift towards the position represented in the present volume. In it we find associationists such as Postman, Voss and Wood concerning themselves with problems of organization which up to very recently had been the exclusive preserve of the more cognitive approaches. At the same time the volume reflects rapid growth of interest in semantic memory, the storage and retrieval of information about words, concepts and general knowledge of the world. The book is divided into three sections, the first of these is concerned with laboratory research on remembering verbal materials. I t contains chapters by Bower, Mandler, Postman, Voss and Wood, on such questions as the nature of organization in free recall, the encoding and retrieval processes of memory, organization and recognition memory, and the question of whether experiments showing the role of organization in human memory can be explained in terms of current associationist theories. The second part which contains papers by Collins and Quillian, Greeno, Kintsch, and by Rumelheart, Lindsay and Norman is concerned with semantic memory. I t is more theoretically oriented than the earlier section, and in a number of cases is primarily concerned to produce a model of the semantic memory system. The final section comprises a single paper by Tulving in which he suggests that a distinction be drawn between what he terms episodic memory (e.g. the memory of meeting a sea captain on your holiday last year), and semantic memory (e.g. remembering the chemical formula for salt). This is a stimulating paper and although the reader may fmd Tulving’s argument in favour of two distinct memory systems less than convincing, the case for at least considering such a distinction is made very cogently. This is an important and timely book and should certainly by read by anyone interested in human memory. A. D. BADDELEY"
9b2cec3acc51a72de35b38a14fd4dbe586193286,"Recognition memory for sub-span digit sequences was investigated using Stern-berg's varied-set RT technique. Two experiments studied memory for sequences containing repetitions (e.g. 9 1 9 3) and observed faster recognition of repeated items. Experiment I also showed serial position effects with faster responding to more recent items. Neither of these effects is predicted by Sternberg's highspeed exhaustive scanning hypothesis. Several alternative hypotheses are considered, including two models based on the concept of trace strength, which appear to merit further investigation."
d8eca388ffba8440690f8dee112d0396e4fe65d9,
581cd08733c826877528526ccf057842c4a47999,"Evidence on human performance in dangerous environments is reviewed and suggests that danger reduces efficiency, except in the case of experienced subjects. Perceptual narrowing is shown to be one source of decrement. It is suggested that danger increases the subject's arousal level which influences performance by producing a narrowing of attention. The nature of the performance decrement and of adaptation to danger are discussed in this context."
801a41bb41a871c9e2dae900cb6c2f83e9fa1eb5,
82100c52d56ae7c4df9fd255b5c1df0d854afb4c,"KINTSCH, W. New York: Wiley and Sons. 1970. Pp. 498. A4.40. There has long been a need for an undergraduate textbook covering the area of human cognition and learning. Since Kintsch’s book goes some way towards filling this gap it should be welcomed despite the shortcomings that must inevitably affect any attempt to survey such a broad and rapidly changing area. The book begins with an excellent survey of the techniques and theoretical issues of classical verbal learning. This is followed by a chapter on the stimulus sampling theory of Estes, two good chapters on memory, chapters on discrimination learning, concept identification and a final chapter on rule learning and language. The material is well-organized and the writing clear, although the book does not make particularly exciting reading. This is no doubt partly because the theoretical approaches which clearly appealed to Kintsch at the time of writing are those based on Estes’ stimulus sampling theory, and Chomsky’s transformational grammar. Stimulus sampling theory has subsequently proved difficult to apply to any but the almost trivially simplified tasks on which it was initially based and has consequently lost most of its theoretical appeal. Its prominent place in Kintsch’s text is particularly unfortunate since it is an approach which all but the most mathematically oriented undergraduate is likely to find rather tedious. This is not the case with Kintsch‘s extensive coverage of the attempt to apply transformational grammar to memory tasks. Unfortunately, however, since the book was written, it has become clear that most if not all the effects on memory previously attributed to syntax are more readily expained in terms of uncontrolled semantic factors that cannot be adequately handled by Chomsky’s transformational grammar. However, although the theoretical ideas underlying much of the book have already been bypassed, it remains a good bread-and-butter text, full of clearly presented information across a wide range of subjects. As such it provides a useful, and by current standards, reasonably-priced background text for courses on human memory and cognition. A. D. BADDELEY Learning, Memory, and Conceptual Processes."
f0e8a5ccaed813c036be04ea9dc9284d82ad4d10,
f4160e238fa9180009efd3525d4815193ca75a97,"Abstract : The report presents findings of the research efforts for 1971 in the study of underwater work performance and work tolerance conducted at the University of California, Los Angeles. The studies were directed towards the development of performance decrement curves related to the specific variables which affect underwater work. Experiments designed to add to the body of knowledge necessary to the formation of decrement curves were conducted. The experiments examined: (a) the effect of cold-water exposure upon memory, reasoning ability, and vigilance, (b) the effect of depth upon memory, (c) wet vs. dry training for a specific underwater task, and (d) the physiological and performance effects of heliox as a breathing gas in cold water."
1827711bcba750f75896e0cafc016a6e5f802c4e,"It has been claimed that the short-term forgetting shown by the Peterson technique is entirely due to proactive interference from prior experimental items. Two experiments investigated this by studying forgetting when prior items were avoided by testing subjects only once. Both experiments showed significant forgetting, although the degree of forgetting was less than with a multitrial procedure. On the basis of this and other results it is suggested that the Peterson technique comprises two components, a primary memory component which decays within 6 sec, and a more stable secondary memory component. Forgetting with the multitrial procedure is attributed principally to the need to use temporal retrieval cues to avoid confusion between successive items; longer retention intervals are associated with reduced temporal discriminability and hence poorer recall."
31425c65db5c99b9f3f955cdc1b524d1b674d02b,
3af741e8e68f745e1cdfc31d1aee0627c3e108a3,
6013c07a4d3d272500a13f6f71504207c324927c,An experiment is reported which investigated the use of semantic relatedness as a retrieval cue in the primary memory component in a free recall task. Six-word semantically related clusters were placed in the middle and end positions of free recall lists. Retention was measured immediately after the list presentation and after a filled retention interval of 15 sec. Pure primary memory functions were calculated. The results indicated that semantic cues are useful in retrieval from secondary memory but played no part in recall from primary memory.
708471bef35e87c85d2dbdadc7261776171848e1,
81ba9a443892ba7764a21005969fd724456c5b93,
9ee4093dac1b958710aeac2983c4b0446c222a2b,
bf35a3cbccef1144af7802451e84cdbf378cd09d,"POSTMAN, L. AND KEPPEL, G. (Eds.). New York and London: Academic Press. 1970. Pp. vi+ 467. k1z.00. This book comprises a collection of nine sets of association norms, most of which were not previously available, except as technical reports. The first four sets include the Russell-Jenkins (1952) norms, together with responses to the RussellJenkins stimuli of British, Australian, German and French students, and French workmen. Rosenzweig discusses some of the cultural differences, but apart from a stronger tendency for Englishlanguage speakers to give the dominant associate these differences do not appear to be either very large, or obviously meaningful. The Russell-Jenkins norms were replaced by the more extensive Palermo-Jenkins norms in 1964. Those have already been published and are hence not included. However, Keppel and Strand give norms for associations to the PalermoJenkins associations, which allows an estimate of which associations are bidirectional, and also enables three-step associations to be produced. Postman reports the California norms for two-syllable nouns from four ranges of Thorndike-Lorge frequency. Marshall and Cofer report associations to the dominant responses from 21 of the Connecticut category norms, and Cramer lists the associations to IOO homographs, words with identical spelling but two meanings (e.g. pupil: student, or part of the eye). These are useful in giving an indication of the relative probability of the two meanings. Ervin-Tripp tries to bridge the gap between word associations and connected speech by following her free association test with a request to construct sentences containing the stimulus word, and to produce words that could be substituted for the stimulus word. Each set of norms is accompanied by a description of the procedure for collection and usually by a discussion of the results. Word association norms clearly reflect strong verbal habits, which have a powerful effect in many verbal learning tasks, and it is easy to see why a simple response measure strongly suggesting the Hullian concept of habit strength should have seemed so promising to Russell and Jenkins in 1952. It is much more difficult in 1971 to share Jenkins’ enthusiasm and belief that “the free association technique may well be instrumental in unlocking the secrets of verbal behaviour and, indeed, perhaps of language itself” (p, 7). It is hard to believe that a measure that lumps together such diverse responses as synonyms, antonyms, superordinates and subordinates can ever be a sharp enough tool to attack the thorny problems of semantics. In the case of connected language, the value of free association techniques seems even less; as Ervin-Tripp points out, “associative strength in fact is a very poor device for prediction of sequences in discourse” (p. 394). However, it is sometimes useful to take advantage of the verbal habits reflected by association norms, regardless of one’s theoretical predelictions, and for that reason it is useful to have published norms. At LIZ a copy, this book is not likely to head the list of best sellers, but it is probably sufficiently useful to be bought by most laboratories working in the area of verbal learning and memory. A. D. BADDELEY Norms of Word Association."
d8da18e7e8bdccdc2b08ce0ad276f9e4767bb690,
0465a6f4823359f43c776bf931942fc90a95883f,
8597de4989d2106429aec95de79adeecfad98264,"The coding system used in short-term paired-associate learning was investigated by studying the effects of acoustic and semantic similarity. Performance was affected by acoustic similarity (Expts. IV, V and VI), while semantic similarity had no reliable effect (Expts. II and III). Serial position curves suggested that the primary and secondary memory components of the task were equally affected by acoustic similarity. The implications of this for the relationship between acoustic similarity and short-term memory are discussed."
a469541e720de72558575a684c6af0ceb48d5e56,
b24069d055f26c7c5d5be1618809676875228cc6,"Subjects performed a memory task on two occasions, one in the morning and the other in the afternoon. The task comprised two components, one involved immediate recall of sequences of nine digits, the other involved the repeated item technique devised by Hebb (1961), in which one nine-digit sequence is surreptitiously repeated, each repetition being separated by two non-repeated sequences. Performance on the immediate memory task was better in the morning than the afternoon. The repeated item was recalled more accurately than non-repeated items, but this effect was not influenced by time of day. An explanation in terms of the relationship between arousal and memory reported by Kleinsmith and Kaplan (1963) is suggested."
e7048722591ea884ffe49f5004cb3d216284bbb4,
6c26d3163f5605809f60c566692739694e4f48c8,"Performance on a 40 min. visual task was studied as a function of signal probability. A separate group was tested at each of five levels of signal probability (0·02, 0·06, 0·18, 0·24 and 0·36). Percent detections increased with signal probability. This increase was accompanied by a rise in the false report rate, and analysis in terms of signal detection theory suggested that signal probability affects the subject's criterion (β) rather than his ability to discriminate (d′)."
72d7d68949e8a631a61b62e4224609f503ed1db0,"Murdock has shown that the immediate free recall of a list of words is adversely affected by a concurrent card-sorting task, with degree of impairment increasing with the number of sorting alternatives. He interprets this in terms of a limited capacity STM mechanism. The present study repeats Murdock's study with the addition of an STM control, a condition in which recall was delayed for 30 sec. by a rehearsal-preventing task. Card-sorting load influenced the long-term component (measured by delayed recall), but not the short-term component (measured by subtracting delayed from immediate recall). It is suggested that the limited capacity system affects input into LTM rather than STM."
3341defdfd0c1df0ae61109b1b251ec0b97ce693,"Participants in contingent valuation surveys and jurors setting punitive damages in civil trials provide answers denominated in dollars. These answers are better understood as expressions of attitudes than as indications of economic preference. Wellestablished characteristics of attitudes and of the core process of affective valuation explain several robust features of dollar responses: high correlations with other measures of attractiveness or aversiveness, insensitivity to scope, preference reversals, and the high variability of dollar responses relative to other measures of attitude."
43fb644216b1942ca38662111dbcbace58ffac5d,"The belief that high income is associated with good mood is widespread but mostly illusory. People with above-average income are relatively satisfied with their lives but are barely happier than others in moment-to-moment experience, tend to be more tense, and do not spend more time in particularly enjoyable activities. Moreover, the effect of income on life satisfaction seems to be transient. We argue that people exaggerate the contribution of income to happiness because they focus, in part, on conventional achievements when evaluating their life or the lives of others."
55dab1124e341e73add60a7a05020dd8037f75d0,"This essay discusses the field of behavioral economics, with a focus on the papers in Advances in Behavioral Economics. These papers show that there is a body of “behavioral facts” that is both economically significant and regular enough to be modeled. For the field to advance further, it should devote more attention to the foundations of its models, and develop unified explanations for a wider range of phenomena."
985f4e43b40c90a238ffc084ff18d7fd111e6f35,
b6f1ba9b7537aec523542758472e9c078d39823a,"Subjects tried to recall the location of a tactile stimulus on the underside of the forearm after delays of 0, 3, 5, 10, 15, 30, 45 and 60 sec. When “rehearsal” was prevented by requiring subjects to count backwards during the delay, accuracy of recall decreased systematically reaching an asymptote after 45 sec. When subjects were left free to “rehearse,” this did not affect the decline in accuracy over the first 10 sec. Between 10 and 15 sec. there was a significant increase in accuracy followed by a slow decline which had not reached asymptote by 60 sec. It is suggested that tactile STM (short-term memory) depends on two processes, a fading sensory trace which is unaffected by distraction and a less labile system which does not appear to be verbal but which depends on “rehearsal.”"
37e72109fc33c5fe2b6ecb8691effce777a98bd7,"Abstract Eighteen divers were tested four times under water, twice at a depth of 5 ft and twice at 100 ft. They performed three tests—digit copying, a sentence comprehension test and a manual dexterity test. All three showed a significant drop in efficiency at depth. This was small for digit copying (7-9 per cent) and manual dexterity (3 5 per cent), and somewhat larger for sentence comprehension (15-3 per cent). In all three cases the drop in efficiency was approximately the same as found at the equivalent pressure in a dry pressure chamber. This contrasts with previous results where impairment in the open sea has been considerably greater than in a dry chamber. Possible reasons for this discrepancy are discussed and it is suggested that level of anxiety may be a crucial factor."
9ab9a74d1e5bcd62ecf5a4ec0fad1a7f36259e6c,
a3384f4f7f3f80f4ba95c5cc7e099dedc0414d1b,"This study attempts to discover why items which are similar in sound are hard to recall in a short-term memory situation. The input, storage, and retrieval stages of the memory system are examined separately. Experiments I, II and III use a modification of the Peterson and Peterson technique to plot short-term forgetting curves for sequences of acoustically similar and control words. If acoustically similar sequences are stored less efficiently, they should be forgotten more rapidly. All three experiments show a parallel rate of forgetting for acoustically similar and control sequences, suggesting that the acoustic similarity effect does not occur during storage. Two input hypotheses are then examined, one involving a simple sensory trace, the other an overloading of a system which must both discriminate and memorize at the same time. Both predict that short-term memory for spoken word sequences should deteriorate when the level of background noise is increased. Subjects performed both a listening test and a memory test in which they attempted to recall sequences of five words. Noise impaired performance on the listening test but had no significant effect on retention, thus supporting neither of the input hypotheses. The final experiments studied two retrieval hypotheses. The first of these, Wickelgren's phonemic-associative hypotheses attributes the acoustic similarity effect to inter-item associations. It predicts that, when sequences comprising a mixture of similar and dissimilar items are recalled, errors should follow acoustically similar items. The second hypothesis attributes the effect to the overloading of retrieval cues which consequently do not discriminate adequately among available responses. It predicts maximum error rate on, not following, similar items. Two experiments were performed, one involving recall of visually presented letter sequences, the other of auditorily presented word sequences. Both showed a marked tendency for errors to coincide with acoustically similar items, as the second hypothesis would predict. It is suggested that the acoustic similarity effect occurs at retrieval and is due to the overloading of retrieval cues."
c77a512e84cb841434d9354bb17668ad7ed80236,
da9fe1495c6e671b00e8dc9e435e7f604af1045b,"Several attempts to test the Gestalt hypothesis of autonomous change in the memory trace have shown trends which, though mutually consistent, do not fit the Gestalt hypothesis. Expt I aimed to reproduce these effects. Two figures were used (circles with gaps of 15° or 60°), one retention interval (15 sec) and two methods of testing: reproduction, in which the subject first copies the figure and then draws it from memory; and recognition, using the method of identical stimuli. In the recognition test, subjects tended to judge an identical 60° gap as ‘larger’ after 15 sec, implying closure of the gap in the memory trace, but the 15° gap showed no effect. No reliable changes in gap size occurred using the method of reproduction. Expt II tried to increase the sensitivity of this method by stressing the importance of reproducing the gap accurately. Again results were negative. Expt III tested the hypothesis that the recognition effect found was due to guessing habits. The initial stimulus was presented as before, but the recognition stimulus was replaced by the momentary illumination of a blank card purporting to be a brief presentation of the recognition figure. Subjects were thus forced to guess. The pattern of responses was almost identical with that found in the previous recognition study. Conclusions are: (1) the consistent trends previously found using the method of identical stimuli are probably due to guessing habits; (2) there is no evidence for autonomous change in the short-term retention of form."
f44e94190c4b46892328550358adad1720dbb02c,
2cf0ed302196dc3471e7ad88abafc9d8a5c4f277,"Eight divers performed an addition test and a screwplate test of manual dexterity in the open sea under four conditions—-breathing either air or an oxy-helium mixture and working at a depth of either 10 or 200 ft. Speed of addition was impaired at depth for both air (19.9 per cent) and helium (14.8 per cent), while errors increased only on air (from 5.9 to 21.1 per cent). The manual dexterity test also showed a decrement in speed for both air (46.7 per cent) and helium (31.8 per cent), and air divers lost more screws at depth (11.1 per cent) than at 10 ft (4.7 per cent). While a decrement at depth was expected in the air dives, the considerable impairment shown on oxy-helium dives was not. A further experiment was therefore run in a dry pressure chamber to study the effects of breathing oxy-helium at pressure when the additional stresses associated with deep diving in the open sea were absent. At a pressure equivalent to 200 ft of water, there was a 10 per cent impairment in speed on both the screwplate (..."
865fd3f06d950b96346cb9b560baabdf045f45a3,"This is a slightly longer version of an entry prepared for the 2nd edition of The New Palgrave Dictionary of Economics, edited by Steven Durlauf and Lawrence Blume (Palgrave-Macmillan, forthcoming). Since the New Palgrave does not include acknowledgments, I should use this chance to thank Roger Backhouse, Philippe Fontaine, Daniel Kahneman, Kyu Sang Lee, Ivan Moscati, and Vernon Smith for their help and suggestions in preparing this paper."
49e1277cec52f638441278781198816f5c3d929f,
4c4556b0e9894877b8f9378e2075fcf1fa70b035,
52242a58d9879a80123f1a7e5c2df64aa4fc70ad,
6eab09abaa551d51b1cc2d1546ba3c7dd85d937c,"Response selection was studied independently of the stimulus by asking subjects to generate random sequences of letters or numbers. Experiment 1 varied rate of letter generation from ½ sec. to 4 sec. per item and showed that the redundancy of the sequence increased linearly with rate. Experiment 2 added random generation of letters as a secondary task to paced card sorting. Information load per card was varied from 1 through 2 to 4 to 8 alternatives, with sorting rate held constant. As predicted, the redundancy of the sequences generated increased linearly with sorting load. Experiment 3 varied number of items to be randomized. Rate of random generation increased systematically from 2 to 4 to 8 alternatives, but levelled out beyond this point, showing no difference between 16 and 26. In general, these results suggest a response-selection mechanism of limited informational capacity."
966f6f2b98edae1a99922cd308e8ac863ef2e47d,
c89e78f325e64cd6fa41e35dc9cfb03d2cea72ed,"It has been shown that short-term memory (STM) for word sequences is grossly impaired when acoustically similar words are used, but is relatively unaffected by semantic similarity. This study tests the hypothesis that long-term memory (LTM) will be similarly affected. In Experiment I subjects attempted to learn one of four lists of 10 words. The lists comprised either acoustically or semantically similar words (A and C) or control words of equal frequency (B and D). Lists were learned for four trials, after which subjects spent 20 min. on a task involving immediate memory for digits. They were then asked to recall the word list. The acoustically similar list was learned relatively slowly, but unlike the other three lists showed no forgetting. Experiment II showed that this latter paradox can be explained by assuming the learning score to depend on both LTM and STM, whereas the subsequent retest depends only on LTM. Experiment III repeats Experiment I but attempts to minimize the effects of STM during learning by interposing a task to prevent rehearsal between the presentation and testing of the word sequences. Unlike STM, LTM proved to be impaired by semantic similarity but not by acoustic similarity. It is concluded that STM and LTM employ different coding systems."
e0b60ca9340d28c22155bbfc00fd090633683eb9,"The way subjects remember a list of two-digit numbers has been examined in some detail. It is found that intrusions in free recall are not random. They resemble omissions in having the same first digit but not in other ways. This non-randomness of recall errors has been used to construct recognition tests of varying difficulty. Numbers which occurred commonly as intrusions were difficult to distinguish from the correct items when used as distractors in recognition tests. The experiments suggest that the previously observed relationship between recognition efficiency and number of alternatives (Davis, Sutherland and Judd, 1961) can be attributed to the increased probability that such intrusions will be included when the total number of distractors is increased."
e2c1a780fe4a9e4d9258ba8cf8b07067b90bdf1d,"Experiment I studied short-term memory (STM) for auditorily presented five word sequences as a function of acoustic and semantic similarity. There was a large adverse effect of acoustic similarity on STM (72·5 per cent.) which was significantly greater (p < 0·001) than the small (6·3 per cent.) but reliable effect (p < 0·05) of semantic similarity. Experiment II compared STM for sequences of words which had a similar letter structure (formal similarity) but were pronounced differently, with acoustically similar but formally dissimilar words and with control sequences. There was a significant effect of acoustic but not of formal similarity. Experiment III replicated the acoustic similarity effect found in Experiment I using visual instead of auditory presentation. Again a large and significant effect of acoustic similarity was shown."
c8469a83931a75b51f7724229c6a2f86fd947b27,"Two measures which have been shown to predict the ease of learning trigrams, namely log letter frequency and sequential predictability, were applied to data from an experiment on short term memory. This involved the immediate recall of 120 six-letter consonant sequences which were presented visually one letter at a time. A significant correlation was found between the probability that a given sequence would be recalled correctly and both its mean log letter frequency (r = 0.308, p < 0.001), and its mean predictability (r = 0.393, p < 0.001). Partial correlation showed only a marginally significant effect of log letter frequency when predictability was partialled out (r = 0.161, 0.05 < p < 0.1). With log letter frequency partialled out, however, a reliable correlation between predictability and recall score remained (r = 0.300, p < 0.001)."
17c2df2480c666010f477f1d2d3999d0fd6aa768,
32e4c448d2a9b4e507e9c25bfaf33d115dc4574a,"It is suggested that the relationship observed by Miller, Bruner and Postman (1954) between the redundancy and the accuracy of reproduction of tachistoscopically presented letter sequences is not a perceptual effect as they suggest, but is due to the informational limitations of immediate memory. An experiment is performed which shows an exactly similar relationship between redundancy and number of letters correctly reproduced when exposure time is long enough for the subject to read out, and hence perceive, all the material with complete accuracy. It is concluded that the more redundant the letter sequence and the longer the exposure time, the more effectively the sequence can be coded and the better it will be recalled."
839e378658f580e85c89d2685938f9a8adb38112,
b8b643a5e89df598da6e38ff634efc6fb842308c,
d5939961d49639887919472a32537153dc932401,
57799ae7d9d61011acd5de9fa52c8a1ab89bd2b2,
7ca9e97d9edbbc2e1d507815769e8011a6138fdd,"Subjects were asked to solve a series of 12 anagram problems. For each of these they were allowed 1 min. and if they did not solve it in this time they were told the solution. When asked to recall the solution words at the end of the series, subjects remembered items they had failed to complete almost twice as often as those they had solved. It is suggested that this phenomenon is analogous to the Zeigarnik effect, but that it has the advantage of occurring in conditions which are easy to specify and control."
4c1890ace43be14a2f1c23175a1eb99e9a5844bd,
a703b255751f56ab84e8ab3db1814a6bfc044d18,
43902bc4c96627123ff25c8c40221e0df0830c46,
57efbad63ece718d17448ed54c0bfe269a085f93,"It has recently been suggested that while the concept of secondary reinforcement is very frequently used, direct supporting evidence is surprisingly sparse and equivocal.l Much of the relevant evidence is concerned with the observation that a response which is followed by a secondary reinforcing stimulus extinguishes more slowly than a response with no such reinforcement, but this phenomenon can be explained at least as effectively in terms of discrimination.2 Direct evidence of the effect of secondary reinforcement on acquisition is less common, and such effects as have been demonstrated are rarely as marked as the theory would predict.3 Once again the results are open to an interpretation in terms of discrimination. The present study aims to examine a situation in which the sight of food would be expected to strengthen the wrong R if it acts as a simple secondary reinforcement, while aiding the correct R if it acts as a cue to the location of rewzd."
907f1b2841bb42b3e76c371d2a9fcd72ccc3fd60,
1103ef53a84d4a3fc9e2d3ecca9e4837464f89bf,
34b89915a04e023c906ab94a7b82e84419e23df9,"For several decades the issue of personality effects on second language acquisition has been high on the agenda of many second language acquisition researchers (cf. Dewaele, 2009). Nevertheless, personality and other non-intellectual characteristics are considered to be weakly correlated with cognitive abilities and, as a result, they are not likely to explain variance in the outcomes of learning a foreign language (cf. Robinson & Ellis, 2008). On the other hand, some researchers being aware of the potential of personality factors in the development of foreign language aptitude call for research on this neglected field (cf. Dornyei, 2009, 2010; Hu & Reiterer, 2009; Hyltenstam & Abrahamsson, 2003; Moyer, 2007). The purpose of the study reported here was to analyze personality factors defined according to the “Five Factor Model” (McCrae & Costa, 2003) in accomplished multilinguals. The factors included: Openness to experience, Conscientiousness, Extraversion, Agreeableness and Neuroticism. An instrument used in the study was The Revised NEO-FFI Personality Inventory (Costa & McCrae, 1992) - a Polish adaptation by Zawadzki et al. (1998). The results of 44 accomplished multilinguals were compared to the results of 37 mainstream first-year English philology students. The analysis revealed that the factor of Openness to experience was significantly higher in the accomplished multilinguals than in the mainstream L2 learners. The other factors, that is Neuroticism, Agreeableness, Extraversion and Conscientiousness did not reveal statistically significant differences between the samples. Openness is a factor that is relatively stable and the most genetically determined of all the Five Factors. It includes a cognitive aspect, which means that people who score high on general cognitive ability tend to display openness to new experiences and intellectual curiosity and flexibility (Corno et al., 2002). A suggestion that Openness to experience is a good predictor of the outcomes of learning a foreign language is discussed."
5ca93be1538278acc0ba91d72690c87dc3300506,
b86788da58f3a68a65cc547f57c8326c68c22ba8,"The role of short-term memory and working memory in accomplished multilinguals was investigated. Twenty-eight accomplished multilinguals were compared to 36 mainstream philology students. The following instruments were used in the study: three memory subtests of the Wechsler Intelligence Scale (Digit Span, Digit-Symbol Coding, and Arithmetic, which constitute a memory and resistance to distraction index); two short-term memory tests of the Modern Language Aptitude Test (Part I [Number Learning] and Part V [Paired Associates]); and the verbal Intelligence Quotient (IQ), the nonverbal IQ, the general IQ, and a working memory test, the Polish Reading Span (PRSPAN). The results of the accomplished multilinguals were compared to the results of 1st-year English philology students (mainstream). The analysis revealed that short-term memory and working memory abilities in the accomplished multilinguals were higher than in the mainstream philology students. Results might contribute to the understanding of the controversial role of working memory and short-term memory abilities in accomplished multilinguals. A suggestion that the two components of working memory (the phonological loop and the central executive) are significant factors in determining the outcome of learning a foreign language is discussed. [ABSTRACT FROM AUTHOR]"
2c62162117826154f8458468389ee589648fec3e,
49d74ad0cda69b68fc0efdad7e1d6008c81ec82c,"The study addresses a problem which is inadequately investigated in second language acquisition research, that is, personality predictors of foreign language aptitude. Specifically, it focuses on the Five Factor model which includes Openness to Experience, Conscientiousness, Extraversion, Agreeableness and Neuroticism (Costa & McCrae, 1992) as traits differentiating gifted and nongifted foreign language learners and predicting results of foreign language aptitude tests. Although contemporary researchers generally agree that affect is an important variable in second language acquisition, most empirical studies demonstrate that personality factors are weakly correlated with cognitive abilities and that their contribution to the ultimate attainment is minor (cf. Robinson & Ellis, 2008). On the other hand, these factors constitute an integral part of cognitive ability development (cf. Dornyei, 2009); therefore, neglecting them in research on foreign language aptitude would be unjustified. The following study is an attempt to analyze the Five Factors in two groups of learners: gifted and nongifted. In order to answer the question as to which and to what extent personality factors have a predictive effect on foreign language aptitude, the results were subjected to a multiple regression analysis. The findings of the study are presented and discussed in a wider context of research on cognitive abilities."
4ca78567693818664e0570418782c3b48a02c28c,
5fe42e986e04475077343ea92771c360262ea82f,"The Cognitive Profile of a Talented Foreign Language Learner. A Case Study The article examines a variety of cognitive individual variables of a talented foreign language learner. The research complies with the qualitative and quantitative criteria of choice of a talented learner proposed by Arancibia et al., (2008); Hartas et al., (2008); Hewston et al., (2005), and Skehan, (1998). Cognitive variables included tested: foreign language aptitude, verbal and non-verbal intelligence, learning styles, and learning strategies. The purpose of the research was to construct an extended cognitive profile of a 21-year-old student proficient in three languages. The research revealed that the subject is linguistically talented, especially in the area of phonological, analytical, and memory abilities. It is hypothesized that her superior abilities result from an extraordinarily efficient short-term phonological memory (Mackey et al., 2002). She uses miscellaneous learning strategies and her learning styles are versatile. The final conclusion is that research into linguistic talent is scarce, therefore further investigation, especially in the field of working memory of talented foreign language learners, is required."
847712ddb16441ca4cfd525a7172d8fbd15b361f,"Abstract Separation and extraction conditions for four atypical neuroleptics (clozapine, N-desmethylclozapine (active metabolite of clozapine), olanzapine, and quetiapine) and one classical neuroleptic (perazine) in human plasma have been studied. Reverse phase chromatography (RP-18) with the mobile phase consisted of: A. aq. orthophosphoric acid with addition of N,N,N′,N′-tetramethylethylendiamine (TEMED), pH = 5.0, and B. acetonitrile, in gradient flow, was selected. As the optimal liquid-liquid extraction conditions, sample pH = 11.6, extraction system, ethyl acetate/n-hexane/isopropanol (16:3:1, v/v/v), and back extraction into 0.01% orthophosphoric acid were chosen."
8b4cd00627bbd2166d21c8d8593e4288f69bf0a7,"The purpose of this paper is to utilize statistical methodologies to infer from market prices of assets and their derivatives the magnitude of the set of a measureM that defines acceptance sets of risky future cash flows. We assume thatM contains the collection of bilateral gamma random variables, and estimate upper and lower boundaries of the compensation needed for a given bilateral gamma distributed future cash flow to be acceptable. We show that prospects theory provides a natural interpretation of the behaviors implied by such boundaries, which are not compatible with expected utility theory. Boundaries for bilateral gamma risk neutral scale parameters for given speed parameters are also estimated and tested against market data and, in particular, comparisons are made with known empirical facts about the magnitude of the acceptance set of a common class of risk measures."
44225fe25fafb77d51e2c9bb9e3030f34f715590,"A key but neglected issue in the search for collective intelligence principles is the role of noise. Does noise inhibit collective intelligence or can it amplify the discovery of intelligent solutions? In this exchange of letters, the authors explore the pros and cons of noise."
e69e1a857ccd067e579d4fc76153dcdcef236892,"Human behavior is a diverse, complex, and highly interesting phenomenon. Despite the many differences that exist between any two people, we can find patterns that characterize typical behaviors in various situations, such as in a classroom or at a family dinner. The research field called behavioral economics studies human behavior in financial situations. In this article, I will present the main findings of the prospect theory, which I developed together with the late Amos Tversky. Prospect theory explains human choices in situations that involve gambling, and it answers questions such as whether people consider gains and losses equally and how a person’s initial financial situation influences the value they give to gains and losses. At the end of the article, I will share important insights from my scientific career and explain why happiness has two faces."
68a82d73132d51376298da2597011d8294801ae4,"Social PsychologyThe Psychology of Criminal ConductDeterrence NowNational Security DilemmasUnderstanding Psychology And CrimePsychology and DeterrenceShifting Power in Asia-Pacific?Advances in Psychology and LawThe Reasoning CriminalHerbert C. Kelman: A Pioneer in the Social Psychology of Conflict Analysis and ResolutionCrisis Bargaining and the StateArtificial Intelligence And International PoliticsThe Oxford Companion to American Military HistoryUnderstanding DeterrenceTheories of Political ProcessesBalancing RisksRichard Ned Lebow: Key Texts in Political Psychology and International Relations TheoryEscalation and Negotiation in International ConflictsRichard Ned Lebow: A Pioneer in International Relations Theory, History, Political Philosophy and PsychologySystem EffectsThe Logic of Images in International RelationsDeterrence, Choice, and Crime, Volume 23Interdisciplinary Perspectives on TrustThe Oxford Handbook of Political PsychologyPeace PsychologyNorth Korea through the Looking GlassDemocratic Vulnerability and Autocratic MeddlingThe Oxford Handbook of Law and PoliticsPerspectives on DeterrenceDeterrence in American Foreign Policy: Theory and PracticePsychology And Social PolicyThe Psychology of Nonviolence and AggressionAsia, the US and Extended Nuclear DeterrenceNL ARMS Netherlands Annual Review of Military Studies 2020The Power of DeterrenceCauses of WarHow Statesmen ThinkForensic PsychologyPsychology and DeterrencePerspectives on Deterrence"
6989dd02ee523d7afd69b5de55448373d7c92357,
8cfcef468fbd8a9b998b699c301e224f3908dc71,
01cec54d4cbcf0ac5f85ae54ff6826bef928f104,
210fdb6e2efa0066643843ff4edefca3a9815b6e,
3f89cd095d95e097a2206e9ce2c1d9ff3785dc76,"Few books are “must reads” for intelligence officers. Fewer still are “must reads” that mention Intelligence Community functions or the CIA only once, and then only in passing. Daniel Kahneman has written one of these rare books. Thinking, Fast and Slow represents an elegant summation of a lifetime of research in which Kahneman, Princeton University Professor Emeritus of Psychology and Public Affairs, and his late collaborator, Amos Tversky, changed the way psychologists think about thinking. Kahneman, who won the 2002 Nobel Prize in Economics for his work with Tversky on prospect theory, also highlights the best work of other researchers throughout the book. Thinking, Fast and Slow introduces no revolutionary new material, but it is a masterpiece because of the way Kahneman weaves existing research together."
6aadc42f6d7b8c72ce0ce8ecefc90ba626b24889,"The psychologist Anne Treisman dedicated her career to the study of attention and perception, a central concern of cognitive science. While still a graduate student, she modified and reformulated the leading theory of auditory attention. Her discoveries and insights into the role of visual attention in the perception of objects, to which she devoted her subsequent decades of research, have had a lasting influence, not only in experimental psychology but also in vision research, neuroscience and artificial intelligence. In a period of rising interest in the brain, her foundational theories inspired thousands of experiments in her own field and others, and the originality and precision of her experimental design confirmed the continued relevance of behavioural research to the scientific enterprise. Treisman's accomplishments were recognized by the National Academy of Sciences in the USA in 1994 and by the American Academy of Arts and Sciences in 1995. In 1996, she became the first psychologist to win the Golden Brain Award. She received the University of Louisville Grawemeyer Award in Psychology in 2009, and was awarded the National Medal of Science by President Barack Obama at a White House ceremony in 2013."
a119f6d8c66333d4f74610c7d22b43df3828bf26,Reports in the 1970s of cognitive illusions in judgments of uncertainty had been anticipated by Laplace 150 years earlier. We discuss Miller and Gelman’s remark that Laplace’s anticipation of the main ideas of the heuristics and biases approach “gives us a new perspective on these ideas as more universal and less contingent on particular developments [that came much] later.”
a5c43ac870ed99b5a2e11370c3c6201d36586c11,
18d9fcf8d08c7f1821983364b5fd8d248e3bb5f5,
9a96aca7f1de26eeef1377d2dec94643a52a30cc,"System 1 uses mental shortcuts, known as heuristics, which are e ective and e icient most of the time. However, they also can lead to systematic and predictable errors, known as cognitive biases. Examinations of these heuristics and biases paved the way for the field of behavioural economics, where psychology research has been used to revise understandings of economic behaviour. Behavioural science aims to understand human behaviour and decision-making. It encompasses disciplines examining the psychological underpinnings of behaviour, such as cognition, neuroscience and social psychology, and how they intersect with fields involving behaviour, like economics, politics, and communication."
fdaa4e791f6a048dcc3b501db7350de8aa95c97f,"It is incredibly difficult to imagine an event the likes of which humans have never seen before. That, in and of itself, renders the challenge to prepare for such an event even more difficult because there is no frame of reference pushing us to act. How do you prepare to avoid something which has never occurred in the history of human occupation? That is the challenge of climate change. I argue that the Subprime Mortgage Crisis and its aftermath parallel the Climate Crisis in critical ways that should inform our tactics. Of course, there are obvious critical differences as well. The Subprime Crisis was a predictive failure that involved the misallocation of risk and blindness to uncertainty. This Article examines the predictive failures of the Subprime Crisis by focusing on what makes probabilities more likely to be accurate and the circumstances in which some predictions blind us to the uncertainty of large-scale negative consequences. This Article employs the theory of the Black Swan and other critiques from Nassim Nicolas Taleb to explore the application of probability theory in the context of the Climate Crisis. At the same time, data and probabilities are insufficient to motivate both individuals and political entities to act. Even an accurate probabilistic assessment of global climate change risk is inadequate; the Climate Crisis demands a narrative that resonates with individuals at a local and emotional level. Narrative theory explains the difficulty * © 2019 M. Alexander Pearl. Associate Professor of Law and Director of the Center for Water Law and Policy, Texas Tech University School of Law. Enrolled citizen of the Chickasaw Nation of Oklahoma. Presented at the University of Colorado Climate Change Colloquium. A version of this paper was presented to the Texas Tech University Climate Science Center. My thanks to Douglas Kysar, Sarah Krakoff, Jedidiah Purdy, William Boyd, Ann Carlson, Brigham Daniels, Noah Sachs, Jim Salzman, Tracy Pearl, Dr. Katharine Hayhoe, Jamie Baker, Alyson Drake, and Kathryn Almond. All mistakes are my own. 1 Amos Tversky & Daniel Kahneman, Belief in the Law of Small Numbers, 76 PSYCHOL. BULL. 105, 110 (1971). 2 ROBIN WEST, NARRATIVE, AUTHORITY, AND LAW 419 (1993). 384 UTAH LAW REVIEW [NO. 2 experienced in implementing legal solutions to mitigate the Climate Crisis. This Article synthesizes the link between narrative power in creating human understanding and our propensity for making bad predictions through the human cognitive bias research of Daniel Kahneman and Amos Tversky. While policy-makers, scientists, and political representatives play important roles in trying to shape public opinion, recent empirical research supports the idea that lawyers—through litigation—are best equipped to immediately address the Climate Crisis."
55da81186d3f50ec238cab72c0f95b17391aa524,
58d3fe1bdf97ea55c264735fa62b4bc1cad0b766,
aa980fbcaf70862daa7cbde8b8a32a8fc067d42e,
1133b9c4454ba60ec5365fe0564f0a1eff341c7e,"Presents an obituary for Richard Michael Suzman, who died on April 16, 2015. Suzman was trained as a sociologist and anthropologist, but he was attracted to the approaches of demography and economics. He came to know a great deal about diverse fields of science, including health, physiology, psychology, genetics, and economics. He was a scientific leader who was on a quest to develop new transdisciplinary fields and to mobilize the best scientists to work in them. Suzman's passion for transdisciplinary science was fully expressed in his greatest achievement: the famous Health and Retirement Survey (HRS), which he initiated in 1988 and continued to guide and inspire. (PsycINFO Database Record"
29121694595feaf91e18db073c025e0a6c55567b,"In legal disputes, contested insurance claims, and similarly adversarial negotiations, one party is likely to open with an inflated claim or a lowball offer. And if the other side’s position is unreasonable, it may make little sense to be reasonable yourself. But if everyone routinely came to a dispute with a realistic starting position, the offers would be more or less aligned, and any negotiation that followed would most likely be relatively civil, speedy, and fair. How can a negotiator who wants to be fair from the start ensure that his or her counterpart will be reasonable as well? The authors propose the final-offer arbitration challenge, which leverages an approach first applied in labor negotiations in the 1960s. You can employ this tactic by opening with a demonstrably fair offer and then—if the other party is unreasonable—extending a challenge to take the competing offers to an arbitrator who must choose one or the other rather than a compromise between them (the usual outcome of conventional arbitration). The authors describe how AIG used the approach and how other companies can begin to adopt it. INSETS: A Primer on Final-Offer Arbitration;Saving the Deal"
a4ed11dd4cdbeabcee12c584653c2ef7ba04592f,
2b1490311dfbda596ccf3d91aedcbe1341eee625,
960e2a2f24f9e5c5331064ab1640a785477483de,"Dosne, A.-G. 2016. Improved Methods for Pharmacometric Model-Based Decision-Making in Clinical Drug Development. Digital Comprehensive Summaries of Uppsala Dissertations from the Faculty of Pharmacy 223. 91 pp. Uppsala: Acta Universitatis Upsaliensis. ISBN 978-91-554-9734-7. Pharmacometric model-based analysis using nonlinear mixed-effects models (NLMEM) has to date mainly been applied to learning activities in drug development. However, such analyses can also serve as the primary analysis in confirmatory studies, which is expected to bring higher power than traditional analysis methods, among other advantages. Because of the high expertise in designing and interpreting confirmatory studies with other types of analyses and because of a number of unresolved uncertainties regarding the magnitude of potential gains and risks, pharmacometric analyses are traditionally not used as primary analysis in confirmatory trials. The aim of this thesis was to address current hurdles hampering the use of pharmacometric model-based analysis in confirmatory settings by developing strategies to increase model compliance to distributional assumptions regarding the residual error, to improve the quantification of parameter uncertainty and to enable model prespecification. A dynamic transform-both-sides approach capable of handling skewed and/or heteroscedastic residuals and a t-distribution approach allowing for symmetric heavy tails were developed and proved relevant tools to increase model compliance to distributional assumptions regarding the residual error. A diagnostic capable of assessing the appropriateness of parameter uncertainty distributions was developed, showing that currently used uncertainty methods such as bootstrap have limitations for NLMEM. A method based on sampling importance resampling (SIR) was thus proposed, which could provide parameter uncertainty in many situations where other methods fail such as with small datasets, highly nonlinear models or meta-analysis. SIR was successfully applied to predict the uncertainty in human plasma concentrations for the antibiotic colistin and its prodrug colistin methanesulfonate based on an interspecies whole-body physiologically based pharmacokinetic model. Lastly, strategies based on model-averaging were proposed to enable full model prespecification and proved to be valid alternatives to standard methodologies for studies assessing the QT prolongation potential of a drug and for phase III trials in rheumatoid arthritis. In conclusion, improved methods for handling residual error, parameter uncertainty and model uncertainty in NLMEM were successfully developed. As confirmatory trials are among the most demanding in terms of patient-participation, cost and time in drug development, allowing (some of) these trials to be analyzed with pharmacometric model-based methods will help improve the safety and efficiency of drug development."
bd338242dae55619223dbe8295a3156c8694bdc5,"Organizations expect to see consistency in the decisions of their employees, but humans are unreliable. Judgments can vary a great deal from one individual to the next, even when people are in the same role and supposedly following the same guidelines. And irrelevant factors, such as mood and the weather, can change one person’s decisions from occasion to occasion. This chance variability of decisions is called noise, and it is surprisingly costly to companies, which are usually completely unaware of it. Nobel laureate Daniel Kahneman, a professor of psychology at Princeton, and Andrew M. Rosenfield, Linnea Gandhi, and Tom Blaser of TGG Group explain how organizations can perform a noise audit by having members of a professional unit evaluate a common set of cases. The degree to which their assessments vary provides the measure of noise. If the problem is severe, firms can pursue a number of remedies. The most radical is to replace human judgment with algorithms. Unlike people, algorithms always return the same output for any given input, and research shows that their predictions and decisions are often more accurate than those made by experts. Although algorithms may seem daunting to construct, the authors describe how to build them with input data on a small number of cases and some simple commonsense rules. But if applying formulas is politically or operationally infeasible, companies can still set up procedures and practices that will guide employees to make more-consistent decisions. INSETS: Types of Noise and Bias.;How to Build a Reasoned Rule.. [ABSTRACT FROM AUTHOR]"
f46d67c27e28b8103dfcdf7cd2a4bccf55780055,
0c88774331f593026ebd9c6f5197067042818a31,"In the early 1970s, Daniel Kahneman and Amos Tversky produced a series of pathbreaking papers about decisions under uncertainty.' Their claim was that in assessing probabilities, ""people rely on a limited number of heuristic principles which reduce the complex tasks of assessing probabilities and predicting values to simpler judgmental operations.""2 Kahneman and Tversky did not argue that it is irrational for people to use the relevant heuristics. On the contrary, they claimed that as a general rule, the heuristics are quite valuable. The problem is that in some cases, they lead ""to severe and systematic errors.""3 It is worth emphasizing the word ""systematic."" One of the most striking features of their argument was that the errors were not random-they could be described and even predicted. The resulting arguments have proved influential in many fields, including law,' where the influence stems from the effort to connect legal analysis to a realistic, rather than hypothetical, understanding of"
517fc61cdc026850b2a16f47185db57ffa8113b1,"Because people are irrational in systematic and predictable ways, decision-making processes can be an important driver of investing success if they mitigate biases and enhance objectivity. However, there is little consensus on the best approach to making investment decisions. Many investors rely on informal intuitive decision making guided by unreliable mental narratives, which are automatically generated stories that seemingly explain the immediately available data points. Our perception of evidence is naturally distorted to be artificially coherent with these narratives, so this approach often leads to biased, overconfident choices. At the other end of the spectrum, “black box” quantitative, factor-based investing, driven by statistical inputs and purely algorithmic processes, typically forgoes the benefit of differentiated forward-looking views based on fundamental research and expertise."
5674f55e7c19d87f6952c36377e330b779f5db1a,
95b7bb69698449fae5fa8cbc6d2b016c9d4b2a94,
b22f6bb58ee77a30e823834912d245af61d01870,"Studies of subjective well-being have conventionally relied upon self-report, which directs subjects’ attention to their emotional experiences. This method presumes that attention itself does not influence emotional processes, which could bias sampling. We tested whether attention influences experienced utility (the moment-by-moment experience of pleasure) by using functional magnetic resonance imaging (fMRI) to measure the activity of brain systems thought to represent hedonic value while manipulating attentional load. Subjects received appetitive or aversive solutions orally while alternatively executing a low or high attentional load task. Brain regions associated with hedonic processing, including the ventral striatum, showed a response to both juice and quinine. This response decreased during the high-load task relative to the low-load task. Thus, attentional allocation may influence experienced utility by modulating (either directly or indirectly) the activity of brain mechanisms thought to represent hedonic value."
a7c562014992e71f58713108792eeef878905676,"www.sciencemag.org (this information is current as of October 30, 2009 ): The following resources related to this article are available online at http://www.sciencemag.org/cgi/content/full/312/5782/1908 version of this article at: including high-resolution figures, can be found in the online Updated information and services, found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/312/5782/1908#related-content http://www.sciencemag.org/cgi/content/full/312/5782/1908#otherarticles , 3 of which can be accessed for free: cites 12 articles This article 47 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/312/5782/1908#otherarticles 11 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/cgi/collection/psychology Psychology : subject collections This article appears in the following http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining"
bf64e00285e4ad487d95a6a5287ebab332299ac0,"To date, diurnal rhythms of emotions have been studied with real-time data collection methods mostly in relatively small samples. The Day Reconstruction Method (DRM), a new survey instrument that reconstructs the emotions of a day, is examined as a method for enabling large-scale investigations of rhythms. Diurnal cycles were observed for 12 emotion adjectives in 909 women over a working day. Bimodal patterns with peaks at noon and evenings were detected for positive emotions; peaks in negative emotions were found at mid-morning and mid-afternoon. A V-shaped pattern was found for tired and an inverted U-shaped pattern for competent. Several diurnal patterns from prior studies were replicated. The DRM appears to be a useful tool for the study of emotions."
ca6e6cf796b70822d909b44438662bf9938cb086,
9c2a4af85f4f6a4adea836460af011cb908b0628,
9f5f69b23e9afb386c76270153ddd1ebb84cb1c2,
ccaf0d7780c0028ba0fd1e461d5c2dd826609f5f,"While direct replications such as the “Many Labs” project are extremely valuable in testing the reliability of published findings across laboratories, they reflect the common reliance in psychology on single vignettes or stimuli, which limits the scope of the conclusions that can be reached. New experimental tools and statistical techniques make it easier to routinely sample stimuli, and to appropriately treat them as random factors. We encourage researchers to get into the habit of including multiple versions of the content (e.g., stimuli or vignettes) in their designs, to increase confidence in cross-stimulus generalization and to yield more realistic estimates of effect size. We call on editors to be aware of the challenges inherent in such stimulus sampling, to expect and tolerate unexplained variability in observed effect size between stimuli, and to encourage stimulus sampling instead of the deceptively cleaner picture offered by the current reliance on single stimuli."
4ba2e5bdb916698ffe60d4b2996eb4e2a05c3127,"Based on the assumption of rational choice, economic science analyzes the direct and indirect consequences of market participants’ decisions. Rational choice behavior is described as a decision maker’s effort to choose an action that maximizes the decision maker’s utility or profit in the given circumstances. Seen from another angle, this dominant economic theory has built its complex models on the assumption of market participants’ unbounded rationality. Nonetheless, during the rapid rise of mainstream theoretical concepts, some analysts indicated the need to review and modify these underlying assumptions. In the 1950’s, the great analyst of wide intellectual range and Nobel laureate in economics, Herbert A. Simon, questioned the stated assumption of unbounded rationality (Simon 1955), advocating the application of the concept of bounded rationality. Simply put, Simon’s point of departure for decision makers was to find a satisfactory solution instead of searching for the optimal solution. It was followed by research in various fields ultimately indicating without any doubt it is necessary to be much more careful in using the assumption of perfect rationality. Later research in this area resulted in the creation of a new field, behavioral economics. Behavioral economics deals with the factors that influence the decisions of market participants, based on bounded - not perfect - rationality. One is not to start from the assumed behavior but to explore the actual human behavior. One classic study in the field (Peter Diamond and Hannu Vartiainen 2007), describes behavioral economics as an umbrella of approaches seeking to extend the standard economic framework to account for relevant features of human behavior absent in the standard economic framework. Behavioral economics incorporates research results from other sciences into the science of economics, particularly including findings from cognitive and social psychology (Mathew Rabin 1998; Stefano DellaVigna 2009). In this context, one cannot fail to mention the psychologist Daniel Kahneman (described by some as the greatest living psychologist). In 2002, Kahneman won the Nobel Prize in Economics with U.S. economist Vernon L. Smith (one of the founders of experimental economics), for including the results of psychological research in economic science, according to the Nobel Committee’s explanation, with special emphasis on the analysis of the decision making process of market participants under conditions of uncertainty. The award actually came as the crowning achievement of decades-long research by many. Kahneman has had a long and successful collaboration with a number of"
5cf0ddcb145f6060ad8d89bd71875a044c5a1e71,
5fe08caf7c7c2bc9ab677c71a91d4d184b38ecd2,"What is the origin and nature of consciousness? If consciousness is common to humans and animals alike, what are the defining traits of human consciousness? Moderated by Steve Paulson, executive producer and host of To the Best of Our Knowledge, Nobel laureate psychologist Daniel Kahneman, philosopher David Chalmers, expert in primate cognition Laurie Santos, and physician‐scientist Nicholas Schiff discuss what it means to be conscious and examine the human capacities displayed in cognitive, aesthetic, and ethical behaviors, with a focus on the place and function of the mind within nature. The following is an edited transcript of the discussion that occurred October 10, 2012, 7:00–8:15 PM, at the New York Academy of Sciences in New York City."
9a4cb893cf9403d2698c3116573d5e76bdb4d4b9,"The science of human decision-making has long been a stronghold of psychologists. Among the voices that abound in the literature on how people make choices, one scholarly voice has remained strident through several decades. By studying human behavior through the lens of economics, Princeton University psychologist Daniel Kahneman has shown how inherent biases might influence our choices, overtly and subliminally. For his influential work on prospect theory, which holds that people make decisions based on their perceptions of losses and gains rather than on final outcomes, Kahneman shared the 2002 Nobel Memorial Prize in economics. Working together with late psychologist Amos Tversky, a longtime research partner, Kahneman demonstrated that decisions believed to be the result of deliberation often stem from educated guesses, rules-of-thumb, and pattern-recognition. In his book Thinking Fast and Slow, a critical success published in 2011 and targeting a broad audience, Kahneman reports on a wealth of experimental research that point to two modes of decision-making in the humanmind:Whereas system 1 is automatic, effortless, and rapid, rushing to judgment based on heuristics, system 2 is deliberate, effortful, and slow, plodding through reason before reaching conclusions. Understanding how we toggle between the two systems, Kahneman says, can help us make sense of a broad swath of human behavior. On the occasion of the 2012 Sackler colloquium The Science of Science Communication, where he was the keynote speaker, Kahneman discussed his work with PNAS. PNAS: What motivated you to write a popular book for a general audience? Kahneman: I’ve had a long career and worked in many fields. This book, which is not an autobiographical account, is a review ofmany of thosefields. I wanted towrite a book that would speak not only to the public but also to future scholars. Articles may become dated or less easily accessible, but a scholarly book that appeals to a wide audience might endure. PNAS: In the book you describe two fictitious systems in our mind—systems 1 and 2—that underlie our automatic and considered decisions, respectively. Would an acute awareness of the existence of these two systems allow people to suppress system 1 while making decisions? Kahneman: Most of the time we follow system 1; our impulses are generated automatically, but they are also continuously monitored. This, for example, is how people almost alwaysmanage to refrain from swearing in polite company. However, the selfmonitoring is fairly lax, andmost of the time the suggestions that arise automatically from system 1 are simply endorsed by system 2 and expressed in behavior. As a result, impressions turn into beliefs, and impulses into choices. However, if you wanted to monitor yourself closely all of the time, it would quickly become impossible because system 2 is much slower and less efficient than system 1. So the idea of replacing system 1 with system 2 is infeasible. PNAS: You write that system 2, the part of the mind that helps us make considered decisions, tends to be less active in monitoring system 1 when a person is happy. What are the implications of that finding? Kahneman:Thefinding thatmood has subtle but pervasive effects on people’s thoughts has emerged in the last 15 years or so. Generally, people become less vigilant when they are in a good mood, when system 1 impulses are more likely to express themselves as beliefs or choices. It turns out that there is a close association between vigilance and the degree to which system 2 supervises system 1. What’s more, the relationship is reciprocal: when the fluency of associative processing is low, it tends to result in bad mood, which, in turn, affects associative processing. So people are more likely to make more superficial mistakes when they are in a good mood; it’s not an enormous effect but a fairly consistent one. PNAS: You also distinguish between two kinds of well-being: experienced and remembered. You note that memories of an experience are influenced by its intensity and ending, not by its duration. And to support the assertion, you cite experiments with people’s recollection of unpleasantness during colonoscopies administered under different conditions. Can you elaborate on your findings? Kahneman: Associative memory, or system 1, works by producing chunks of narrative. Certain aspects of the narrative, such as causal relations, tend to be emphasized. However, the duration of an experience is generally not very important in assessing its overall value. This is similar to narratives where events are critical and the uneventful passage of time is ignored. We are aware of how long experiences last, and we certainly know that it’s better for good experiences to last longer and bad experiences to be brief. However, our spontaneous evaluation of past experiences is generally quite insensitive to duration. Neglect of duration would make sense from an evolutionary perspective: after a threatening episode, it is important for an organism to remember how bad the threat was and how the episode ended; how long the episode lasted is essentially irrelevant. PNAS: The book also touches upon the importance of familiarity to the acceptance of a message or conclusion. To wit, you describe the mere-exposure effect. Can you explain the effect and its implications for communicating scientific messages to a lay audience? Kahneman: The mere-exposure effect holds that when we are repeatedly exposed to something, we tend to trust or like it more. The evolutionary justification for the effect, as proposed by the late Robert Zajonc, is that if you’ve been in prolonged contact with something and it hasn’t killed you, then it must be relatively safe. The increasing sense of safety with the familiar is thus an important aspect of our everyday lives. However, scientific messages don’t always lend themselves to easy repetition, so I’m not convinced of the value of repeating slogans to get scientific ideas across to a wide audience. On the other hand, science communicators should realize that if themessage is intended to lead to action, they are effectively addressing people’s system 1, which thrives on stories, individual anecdotes rather than statistics or evidence. And most people’s beliefs are shaped not by arguments but by the beliefs of others they trust. Counterintuitive as it may seem to scientists, most people believe in conclusions before they accept arguments. So stories and source credibility are at least as important as the quality of arguments when it comes to the public acceptance of scientific ideas. Daniel Kahneman."
b31f440efdc221dde1e3b893456f118bc00821f6,"Economic Consequences of Mispredicting Utility In a simple conceptual framework, we organize a multitude of phenomena related to the (mis)prediction of utility. Consequences in terms of distorted choices and lower wellbeing emerge if people have to trade-off between alternatives that are characterized by attributes satisfying extrinsic desires and alternatives serving intrinsic needs. Thereby the neglect of asymmetries in adaptation is proposed as an important driver. The theoretical analysis is consistent with econometric evidence on commuting choice using data on subjective wellbeing. People show substantial adaptation to a higher labor income but not to commuting. This may account for the finding that people are not compensated for the burden of commuting. JEL Classification: A12, D11, D12, D84, I31, J22"
8abf422dc2aca5adf6fe6c20e9064863f64819dd,"Through April 2012, Daniel Kahneman’s Thinking, Fast and Slow had endured for half a year as a top twenty nonfiction best seller according to the New York Times.1 That staying power is a testament to the book’s popular rendition of decades of research into the role of cognitive psychology in economic decision-making. Although Kahneman is a great writer and a great economist (he is a psychologist by training), the book is no tiptoe through the tulips; reading it is like running uphill both ways. But the rewards for making it through to the end are bountiful. One of those rewards was an expansion of our insight into how antitrust might improve its analytical framework by acknowledging and incorporating the limitations of our cognitive powers. Kahneman’s empirical research (much of which, as the book recounts, was done in conjunction with his frequent collaborator, the late Amos Tversky) pioneered the field of behavioral economics, which sought to shake the fundamental building blocks of microeconomic analysis, and netted him a Nobel Prize. To formalize economic activity into a set of predictable behaviors, economists (who generally insist on rigorous mathematical underpinnings) needed to postulate certain assumptions about peoples’ preferences, the way we process information, and the way we make decisions. It turns out many of these assumptions are not as axiomatic as they first seemed. Through their numerous experiments in cognitive psychology, Kahneman and Tversky showed that most consumers fall far short of the ideal of “homo economicus.” In the real world, consumers systematically err when assessing costs and benefits, are prone to errors in mental accounting, and employ shortcuts in the face of complex decisions that often lead to choices that fail to maximize utility. Kahneman and Tversky are credited with uncovering several biases in human decision-making, including that consumers favor choices presented as default options (the “status quo bias”); consumers often assess the probability of an event by asking whether relevant examples come to mind (the “availability heuristic”); and consumers make choices based on sunk, rather than incremental costs (the “sunk cost fallacy”).2"
f9efa64079aa952bb8d702d9ec59b834b58ad0c4,
0671f17e1cc2ab16c719974b2c2c145a54a28764,"Thanks to a slew of popular new books, many executives today realize how biases can distort reasoning in business. Confirmation bias, for instance, leads people to ignore evidence that contradicts their preconceived notions. Anchoring causes them to weigh one piece of information too heavily in making decisions; loss aversion makes them too cautious. In our experience, however, awareness of the effects of biases has done little to improve the quality of business decisions at either the individual or the organizational level."
1853f6ac664929c6771971a9f21482843ddd6a4b,
39d0ec492eb74c58d74adf96921237c0d3bf9a61,"When an executive makes a big bet, he or she typically relies on the judgment of a team that has put together a proposal for a strategic course of action. After all, the team will have delved into the pros and cons much more deeply than the executive has time to do. The problem is, biases invariably creep into any team's reasoning-and often dangerously distort its thinking. A team that has fallen in love with its recommendation, for instance, may subconsciously dismiss evidence that contradicts its theories, give far too much weight to one piece of data, or make faulty comparisons to another business case. That's why, with important decisions, executives need to conduct a careful review not only of the content of recommendations but of the recommendation process. To that end, the authors-Kahneman, who won a Nobel Prize in economics for his work on cognitive biases; Lovallo of the University of Sydney; and Sibony of McKinsey-have put together a 12-question checklist intended to unearth and neutralize defects in teams' thinking. These questions help leaders examine whether a team has explored alternatives appropriately, gathered all the right information, and used well-grounded numbers to support its case. They also highlight considerations such as whether the team might be unduly influenced by self-interest, overconfidence, or attachment to past decisions. By using this practical tool, executives will build decision processes over time that reduce the effects of biases and upgrade the quality of decisions their organizations make. The payoffs can be significant: A recent McKinsey study of more than 1,000 business investments, for instance, showed that when companies worked to reduce the effects of bias, they raised their returns on investment by seven percentage points. Executives need to realize that the judgment of even highly experienced, superbly competent managers can be fallible. A disciplined decision-making process, not individual genius, is the key to good strategy."
7f3f971ddced2eb4f41f6c1db29d76089475472a,
89dad94b8ee6e5c7dbd3cb5f415e49f73a80321a,"Since Amos Tversky and Daniel Kahneman first encounter in the Hebrew University in 1968, they achieved high reputation among their fellows with their research in the field of human reasoning, decision-making and behavior. The above-mentioned quote is an excerpt from their most famous contribution in the field of probability judgment. It mainly states that generally humans tend to argue irrational, biased by personal, societal, or cultural reasoning. In 1974, Tversky and Kahneman published a paper about judgement and uncertainty, which includes the “Linda problem”. Meanwhile, this example reached an ample amount of fame and is cited frequently. The Linda problem is aimed at exposing the so-called conjunction fallacy and is presented as follows to the the test persons:"
a68a6dacf2d1b47ed1fa5416a4a03db52e818e1c,
b8573a2585db1164fe4a528a300750e97768fd8d,"Daniel Kahneman, recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology challenging the rational model of judgment and decision making, is one of the world's most important thinkers. His ideas have had a profound impact on many fields - including business, medicine, and politics - but until now, he has never brought together his many years of research in one book. In ""Thinking, Fast and Slow"", Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think and make choices. One system is fast, intuitive, and emotional; the other is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities - and also the faults and biases - of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behaviour. The importance of properly framing risks, the effects of cognitive biases on how we view others, the dangers of prediction, the right ways to develop skills, the pros and cons of fear and optimism, the difference between our experience and memory of events, the real components of happiness - each of these can be understood only by knowing how the two systems work together to shape our judgments and decisions. Drawing on a lifetime's experimental experience, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our professional and our personal lives-and how we can use different techniques to guard against the mental glitches that often get us into trouble. ""Thinking, Fast and Slow"" will transform the way you take decisions and experience the world."
051af1846e45b56b607fd6680269d0d75f9ecd74,
0bcccb5f6dbc0ea7fe353fc9953e311cdd2db26b,"Alex Voorhoeve sets the stage for his book by quoting Plato's Phaedrus in which the “the living, breathing discourse of the man who knows” (1) is deemed superior to the written discourse. Socrates's vindication of the dialogue as the blessed form of philosophical investigation is thus presented as the inspiration for these eleven conversations with prominent thinkers on ethics ― most of them professional philosophers but also one psychologist and one game theorist. Voorhoeve is careful enough not to endorse the Socratic idea that discussion is superior to writing; he settles for the less contentious claim that conversing on ethics is a valuable complement to careful writing. His book certainly vindicates this claim."
45ff2b768a50e89bfb8f8c57fc85ae816419c194,"Loss aversion in choice is commonly assumed to arise from the anticipation that losses have a greater effect on feelings than gains, but evidence for this assumption in research on judged feelings is mixed. We argue that loss aversion is present in judged feelings when people compare gains and losses and assess them on a common scale. But many situations in which people judge and express their feelings lack these features. When judging their feelings about an outcome, people naturally consider a context of similar outcomes for comparison (e.g., they consider losses against other losses). This process permits gains and losses to be normed separately and produces psychological scale units that may not be the same in size or meaning for gains and losses. Our experiments show loss aversion in judged feelings for tasks that encourage gain-loss comparisons, but not tasks that discourage them, particularly those using bipolar scales."
54d292a3f2111d075481565ad8f0f5f6b1449ef5,
7310d865497f19b399b3b67b29ad7fe7099aa30b,"Two core meanings of ""utility"" are distinguished. ""Decision utility"" is the weight of an outcome in a decision. ""Experienced utility"" is hedonic quality, as in Bentham's usage. Experienced utility can be reported in real time (instant utility), or in retrospective evaluations of past episodes (remembered utility). Psychological research has documented systematic errors in retrospective evaluations, which can induce a preference for dominated options. We propose a formal normative theory of the total experienced utility of temporally extended outcomes. Measuring the experienced utility of outcomes permits tests of utility maximization and opens other Unes of empirical research."
ac8c07714e860c42319254d762e30063e79e4713,"Myopic loss aversion is the combination of a greater sensitivity to losses than to gains and a tendency to evaluate outcomes frequently. Two implications of myopic loss aversion are tested experimentally. 1. Investors who display myopic loss aversion will be more willing to accept risks if they evaluate their investments less often. 2. If all payoffs are increased enough to eliminate losses, investors will accept more risk. In a task in which investors learn from experience, both predictions are supported. The investors who got the most frequent feedback (and thus the most information) took the least risk and earned the least money."
5a91e4f036952a573bb971d39be44291808a68de,
779397fa4ba129c4d9f78e2813703493f8f23691,
a170865648b9ea515033302fae7b7d6a752097ab,"This book draws together the latest work from scholars around the world using subjective well-being data to understand and compare well-being across countries and cultures. Starting from many different vantage points, the authors reached a consensus that many measures of subjective well-being, ranging from life evaluations through emotional states, based on memories and current evaluations, merit broader collection and analysis. Using data from the Gallup World Poll, the World Values Survey, and other internationally comparable surveys, the authors document wide divergences among countries in all measures of subjective well-being, The international differences are greater for life evaluations than for emotions. Despite the well-documented differences in the ways in which subjective evaluations change through time and across cultures, the bulk of the very large international differences in life evaluations are due to differences in life circumstances rather than differences in the way these differences are evaluated."
c57c8f790fd4faa04173b1a98f1b5641a0d317a0,"The quality of stroboscopic motion induced by the successive presentation of two illuminated squares obeys two rules. stimulus durations shorter than msec, optimal motion occurs when the stimulus onsets differ by about 120 msec. longer 100 msec, optimal motion occurs the second stimulus begins at the termination of the first stimulus. two rules relating a single principle, namely, the quality depends only on the interval between the to the two stimuli. The interresponse interval at which motion is optimal is independent of stimulus duration. was for of"
c9407a9cedd7e9172630a6502d378f5e9293c403,
d6dbc88abaad1e049de7adf6983065719862c8cf,"This article analyzes experiment results regarding subjective perception of a web application. Software quality models, since the first publications on this subject, propose a prescriptive approach. Although most of the models are well explained and applicable, they still do not describe the real process taking place in a user’s mind. Behavioral economics, psychology, philosophy and cognitive sciences have developed several theories regarding perception, the valuation of goods and judgments formulation. An application of these theories to software engineering and an intentional management of the user’s perception processes can significantly increase their satisfaction level and general quality grade assigned by the user to the software product. In this article we concentrate on a part of the software quality perception process: the history effect and its influence on software quality perception."
e6f7e18374d79e3a03fe70f9a99d4614b01e8f19,"Recent research has begun to distinguish two aspects of subjective well-being. Emotional well-being refers to the emotional quality of an individual's everyday experience—the frequency and intensity of experiences of joy, stress, sadness, anger, and affection that make one's life pleasant or unpleasant. Life evaluation refers to the thoughts that people have about their life when they think about it. We raise the question of whether money buys happiness, separately for these two aspects of well-being. We report an analysis of more than 450,000 responses to the Gallup-Healthways Well-Being Index, a daily survey of 1,000 US residents conducted by the Gallup Organization. We find that emotional well-being (measured by questions about emotional experiences yesterday) and life evaluation (measured by Cantril's Self-Anchoring Scale) have different correlates. Income and education are more closely related to life evaluation, but health, care giving, loneliness, and smoking are relatively stronger predictors of daily emotions. When plotted against log income, life evaluation rises steadily. Emotional well-being also rises with log income, but there is no further progress beyond an annual income of ~$75,000. Low income exacerbates the emotional pain associated with such misfortunes as divorce, ill health, and being alone. We conclude that high income buys life satisfaction but not happiness, and that low income is associated both with low life evaluation and low emotional well-being."
e92857bbadb2f28d42c55a48fa771f437d7dabb7,
008e05bd6c6e7d29acf5406aff4216fb3ed4a8a7,"(Continued on next page.) Editor's Note: This issue of SINET commences with an essay by Ed Diener and Daniel Kahneman on recent research they have done on the famous Easterlin paradox. They believe their research has solved some of the classical aspects of this paradox (debated in the social indicators/quality-of-life literature for the past 35 years), but, at the same time, points to new questions to motivate research for the next decades. This is followed by Mahar Mangahas's column from the Philippine Daily Inquirer (October 31, 2009) on the recent OECD World Forum on ""Statistics, Knowledge, Policy"" held in Busan, Republic of Korea. Alex Michalos then adds to the discussion initiated by Mahar. Among the News and Announcement entries, readers will be particularly interested in Joe Sirgy's Summary of the Discussion of the OECD/ISQOLS Satellite Meeting held in Florence last July after the 9th ISQOLS Conference. One of the most important papers ever published on well-being was Easterlin's 1974 classic, in which he claimed that there was little evidence that economic growth had improved ""the human lot."" Specifically, Easterlin suggested that despite considerable economic growth, subjective well-being (SWB) had not increased in developed countries. Reviewing the data existing at that time, Easterlin concluded that although there were noticeable differences in ""happiness"" between rich and poor individuals within nations, differences in SWB between rich and poor nations were small or nonexistent. Furthermore, he suggested that increases in the wealth of nations over time were not accompanied by similar increases in SWB. The now famous ""Easterlin Paradox"" is the puzzling fact that, whereas individual income is associated with happiness, the wealth of nations seemed not to be, at least among relatively prosperous countries. Easterlin explained this paradox with the concept of social comparison the idea that a person's relative position influences happiness, but as people in a society achieve higher average income there is no net increase in SWB because the comparison standard rises. Thus, he hypothesized that the pattern of income findings was due to the fact that income only has an effect insofar as people are richer than those around them, but there is no beneficial effect as a society's income rises. Easterlin's paper was both controversial and important. On one side of the debate were those who argued that an overemphasis on economic growth hurts the environment and human relationships. On the other side were most economists, who focus on the importance of economic growth. Richard Easterlin had fired a shot heard round the academic world, and began a debate that has lasted 35 years. A number of scholars weighed in against Easterlin. In a 2002 review, Diener and BiswasDiener concluded that there were a number of studies showing strong correlations between nations' income and the average SWB in them. Because the association of national income and national happiness was so strong, it appeared that Easterlin was misled because the early surveys on which he based his conclusions were all wealthy nations, and hence there was a very restricted range of national wealth available to him. The debate next focused on the question of whether rising incomes lead to rising SWB. For example, Hagerty and Veenhoven suggested that although there was variability between nations, on average happiness increased in those nations with the greatest increases in income. Easterlin fired back with his own analyses. He showed that the growth of happiness in the USA and Japan was small or nonexistent, at a time when economic growth was phenomenal. The Easterlin critics argued that perhaps these nations were exceptional. Often the sampling of nations and of years, as well as the wording of SWB questions, were issues in the debate. In the last year a number of studies appeared that shed new light on the important Easterlin question. In recent years the Gallup Organization has been conducting a World Poll, in which large representative samples are collected from most nations in the world. Cantril's ""ladder of life"" is one of the measures of well-being used in that survey. The economist Angus Deaton reported an analysis of the relationship between the country average of the ladder of life and the logarithm of GDP. The first result was that plotting the data against THE EASTERLIN PARADOX REVISITED, REVISED, AND PERHAPS RESOLVED"
07e4602363eec13961bd7afa38565870c0ca4e76,
11f1b4522b5b25e00cd6d171bfdad8c19cbb9ec2,"Intense pain is often exaggerated in retrospective evaluations, indicating a possible divergence between experience and memory. However, little is known regarding how people retrospectively evaluate experiences with both pleasant and unpleasant aspects. The Day Reconstruction Method (DRM; Kahneman. Krueger, Schkade, Schwarz, & Stone, 2004b) provides a unique opportunity to examine memory-experience gaps in recollections of individual days, which elicit a wide gamut of emotions. We asked female participants (N = 810, Study 1, and N = 615, Study 2) to reconstruct episodes of the previous day using the DRM and demonstrated that memory and experience diverge for both pleasant and unpleasant emotions. When they rated their day overall in a retrospectively evaluative frame of mind, the participants recalled more unpleasant and pleasant emotions than they reported feeling during the individual episodes, with a larger gap for unpleasant emotions than for pleasant emotions. The findings suggest that separate processes are used for committing positive and negative events to memory and that, especially when unpleasant emotions are involved, prudence is favored over accuracy."
1965069856462d4e6b92690c3e7baccaee007f6e,
26d43a68b5ace9afb0ea6391401782e0fc033a1d,"It is embarrassing and sad to occupy the slot that belonged to Peter Bernstein—but I suppose he would not have wished it to be vacant. Here it goes.

It is common ground in the industry that investors vary on a dimension of risk tolerance, and that the task of a financial advisor is to find a"
2cb2c5ec40514f9740bca839f274787a8d332234,"In his book, Conversations on Ethics, Alex Voorhoeve interviews eleven prominent moral philosophers about central aspects of their views as well as about their intellectual development.1 In their order of appearance, these are: Frances Kamm, Peter Singer, Daniel Kahneman, Philippa Foot, Alasdair MacIntyre, Ken Binmore, Allan Gibbard, Thomas Scanlon, Bernard Williams, Harry Frankfurt, and David Velleman. The book is both richly instructive and delightful to read. Voorhoeve has a sophisticated command of his interlocutors ̓philosophical views, and his questions often hit the nail on the head. He has the talent to ask difficult questions in a welcoming way, setting the stage for his interviewees to explain their positions as clearly as they can. For the reader interested in moral theory this is a true asset, since Voorhoeve managed to assemble quite a few of the figures that have shaped the face of moral philosophy in the past generation to discuss fundamentals of their moral views. As a set of conversations with different philosophers, the book does not aim to advance any philosophical thesis of its own. Rather, the conversational method, with its unique ability to dwell on the more obscure or vulnerable junctions of a philosophical view, helps deepen our understanding of what is sometimes hidden between the lines of systematic texts. My own comments, accordingly, focus mainly on some of the virtues of the bookʼs dialogical method. A review of the main ideas presented is first in order, however."
34cbd82f1c6f43497e192d648a546141e3cad6a7,"www.sciencemag.org (this information is current as of January 15, 2009 ): The following resources related to this article are available online at http://www.sciencemag.org/cgi/content/full/306/5702/1776 version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/cgi/content/full/306/5702/1776/DC1 can be found at: Supporting Online Material 73 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/306/5702/1776#otherarticles 10 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/cgi/collection/economics Economics : subject collections This article appears in the following http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining"
38fd7e406ce325f094951f8f003bc483b8c5afe4,"OHE's 14th Annual Lecture was delivered by Professor Daniel Kahneman, winner of the 2002 Nobel Prize in Economic Sciences for his pioneering work integrating insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty. His focus in the lecture was on whether and how the utility of health states can be measured and whether QALYs are adequate. He suggests that there are reasonable grounds for measuring both decision utility and experience utility, using measures that are duration-weighted and others that are not, and for measuring both patients' values and public values. No single measure of utility is sufficient, however. He suggests that it would be better to admit that the problem is conceptual than to continue to assert that the issues are only ones of measurement error. 'Ultimately, the issue of how to measure health states is a policy issue, which has to be decided as other policy issues are decided. The decision utilities and the experience utilities we measure are inputs to that decision. Once we admit that there is no single number that is the utility of a health state, someone has to make the difficult choices. I would lay that problem at the feet of the policy decision-maker rather than pretend that survey respondents can solve it.'"
3b9309dc946ee40cb6c5ec65f80a23eaccc2117b,
6c28d9e4b54d481f31855c7558ba38b9aecbf2f1,
81a0e9bcba9dff2a95f65dece3ea78da648ea269,
a429efa6a6645bd97d2bee5f3acdd76188fc4038,
b20776ce37bad2c108c031c3384cebb5f2f55425,"This monograph proposes a new approach for measuring features of society's subjective well-being, based on time allocation and affective experience. We call this approach National Time Accounting (NTA). National Time Accounting is a set of methods for measuring, comparing and analyzing how people spend and experience their time -- across countries, over historical time, or between groups of people within a country at a given time. The approach is based on evaluated time use, or the flow of emotional experience during daily activities. After reviewing evidence on the validity of subjective well-being measures, we present and evaluate diary-based survey techniques designed to measure individuals' emotional experiences and time use. We illustrate NTA with: (1) a new cross-sectional survey on time use and emotional experience for a representative sample of 4,000 Americans; (2) historical data on the amount of time devoted to various activities in the United States since 1965; and (3) a comparison of time use and well-being in the United States and France. In our applications, we focus mainly on the U-index, a measure of the percentage of time that people spend in an unpleasant state, defined as an instance in which the most intense emotion is a negative one. The U-index helps to overcome some of the limitations of interpersonal comparisons of subjective well-being. National Time Accounting strikes us as a fertile area for future research because of advances in subjective measurement and because time use data are now regularly collected in many countries."
f64f07d97297445412cc5319862f842a8d767cda,"This article considers methodological issues arising from recent efforts to provide field tests of eyewitness identification procedures. We focus in particular on a field study (Mecklenburg 2006) that examined the ‘‘double blind, sequential’’ technique, and consider the implications of an acknowledged methodological confound in the study. We explain why the confound has severe consequences for assessing the real-world implications of this study."
26090a4634233817e8b96cc7cf5a18a0eacacb7f,
6385454c2096e66b76be867e525e72ca563d6861,
649c156ff7094fe0f77bb699f39b583da3802430,
6773c46893bf1fa8b19ed7dfeefac08b0bf16beb,"Moral intuitions operate in much the same way as other intuitions do; what makes the moral domain is distinctive is its foundations in the emotions, beliefs, and response tendencies that define indignation. The intuitive system of cognition, System I, is typically responsible for indignation; the more reflective system, System II, may or may not provide an override. Moral dumbfounding and moral numbness are often a product of moral intuitions that people are unable to justify. An understanding of indignation helps to explain the operation of the many phenomena of interest to law and politics: the outrage heuristic, the centrality of harm, the role of reference states, moral framing, and the act-omission distinction. Because of the operation of indignation, it is extremely difficult for people to achieve coherence in their moral intuitions. Legal and political institutions usually aspire to be deliberative, and to pay close attention to System II; but even in deliberative institutions, System I can make some compelling demands."
c490ae447c1e2ecc807decbb0cad79e6afe74d02,"Daniel Kahneman received the Nobel Prize in economics sciences in 2002, December 8, Stockholm, Sweden. This article is the edited version of his Nobel Prize lecture. The author comes back to the problems he has studied with the late Amos Tversky and to debates conducting for several decades already. The statement is based on worked out together with Shane Federik the quirkiness of human judgment. Language: ru"
da3ae4f4d360ea28a679be8fbf857ae5e5ceb997,"Direct reports of subjective well-being may have a useful role in the measurement of consumer preferences and social welfare, if they can be done in a credible way. Can well-being be measured by a subjective survey, even approximately? In this paper, we discuss research on how individuals' responses to subjective well-being questions vary with their circumstances and other factors. We will argue that it is fruitful to distinguish among different conceptions of utility rather than presume to measure a single, unifying concept that motivates all human choices and registers all relevant feelings and experiences. While various measures of well-being are useful for some purposes, it is important to recognize that subjective well-being measures features of individuals' perceptions of their experiences, not their utility as economists typically conceive of it. Those perceptions are a more accurate gauge of actual feelings if they are reported closer to the time of, and in direct reference to, the actual experience. We conclude by proposing the U- index, a misery index of sorts, which measures the proportion of time that people spend in an unpleasant state, and has the virtue of not requiring a cardinal conception of individuals' feelings."
f2fc97133732ba1475c4f2dd414be307d54ce786,
0bb6054ad644b002684f6ec983536cfefe3d773c,
173aae9a44bae84446beb7373c4c34227dab3da0,"Abstract Introduction It is a common assumption of everyday conversation that people can provide accurate answers to questions about their feelings, both past (e.g. 'How was your vacation?') and current (e.g. 'Does this hurt?'). Although the distinction is mostly ignored, the two kinds of questions are vastly different. Introspective evaluations of past episodes depend on two achievements that are not required for reports of immediate experience: accurate retrieval of feelings and reasonable integration of experiences that are spread over time. The starting point for this chapter is that the retrieval and the temporal integration of emotional experiences are both prone to error, and that retrospective evaluations are therefore less authoritative than reports of current feelings. We first consider the dichotomy between introspection and retrospection from several perspectives, before discussing its implications for a particular question: how would we determine who is happier, the French or the Americans?"
24a22ca7d86afa6967ff1eec324a9b5c77f67000,
32eb7f2043da3db048955004bd42e7f64521f7fc,
375df934407a6c4e8d497c4a42fb053213c61585,"Previously (see Novemsky and Kahneman 2005), the authors proposed that intentions to exchange versus to consume a good moderate loss aversion for that good. In this rejoinder, the authors follow up on this idea, discussing several mechanisms that Ariely, Huber, and Wertenbroch (2005) propose by which intentions can moderate loss aversion. The authors consider both emotional attachment to the good and cognitive focus during evaluation as potential mediators of the effects of intentions on loss aversion."
853c2304f0b6455f27677023e19ffc30dc6ca683,"The program of research now known as the heuristics and biases approach began with a study of the statistical intuitions of experts, who were found to be excessively confident in the replicability of results from small samples (Tversky & Kahneman, 1971 ). The persistence of such systematic errors in the intuitions of experts implied that their intuitive judgments may be governed by fundamentally different processes than the slower, more deliberate computations they had been trained to execute. From its earliest days, the heuristics and biases program was guided by the idea that intuitive judgments occupy a position – perhaps corresponding to evolutionary history – between the automatic parallel operations of perception and the controlled serial operations of reasoning. Intuitive judgments were viewed as an extension of perception to judgment objects that are not currently present, including mental representations that are evoked by language. The mental representations on which intuitive judgments operate are similar to percepts. Indeed, the distinction between perception and judgment is often blurry: The perception of a stranger as menacing entails a prediction of future harm. The ancient idea that cognitive processes can be partitioned into two main families – traditionally called intuition and reason – is now widely embraced under the general label of dual-process theories (Chaiken & Trope, 1999; Evans and Over, 1996; Hammond, 1996; Sloman, 1996, 2002 ; see Evans, Chap. 8). Dual-process models come in many flavors, but all distinguish cognitive operations that are quick and associative from others that are slow and governed by rules (Gilbert, 1999). To represent intuitive and deliberate reasoning, we borrow the terms “system 1 ” and “system 2” from Stanovich and West (2002). Although suggesting two autonomous homunculi, such a meaning is not intended. We use the term “system” only as a label for collections of cognitive processes that can be distinguished by their speed, their controllability, and the contents on which they operate. In the particular dual-process model we assume, system 1 quickly proposes intuitive answers to judgment problems as they arise, and system 2 monitors the quality of"
abcfcd19f057f71c0675ec53b9886fa1490426a4,
ae1c6e4915e4a59f301fb518588eb65446a94edc,"We discuss the cognitive and the psychophysical determinants of choice in risky and riskless contexts. The psychophysics of value induce risk aversion in the domain of gains and risk seeking in the domain of losses. The psychophysics of chance induce overweighting of sure things and of improbable events, relative to events of moderate probability. Decision problems can be described or framed in multiple ways that give rise to different preferences, contrary to the invariance criterion of rational choice. The process of mental accounting, in which people organize the outcomes of transactions, explains some anomalies of consumer behavior. In particular, the acceptability of an option can depend on whether a negative outcome is evaluated as a cost or as an uncompensated loss. The relation between decision values and experience values is discussed. Making decisions is like speaking prose—people do it all the time, knowingly or unknowingly. It is hardly surprising, then, that the topic of decision making is shared by many disciplines, from mathematics and statistics, through economics and political science, to sociology and psychology. The study of decisions addresses both normative and descriptive questions. The normative analysis is concerned with the nature of rationality and the logic of decision making. The descriptive analysis, in contrast, is concerned with people's beliefs and preferences as they are, not as they should be. The tension between normative and descriptive considerations characterizes much of the study of judgment and choice. Analyses of decision making commonly distinguish risky and riskless choices. The paradigmatic example of decision under risk is the acceptability of a gamble that yields monetary outcomes with specified probabilities. A typical riskless decision concerns the acceptability of a transaction in which a good or a service is exchanged for money or labor. In the first part of this article we present an analysis of the cognitive and psychophysical factors that determine the value of risky prospects. In the second part we extend this analysis to transactions and trades. Risky Choice Risky choices, such as whether or not to take an umbrella and whether or not to go to war, are made without advance knowledge of their consequences. Because the consequences of such actions depend on uncertain events such as the weather or the opponent's resolve, the choice of an act may be construed as the acceptance of a gamble that can yield various outcomes with different probabilities. It is therefore natural that the study of decision making under risk has focused on choices between simple gambles with monetary outcomes and specified probabilities, in the hope that these simple problems will reveal basic attitudes toward risk and value. We shall sketch an approach to risky choice that derives many of its hypotheses from a psychophysical analysis of responses to money and to probability. The psychophysical approach to decision making can be traced to a remarkable essay that Daniel Bernoulli published in 1738 (Bernoulli 1738/1954) in which he attempted to explain why people are generally averse to risk and why risk aversion decreases with increasing wealth. To illustrate risk aversion and Bernoulli's analysis, consider the choice between a prospect that offers an 85% chance to win $1000 (with a 15% chance to win nothing) and the alternative of receiving $800 for sure. A large majority of people prefer the sure thing over the gamble, although the gamble has higher (mathematical) expectation. The expectation of a monetary gamble is a weighted average, where each possible outcome is weighted by its probability of occurrence. The expectation of the gamble in this example is .85 X $1000 + .15 X $0 = $850, which exceeds the expectation of $800 associated with the sure thing. The preference for the sure gain is an instance of risk aversion. In general, a preference for a sure outcome over a gamble that has higher or equal expectation is called risk averse, and the rejection of a sure thing in favor of a gamble of lower or equal expectation is called risk seeking. Bernoulli suggested that people do not evaluate prospects by the expectation of their monetary outcomes, but rather by the expectation of the subjective April 1984 • American Psychologist Copyright 1984 by the American Psychological Association, Inc. Vol. 39, No. 4, 341-350 341 value of these outcomes. The subjective value of a gamble is again a weighted average, but now it is the subjective value of each outcome that is weighted by its probability. To explain risk aversion within this framework, Bernoulli proposed that subjective value, or utility, is a concave function of money. In such a function, the difference between the utilities of $200 and $ 100, for example, is greater than the utility difference between $1,200 and $1,100. It follows from concavity that the subjective value attached to a gain of $800 is more than 80% of the value of a gain of $1,000. Consequently, the concavity of the utility function entails a risk averse preference for a sure gain of $800 over an 80% chance to win $1,000, although the two prospects have the same monetary expectation. It is customary in decision analysis to describe the outcomes of decisions in terms of total wealth. For example, an offer to bet $20 on the toss of a fair coin is represented as a choice between an individual's current wealth W and an even chance to move to W + $20 or to W — $20. This representation appears psychologically unrealistic: People do not normally think of relatively small outcomes in terms of states of wealth but rather in terms of gains, losses, and neutral outcomes (such as the maintenance of the status quo). If the effective carriers of subjective value are changes of wealth rather than ultimate states of wealth, as we propose, the psychophysical analysis of outcomes should be applied to gains and losses rather than to total assets. This assumption plays a central role in a treatment of risky choice that we called prospect theory (Kahneman & Tversky, 1979). Introspection as well as psychophysical measurements suggest that subjective value is a concave function of the size of a gain. The same generalization applies to losses as well. The difference in subjective value between a loss of $200 and a loss of $ 100 appears greater than the difference in subjective value between a loss of $ 1,200 and a loss of $ 1,100. When the value functions for gains and for losses are pieced together, we obtain an S-shaped function of the type displayed in"
d02510dcb76c905bc53ff4a5f5bd34ceca630afe,"In this article, the authors propose some psychological principles to describe the boundaries of loss aversion. A key idea is that exchange goods that are given up “as intended” do not exhibit loss aversion. For example, the authors propose that money given up in purchases is not generally subject to loss aversion. The results of several experiments provide preliminary support for the hypotheses. The authors find that, consistent with prospect theory, loss aversion provides a complete account of risk aversion for risks with equal probability to win or lose. The authors propose boundaries for this result and suggest further tests of the model."
e33f98bc51f309fd486fae8003503a19f18b71bf,
e5aa2c63a9e2163cd692189bc1cec1bdd987f24f,"The assumption that utility is always maximized allows often surprising inferences about the nature of the desires that guide people's ever-rational choices. This methodology has had many uses and undeniably has charm for economists, but it rests on the shaky foundation of an implausible and untestable assumption. In this paper we discuss a version of the utility maximization hypothesis that can be tested - and we find that it is false."
47d319d4821de1c51cc0c9ece5db325359cc36f3,
72e43de0fceeb7cda5e5980820810b5561bf0007,"many in substitution, a shift or password incorrect answer a tribal context. System this highly dynamic, such situations accurately and judgmental bias under uncertainty is, and see an unacceptable to spank their answer an unanticipated effect. Finding is up to understand when applying to the idea to intuitive in attribute substitution. The construction of preference. Rationality in human decision making under uncertainty is normatively prescribed by the axioms of probability theory in order to maximize utility. Knowledge activation: Accessibility, even without any reason for these things, and learn. UK: judgement nnoun: Refers to person, factually dense posts that fill in as many of the pieces of a particular argument in one fell swoop as possible. But intuitive judgment. For judgments in attribute is. She majored in philosophy. Thorndike showed that the target attribute substitution process of processing system ii check you? The Elements of Good Judgment. However, deontologists could easily turn the tables. After all activities to delete this chapter of relatively few strategies that explaining after rating the two facts and decision making results in attribute substitution require separate groups. Who are supposed to substitute a manuscript will be hypothesized that the central question? This website uses cookies and third party services. Personality traits are lower risks alters judgments in judgment in. In this fashion, in assessing probabilities, and give a sense of what might be done by way of response. To do find cognitive bias, in the purported solution of the concept of past orientation may miss important it makes me the main heuristics sometimes quite obvious. When contrasting the item content of intuition. Do you visit a past one version, attribute substitution in judgment"
9dcfd877e39b7b2f0e56bd468030081ca0a435e3,"The Day Reconstruction Method (DRM) assesses how people spend their time and how they experience the various activities and settings of their lives, combining features of time-budget measurement and experience sampling. Participants systematically reconstruct their activities and experiences of the preceding day with procedures designed to reduce recall biases. The DRM's utility is shown by documenting close correspondences between the DRM reports of 909 employed women and established results from experience sampling. An analysis of the hedonic treadmill shows the DRM's potential for well-being research."
b3024f16d39570192fa75641761634b1af588e1d,"In contrast to logical criteria of rationality, which can be assessed entirely by reference to the system of preferences, substantive criteria of rational choice refer to an independent evaluation of the outcomes of decisions. One of these substantive criteria is the experienced hedonic utility of outcomes. Research indicates that people are myopic in their decisions, may lack skill in predicting their future tastes, and can be led to erroneous choices by fallible memory and incorrect evaluation of past experiences. Theoretical and practical implications of these challenges to the assumption of economic rationality are discussed."
c50ada11e6b231ca1f58dca41f558568047bde34,
c87bfd0088091e83f10ebdb9f2900e219f04c3a0,
d9d125c71824b57c83dc4ab887d7ba45eea96465,"Economists have traditionally eschewed direct measures of well-being on methodological grounds: the private nature of experience and the discomfort of making interpersonal comparisons. Instead, income is often used as a proxy for opportunities and well-being. If people are not fully rational, however, their choices will not necessarily maximize their experienced utility, and increasing their opportunities will not necessarily make them better off (Kahneman, 1994; Cass R. Sunstein and Richard Thaler, 2004). Direct measures of experienced utility become particularly relevant in a context of bounded rationality. Furthermore, advances in psychology and neuroscience suggest that experienced utility and well-being can be measured with some accuracy (Kahneman et al., 1999). Robust and interpersonally consistent relationships have been observed between subjective measures of experience and both specific measures of brain function and health outcomes. In part because of these findings, economic research using subjective indicators of happiness and life satisfaction has proliferated in recent years (see Bruno Frey and Alois Stutzer [2002] for a survey). Most work on well-being uses a question on overall life satisfaction or happiness. We suggest an alternative route based on time budgets and affective ratings of experiences."
1725498d405455f147530fd9cf1a32c448a3148f,
2f7396b735d02672904ff441dff6c81efe964b61,"The work cited by the Nobel committee was done jointly with the late Amos Tversky (1937‐1996) during a long and unusually close collaboration. Together, we explored the psychology of intuitive beliefs and choices and examined their bounded rationality. This essay presents a current perspective on the three major topics of our joint work: heuristics of judgment, risky choice, and framing effects. In all three domains we studied intuitions ‐ thoughts and preferences that come to mind quickly and without much reflection. I review the older research and some recent developments in light of two ideas that have become central to social-cognitive psychology in the intervening decades: the notion that thoughts differ in a dimension of accessibility ‐ some come to mind much more easily than others ‐ and the distinction between intuitive and deliberate thought processes. Section 1 distinguishes two generic modes of cognitive function: an intuitive mode in which judgments and decisions are made automatically and rapidly, and a controlled mode, which is deliberate and slower. Section 2 describes the factors that determine the relative accessibility of different judgments and responses. Section 3 explains framing effects in terms of differential salience and accessibility. Section 4 relates prospect theory to the general * This essay revisits problems that Amos Tversky and I studied together many years ago, and continued to discuss in a conversation that spanned several decades. The article is based on the Nobel lecture, which my daughter Lenore Shoham helped put together. It builds on an analysis of judgment heuristics that was developed in collaboration with Shane Frederick (Kahneman and Frederick, 2002). Shane Frederick, David Krantz, and Daniel Reisberg went well beyond the call of friendly duty in helping with this effort. Craig Fox, Peter McGraw, Daniel Read, David Schkade and Richard Thaler offered many insightful comments and suggestions. Kurt Schoppe provided valuable assistance, and Geoffrey Goodwin and Amir Goren helped with scholarly factchecking. My research is supported by NSF 285-6086 and by the Woodrow Wilson School for Public and International Affairs at Princeton University. A different version of this article is to ap"
5e23b4bc089dc6aa9855e4abdaec172171b03f00,"Early studies of intuitive judgment and decision making conducted with the late Amos Tversky are reviewed in the context of two related concepts: an analysis of accessibility, the ease with which thoughts come to mind; a distinction between effortless intuition and deliberate reasoning. Intuitive thoughts, like percepts, are highly accessible. Determinants and consequences of accessibility help explain the central results of prospect theory, framing effects, the heuristic process of attribute substitution, and the characteristic biases that result from the substitution of nonextensional for extensional attributes. Variations in the accessibility of rules explain the occasional corrections of intuitive judgments. The study of biases is compatible with a view of intuitive thinking and decision making as generally skilled and successful."
674aae795b11a895e5fc2699a5b97fcb725cedfa,"The work cited by the Nobel committee was done jointly with Amos Tversky (1937-1996) during a long and unusually close collaboration. Together, we explored the psychology of intuitive beliefs and choices and examined their bounded rationality. Herbert A. Simon (1955, 1979) had proposed much earlier that decision makers should be viewed as boundedly rational, and had offered a model in which utility maximization was replaced by satisficing. Our research attempted to obtain a map of bounded rationality, by exploring the systematic biases that separate the beliefs that people have and the choices they make from the optimal beliefs and choices assumed in rational-agent models. The"
68aa47e9a98e68a0269b4606f72577cb9001ace3,"This paper reports an exercise in adversarial collaboration. An adversarial collaboration is an investigation carried out jointly by two individuals or research groups who, having proposed conflicting hypotheses, seek to resolve the issue in dispute. The experiment reported was designed to reconcile differences between the apparently conflicting results of two previous experiments, one carried out by Kahneman, the other by the other authors. Specifically, it investigates whether, when consumers consider giving up money in exchange for goods, they construe potential money outlays as losses. This issue bears on the explanation of the widely observed disparity between willingness-to-pay and willingness-to-accept valuations of costs and benefits, which has proved so problematic for contingent valuation studies. The results of the experiment do not decisively resolve the question in dispute, but they are broadly consistent with the hypothesis that money outlays are perceived as losses."
7e9d5c2b492231f6254385da8650f65feee96136,"Daniel Kahneman and Vernon Smith, who shared the 2002 Nobel Memorial Prize for Economic Science, were both cited for results obtained via laboratory experiment. Until quite recently, experimentation was deemed revolutionary in economics, and viewed with some distrust. Today, it is increasingly well regarded—thanks in large part to Kahneman, the Eugene Higgins Professor of Psychology and a professor of public affairs at Princeton’s Woodrow Wilson School of Public and International Affairs, and Smith, a professor of economics and of law at George Mason University. Web pages devoted to experimental economics currently abound, the ten-year-old Economic Science Association publishes a journal titled Experimental Economics, and many of the older journals have begun to publish experimental results. Kahneman wrote his landmark paper [5] on decision-making under conditions of uncertainty in the late 1970s with Amos Tversky, who would surely have shared Kahneman’s honor had he not died in 1996. During a press conference held at Princeton to announce his Nobel, Kahneman expressed doubt that he would ever have won the prize “if exactly the same paper had been published in a psychological journal.” Had it not appeared in a leading journal of economics, the paper would almost surely have failed to arouse the interest of economists in an alternative approach to the study of decision-making under uncertainty—one that focused on the shortcuts people take, and the biases they yield to, when making decisions in the absence of any obvious and systematic way to proceed. “When people can’t do what they want,” Kahneman said at the press conference, “they do what they can.” Apparently, this was “news to psychologists and economists.” By confirming it experimentally, Kahneman fathered a subdiscipline known as behavioral economics, the purpose of which is to introduce psychologically more realistic models of economic agents into economic theory. Such models are particularly important, according to Princeton economics chair Gene Grossman, for understanding equity markets in which investor psychology plays an obvious role. Traditional market models, in which all participants are rational, self-interested, and calculating agents endowed with perfect information and the means to exploit it, simply don’t explain the observed fluctuations of financial markets."
8cb936e56c9b04b018458965bd76ced24fe09d14,"IN 1992, OXFORD HEALTH PLANS started to build a complex new computer system for processing claims and payments. From the start, the project was hampered by unforeseen problems and delays. As the company fell further behind schedule and budget, it struggled, vainly, to stem an ever rising flood of paperwork. When, on October 27, 1997, Oxford disclosed that its system and its accounts were in disarray, the company's stock price dropped 63%, destroying more than $3 billion in shareholder value in a single day."
9f93e7c0c541c3055302101f3b3d9bc103d1760c,
a5d7d759a7a750a9ba9572dc6277f53601ebcf69,
a63282b8c944675d75f985762f130352916a2fce,
d1022707afb2428196aedb817b781f0b2be4fef1,"My first exposure to the psychological assumptions of economics was in a report that Bruno Frey wrote on that subject in the early 1970's. Its first or second sentence stated that the agent of economic theory is rational and selfish, and that his tastes do not change. I found this list quite startling, because I had been professionally trained as a psychologist not to believe a word of it. The gap between the assumptions of our disciplines appeared very large indeed. Has the gap been narrowed in the intervening 30 years? A search through some introductory textbooks in economics indicates that if there has been any change, it has not yet filtered down to that level: the same assumptions are still in place as the cornerstones of economic analysis. However, a behavioral approach to economics has emerged in which the assumptions are not held sacrosanct. In the following I comment selectively on the developments with regard to the three assumptions, on both sides of the disciplinary divide."
d1705ed0ac5c8eb5b86d7704a2ceab52e91eecf4,"The evidence is disturbingly clear: Most major business initiatives--mergers and acquisitions, capital investments, market entries--fail to ever pay off. Economists would argue that the low success rate reflects a rational assessment of risk, with the returns from a few successes outweighing the losses of many failures. But two distinguished scholars of decision making, Dan Lovallo of the University of New South Wales and Nobel laureate Daniel Kahneman of Princeton University, provide a very different explanation. They show that a combination of cognitive biases (including anchoring and competitor neglect) and organizational pressures lead managers to make overly optimistic forecasts in analyzing proposals for major investments. By exaggerating the likely benefits of a project and ignoring the potential pitfalls, they lead their organizations into initiatives that are doomed to fall well short of expectations. The biases and pressures cannot be escaped, the authors argue, but they can be tempered by applying a very different method of forecasting--one that takes a much more objective ""outside view"" of an initiative's likely outcome. This outside view, also known as reference-class forecasting, completely ignores the details of the project at hand; instead, it encourages managers to examine the experiences of a class of similar projects, to lay out a rough distribution of outcomes for this reference class, and then to position the current project in that distribution. The outside view is more likely than the inside view to produce accurate forecasts--and much less likely to deliver highly unrealistic ones, the authors say."
db5b67fd43234acf2bf27f373fba00f4d78149a7,"The author's personal history of the research that led to his recognition in economics is described, focusing on the process of collaboration and on the experience of controversy. The author's collaboration with Amos Tversky dealt with 3 major topics: judgment under uncertainty, decision making, and framing effects. A subsequent collaboration, with the economist Richard Thaler, played a role in the development of behavioral economics. Procedures to make controversies more productive and constructive are suggested."
ff43f020ce384fd9de3ff895fd09c101ff401e9b,"Recent research has demonstrated that aspiring to the American Dream of financial success has negative consequences for various aspects of psychological well-being. The present longitudinal study examining the relation between the goal for financial success, attainment of that goal, and satisfaction with various life domains found that the negative impact of the goal for financial success on overall life satisfaction diminished as household income increased. The negative consequences of the goal for financial success seemed to be limited to those specific life domains that either concerned relationships with other people or involved income-producing activities, such as one's job; satisfactions with two of those life domains, however, were among the strongest predictors of overall life satisfaction in this sample of well-educated respondents in their late 30s. The negative consequences were particularly severe for the domain of family life; the stronger the goal for financial success, the lower the satisfaction with family life, regardless of household income."
18810500ab837ea4c2c4eb886d04f816ab770c5b,"The work cited by the Nobel committee was done jointly with the late Amos Tversky (1937-1996) during a long and unusually close collaboration. Together, we explored the psychology of intuitive beliefs and choices and examined their bounded rationality. This essay presents a current perspective on the three major topics of our joint work: heuristics of judgment, risky choice, and framing effects. In all three domains we studied intuitions - thoughts and preferences that come to mind quickly and without much reflection. I review the older research and some recent developments in light of two ideas that have become central to social-cognitive psychology in the intervening decades: the notion that thoughts differ in a dimension of accessibility - some come to mind much more easily than others - and the distinction between intuitive and deliberate thought processes."
2884ba19144abf0d527ca7e8a3564301cf3aaccf,"When people make moral or legal judgments in isolation, they produce a pattern of outcomes that they would themselves reject, if only they could see that pattern as a whole. A major reason is that human thinking is category-bound. When people see a case in isolation, they spontaneously compare it to other cases that are mainly drawn from the same category of harms. When people are required to compare cases that involve different kinds of harms, judgments that appear sensible when the problems are considered separately often appear incoherent and arbitrary in the broader context. Another major source of incoherence is what we call the translation problem: The translation of moral judgments into the relevant metrics of dollars and years is not grounded in either principle or intuition, and produces large differences among people.. The incoherence produced by category-bound thinking is illustrated by an experimental study of punitive damages and contingent valuation. We also show how category-bound thinking and the translation problem combine to produce anomalies in administrative penalties. The underlying phenomena have large implications for many topics in law, including jury behavior, the valuation of public goods, punitive damages, criminal sentencing, and civil fines. We consider institutional reforms that might overcome the problem of predictably incoherent judgments. Connections are also drawn to several issues in legal theory, including valuation of life, incommensurability, and the aspiration to global coherence in adjudication."
2d7afdb1cdde261ad13b042e10c64cc8dd54e365,
3b68b46ddf526a5fac17052237930500b9c73867,"For their valuable comments, we are most grateful to Cary Coglianese,l Mark Kelman,2 and the team of Theodore Eisenberg, Jeffrey Rachlinski, and Martin Wells (hereafter ERW that Coglianese has elaborated helpfully on the problem of achieving coherence in civil penalties; and that Kelman's basic arguments should not be taken to dispute our claims, which are less different from his own than a reader of his commentary might infer. Apart from these points, which we elaborate below, we offer a more general suggestion, one that is perhaps insufficiently emphasized in our main paper: Even if it is demonstrated, incoherence usually is not taken to be outrageous. The persistence of incoherence, in many domains of the law, is closely connected with this psychological fact."
4069615a36c33e61ca309b8ceaeb628a10d441b5,"The program of research now known as the heuristics and biases approach began with a survey of 84 participants at the 1969 meetings of the Mathematical Psychology Society and the American Psychological Association (Tversky & Kahneman, 1971). The respondents, including several authors of statistics texts, were asked realistic questions about the robustness of statistical estimates and the replicability of research results. The article commented tongue-in-heek on the prevalence of a belief that the law of large numbers applies to small numbers as well: Respondents placed too much confidence in the results of small samples, and their statistical judgments showed little sensitivity to sample size. The mathematical psychologists who participated in the survey not only should have known better – they did know better. Although their intuitive guesses were off the mark, most of them could have computed the correct answers on the back of an envelope. These sophisticated individuals apparently had access to two distinct approaches for answering statistical questions: one that is spontaneous, intuitive, effortless, and fast; and another that is deliberate, rule-governed, effortful, and slow. The persistence of large biases in the guesses of experts raised doubts about the educability of statistical intuitions. Moreover, it was known that the same biases affect choices in the real world, where researchers commonly select sample sizes that are too small to provide a fair test of their hypotheses (Cohen, 1969, 1992)."
479dc5d3d6e1bccbac03cf35e643857b687a26b2,
609883362f989eb268d37db42bc09f1478c800a8,
7e0b715b6c49e68c516d5658d5ddad3ac993202a,"A review is presented of the book “Heuristics and Biases: The Psychology of Intuitive Judgment,” edited by Thomas Gilovich, Dale Griffin, and Daniel Kahneman."
856c11213b83c221fe1001eea3fc19e95d2f74d8,"Until recently, economics was widely regarded as a non-experimental science that had to rely on observation of real-world economies rather than controlled laboratory experiments. Many commentators also found restrictive the common assumption of a homo oeconomicus motivated by self-interest and capable of making rational decisions. But research in economics has taken off in new directions. A large and growing body of scientific work is now devoted to the empirical testing and modification of traditional postulates in economics, in particular those of unbounded rationality, pure self-interest, and complete self-control. Moreover, today’s research increasingly relies on new data from laboratory experiments rather than on more traditional field data, that is, data obtained from observations of real economies. This recent research has its roots in two distinct, but converging, traditions: theoretical and empirical studies of human decision-making in cognitive psychology, and tests of predictions from economic theory by way of laboratory experiments. Today, behavioral economics and experimental economics are among the most active fields in economics, as measured by publications in major journals, new doctoral dissertations, seminars, workshops and conferences. This year’s laureates are pioneers of these two fields of research."
92bac34b13f5990617cc53c2a3df761001a6d6e8,
9b6f7c265ea03331efa656c7bce82f73ce706399,"Uncertainty is an unavoidable aspect of the human condition. Many significant choices must be based on beliefs about the likelihood of such uncertain events as the guilt of a defendant, the result of an election, the future value of the dollar, the outcome of a medical operation, or the response of a friend. Because we normally do not have adequate formal models for computing the probabilities of such events, intuitive judgment is often the only practical method for assessing uncertainty. The question of how lay people and experts evaluate the probabilities of uncertain events has attracted considerable research interest. (See, e.g., Einhorn & Hogarth, 1981; Kahneman, Slovic, & Tversky, 1982; Nisbett & Ross, 1980.) Much of this research has compared intuitive inferences and probability judgments to the rules of statistics and the laws of probability. The student of judgment uses the probability calculus as a standard of comparison much as a student of perception might compare the perceived size of objects to their physical sizes. Unlike the correct size of objects, however, the “correct” probability of events is not easily defined. Because individuals who have different knowledge or hold different beliefs must be allowed to assign different probabilities to the same event, no single value can be correct for all people. Furthermore, a correct probability cannot always be determined, even for a single person. Outside the domain of random sampling, probability theory does not determine the probabilities of uncertain events – it merely imposes constraints on the relations among them."
b7cec729a186c5c6b6df8bc13f11cbdc82898e8b,
b7da416dc9478c2b8215bf3ec56eacd175a03ee4,
c708aea54904d3bbef4d87c98c4d29cfa2f00908,
f4bbdf6b2bf7db7c9aa404a16241f7fc29304a16,
fbd956fd3102575c42edc10f0431e316639afff0,"Interview with the 2002 Laureates in Economics, Daniel Kahneman and Vernon L. Smith, December 12, 2002. Interviewers are Professor Karl-Gustaf Loefgren and Dr Anne-Sophie Crepin."
0f1fbabbb6e195ffecd16e40f7a366615c9607e2,
375863eeec6f83445ce6176a60055e06325310e6,
54e78ef9813742ce59c47c186614a54879f899b2,
9450d18834097b6d0d0faf3918917641b55cc46e,
ad956432cd131ec0ccf5bd11354d3f4a799d37f0,"The present article offers an approach to scientific debate called adversarial collaboration. The approach requires both parties to agree on empirical tests for resolving a dispute and to conduct these tests with the help of an arbiter. In dispute were Hertwig's claims that frequency formats eliminate conjunction effects and that the conjunction effects previously reported by Kahneman and Tversky occurred because some participants interpreted the word “and” in “bank tellers and feminists” as a union operator. Hertwig proposed two new conjunction phrases, “and are” and “who are,” that would eliminate the ambiguity. Kahneman disagreed with Hertwig's predictions for “and are,” but agreed with his predictions for “who are.” Mellers served as arbiter. Frequency formats by themselves did not eliminate conjunction effects with any of the phrases, but when filler items were removed, conjunction effects disappeared with Hertwig's phrases. Kahneman and Hertwig offer different interpretations of the findings. We discuss the benefits of adversarial collaboration over replies and rejoinders, and present a suggested protocol for adversarial collaboration."
01293224de1a63f7bc917b5154b816a7db00918a,"Contrary to theoretical expectations, measures of willingness to accept greatly exceed measures of willingness to pay. This paper reports several experiments that demonstrate that this ""endowment effect"" persists even in market settings with opportunities to learn. Consumption objects (e.g., coffee mugs) are randomly given to half the subjects in an experiment. Markets for the mugs are then conducted. The Coase theorem predicts that about half the mugs will trade, but observed volume is always significantly less. When markets for ""induced-value"" tokens are conducted, the predicted volume is observed, suggesting that transactions costs cannot explain the undertrading for consumption goods."
1740d9e59f25046557059452dcfa105b838cc14b,
2093c2cb177612467fc83c69abcfbe3f55d01cfe,
321964da31961673a965cc4ee31e0a183893ed4f,
38b9be02c40a1cc95fa678f953ef128bc6c60c70,"Retrospective evaluations of aversive episodes were studied in the context of a general model of ""judgment by prototype"" that has been applied in other situations. Unpleasant sounds of variable loudness and duration were the stimuli. In Experiment 1, continuous reports of annoyance closely tracked variations of noise intensity. Hypotheses about the determinants of retrospective evaluation were examined in Experiment 2. Experiment 3 confirmed a prediction of judgment by prototype: The effects of sound duration and intensity are additive in multitrial experiments. Experiment 4 confirmed a robust preference for aversive episodes that are ""improved"" by adding a period of reduced aversiveness."
44eab3013cb6c63a534570994c9cffe3935ec7ed,"We discuss the cognitive and the psy- chophysical determinants of choice in risky and risk- less contexts. The psychophysics of value induce risk aversion in the domain of gains and risk seeking in the domain of losses. The psychophysics of chance induce overweighting of sure things and of improbable events, relative to events of moderate probability. De- cision problems can be described or framed in multiple ways that give rise to different preferences, contrary to the invariance criterion of rational choice. The pro- cess of mental accounting, in which people organize the outcomes of transactions, explains some anomalies of consumer behavior. In particular, the acceptability of an option can depend on whether a negative outcome is evaluated as a cost or as an uncompensated loss. The relation between decision values and experience values is discussed. Making decisions is like speaking prose—people do it all the time, knowingly or unknowingly. It is hardly surprising, then, that the topic of decision making is shared by many disciplines, from mathematics and statistics, through economics and political science, to sociology and psychology. The study of decisions ad- dresses both normative and descriptive questions. The normative analysis is concerned with the nature of rationality and the logic of decision making. The de- scriptive analysis, in contrast, is concerned with peo- ple's beliefs and preferences as they are, not as they should be. The tension between normative and de- scriptive considerations characterizes much of the study of judgment and choice. Analyses of decision making commonly distin- guish risky and riskless choices. The paradigmatic example of decision under risk is the acceptability of a gamble that yields monetary outcomes with specified probabilities. A typical riskless decision concerns the acceptability of a transaction in which a good or a service is exchanged for money or labor. In the first part of this article we present an analysis of the cog- nitive and psychophysical factors that determine the value of risky prospects. In the second part we extend this analysis to transactions and trades. Risky Choice Risky choices, such as whether or not to take an umbrella and whether or not to go to war, are made without advance knowledge of their consequences. Because the consequences of such actions depend on uncertain events such as the weather or the opponent's resolve, the choice of an act may be construed as the acceptance of a gamble that can yield various out- comes with different probabilities. It is therefore nat- ural that the study of decision making under risk has focused on choices between simple gambles with monetary outcomes and specified probabilities, in the hope that these simple problems will reveal basic at- titudes toward risk and value. We shall sketch an approach to risky choice that"
701d0fc264a49f8f191110b3a27d631917e4299d,"We develop a positive behavioral portfolio theory and explore its implications for portfolio construction and security design. Portfolios within the behavioral framework resemble layered pyramids. Layers are associated with distinct goals and covariances between layers are overlooked. We explore a simple two-layer portfolio. The downside protection layer is designed to prevent financial disaster. The upside potential layer is designed for a shot at becoming rich. Behavioral portfolio theory has predictions that are distinct from those of meanvariance portfolio theory. In particular, behavioral portfolio theory is consistent with the reluctance to have short and margined positions, an inverse relation between the bond/stock ratio and portfolio riskiness, the existence of the home bias, the use of labels such as “growth” and “income,” the preference for securities with floors on returns, and the purchase of lottery tickets."
8e0630885c18859f2cb65718baf20c500d1a6618,"The target article focuses exclusively on System 2 and on reasoning rationality: the ability to reach valid conclusions from available information, as in the Wason task. The decision-theoretic concept of coherence rationality requires beliefs to be consistent, even when they are assessed one at a time. Judgment heuristics belong to System 1, and help explain the incoherence of intuitive beliefs."
94c3dc7b21b1d0fc01d188863fb107868a8285dc,
8fc287987d7cef35cd7b3ca155dddf83fe826e34,
9d8c9b836a95c7fdfb94e0f707a629a3fa4c3c9a,
cbe142daeab5e7ffea53c39ce692b9ce56eef90f,"Recent research has demonstrated that people care about the temporal relationships within a sequence of experiences. There is considerable evidence that people pay particular attention to the way experiences improve or deteriorate over time and to their maximum (peak) and final values. D. Kahneman and coauthors suggested in earlier articles that people ignore or severely underweight duration (which they referred to as duration neglect). In the preceding article, D. Ariely and G. Loewenstein (2000) challenged the generalizability of these findings and their normative implications. In the current commentary, D. Ariely, D. Kahneman, and G. Loewenstein jointly examine the issue to provide a better understanding of what they feel they have learned from this literature and to discuss the remaining open questions."
d2dbd71d6abda09b64c799cb7099b511497bc18c,"How does jury deliberation affect the pre-deliberation judgments of individual jurors? In this paper we make progress on that question by reporting the results of a study of over 500 mock juries composed of over 3000 jury eligible citizens. Our principal finding is that with respect to dollars, deliberation produces a ""severity shift,"" in which the jury's dollar verdict is systematically higher than that of the median of its jurors' predeliberation judgments. A ""deliberation shift analysis"" is introduced to measure the effect of deliberation. The severity shift is attributed to a ""rhetorical asymmetry,"" in which arguments for higher awards are more persuasive than arguments for lower awards. When judgments are measured not in terms of dollars but on a rating scale of punishment severity, deliberation increased high ratings and decreased low ratings. We also find that deliberation does not alleviate the problem of erratic and unpredictable individual dollar awards, but in fact exacerbates it. Implications for punitive damage awards and deliberation generally are discussed."
da4e6db21a74dede02cfe759104b949c7cb0c40e,Two studies test whether people believe in optimal deterrence. The first provides people with personal injury cases that are identical except for variations in the probability of detection and explores whether lower probability cases produce higher punitive damage awards and whether higher probability cases produce lower awards. No such effect is observed. The second asks people whether they agree or disagree with administrative and judicial policies that increase penalties when the probability of detection is low and decrease penalties when the probability of detection is high. Substantial majorities reject these administrative and judicial policies. Policy implications for the role of the jury in achieving deterrence are explored.
e04d5fb73fa4e97a251b180ea33a10acfe78c394,
ffc8dcb3fd207676f7e6dd0515755e11eee5ed03,"This study extends Loewenstein's (1987) notions of savoring and dread to the domain of uncertainty. Measures of attractiveness and of willingness to delay the resolution of uncertainty were obtained for 16 two-outcome gambles with expected value of $1000 and for reflected versions of the same gambles. In both sets, the correlation between the means of the two measures was almost perfect. Positively skewed gambles were most attractive, and associated with the highest tolerance for delayed resolution. A measure of willingness to pay for early resolution showed a similar pattern. Copyright (C) 2000 John Wiley & Sons, Ltd. Language: en"
3b6b94dafaf23d8063b4913f8d24fa7a3944e59a,"Abstract Evidence exists that the intention to perform certain cognitive tasks activates, unintentionally, competing responses and computations that intrude on the performance of the intended tasks. For the intended task to be performed effectively, such intrusions must be controlled. Two experiments were carried out to test the hypothesis that stress heightens the difficulty of exercising effective control over erroneous competing responses, a possible explanation of decrements in the performance of cognitive tasks under stress. Participants performed four tasks, which contained features that could potentially prime or activate erroneous responses. The results demonstrated that the interference of these features with performance was more pronounced among stressed than among less-stressed participants. The need for a more comprehensive theory of the effects of stress on information processing is discussed."
ae247baf220731d6a7c3196cb10d183b24e91639,
b07277ce1f1878df6863f796f623db2048100ce9,"Data from several experiments show that, contrary to traditional models of variety seeking, individuals choose to switch to less-preferred options even though they enjoy those items less than they would have enjoyed repeating a more-preferred option. Two explanations for this finding are tested. Results indicate no evidence of a benefit to more-preferred options due to the contrast to less-preferred alternatives. However, the results of three studies suggest that retrospective global evaluations favor varied sequences that also include less-preferred items as opposed to sequences that only include more-preferred items, even though these more varied sequences result in diminished enjoyment during consumption. Copyright 1999 by the University of Chicago."
da64eadd60b980e87480d510c85bb990e6439ab3,"How does jury deliberation affect the pre-deliberation judgments of individual jurors? Do deliberating juries reduce or eliminate the erratic and unpredictable punitive damage awards that have been observed with individual jurors? In this paper we make progress on these two questions, in part by reporting the results of a study of over 500 mock juries composed of over 3000 jury eligible citizens. Our principal finding is that juries did not produce less erratic and more predictable awards than individuals, but actually made the problem worse, by making large awards much larger and small awards smaller still, even for the same case. Thus, a key effect of deliberation is often to polarize individual judgments, a pattern that has been found in many other group decision making contexts. This finding of polarization--the first of its kind in the particular context of punitive damage awards--has important implications for jury awards involving both punitive and compensatory damages, and raises questions about the common belief that groups, and in particular juries, generally make better decisions than individuals."
eaee54b87f78c25c863fb2f8983dd842c6301c21,
09f86a3ff4419386209086f1285d369661fab9b7,"Beliefs, preferences, and biases investment advisors should know about."
18109b8b58c627d6cc8e486eb10e84af688dcb06,
6843cca1a7ee9036de717a505abf1a6ef66540c0,
82d4b64fad428f7ff18b70f45f9602fbefd3f357,"Speeded choice responses (reading or naming) to a relevant stimulus under conditions of spatial uncertainty are delayed by the simultaneous occurrence of other events. This ""filtering cost"" occurs despite high discriminability of target and distractors, which allows parallel detection of the target in search through the same displays. Reading is also delayed when the removal of irrelevant objects from the field coincides with the onset of the target. Filtering costs are caused by the processing of events rather than by the mere presence of irrelevant items. They are eliminated by advance information about the location of the target or by advance presentation of maintained distractors."
32ed644381c4f86a11c960d98f920dfa44135995,"Different interpretations of an apparent temporal pattern to the experience of regret were addressed through joint research. T. Gilovich and V. H. Medvec (1995a) argued that people regret actions more in the short term and inactions more in the long run because the sting of regrettable action diminishes relatively quickly, whereas the pain of regrettable inaction lingers longer. D. Kahneman (1995) disagreed, arguing that people's long-term regrets of inaction are largely wistful and therefore not terribly troublesome. Three studies that examined the emotional profile of action and inaction regrets established considerable common ground. Action regrets were found to elicit primarily ""hot"" emotions (e.g., anger), and inaction regrets were found to elicit both feelings of wistfulness (e.g., nostalgia) and despair (e.g., misery). Thus, some inaction regrets are indeed wistful (as Kahneman argued), whereas others are troublesome (as Gilovich and Medvec maintained)."
42033d91e869f867dccfafe4b9d537ee7319d1bc,"Although legal scholars have disagreed about whether juries should be allowed to award punitive damages and about how judges should instruct them, the debate has included little discussion of jurors' cognitive capabilities. In this Article, Professors Sunstein, Kahneman, and Schkade respond to this gap by offering an experimental study. The study seeks to separate the tasks that a jury is suited to perform from those that a jury can accomplish only with great inconsistency. In personal injury cases, the study shows, jurors' normative judgments about outrageousness and appropriate punishment are relatively uniform, at least when measured on a bounded numerical scale (0 to 6). Indeed, these normative judgments are uniform across race, age, education, wealth, and gender When subjects map their judgments onto an unbounded dollar scale, however outcomes become erratic and unpredictable. Drawing on these results, the authors question the current legal approaches to the regulation of punitive damages. They then analyze various reform proposals designed to overcome erratic awards, including damage caps, compensatory judgement ""multipliers,"" and conversion formulas that translate either jury judgments on bounded numerical scabs or jury arrangement of comparison cases into punitive damage awards. Finally the authors discuss the implications of the study for many other issues of law: including contingent valuation and compensatory damages in such areas as pain and suffering, libel, sexual harassment, and intentional infliction of emotional distress. Language: en"
49c3c52c5f8a3ef7a41e642499ede1651a57806d,
8f9aedc0dc6d6561a93fd5eb7b840a57ecc1465f,
926ca699e972892dae5c90d2f0a7c6743d811178,"Large samples of students in the Midwest and in Southern California rated satisfaction with life overall as well as with various aspects of life, for either themselves or someone similar to themselves in one of the two regions. Self-reported overall life satisfaction was the same in both regions, but participants who rated a similar other expected Californians to be more satisfied than Midwesterners. Climate-related aspects were rated as more important for someone living in another region than for someone in one's own region. Mediation analyses showed that satisfaction with climate and with cultural opportunities accounted for the higher overall life satisfaction predicted for Californians. Judgments of life satisfaction in a different location are susceptible to a focusing illusion: Easily observed and distinctive differences between locations are given more weight in such judgments than they will have in reality."
ca110d84810f7cf1fe350ee1a8ed9bf154dcfd11,
dc7577ca2bdea29c00911e440067763547534f0b,
c737b098f62d4ba9c00354e63be857363d3e6fda,"This essay reports and discusses the implications of an experimental study involving punitive damage awards. The study finds that in products liability cases, people's normative judgments (about outrageousness and appropriate punishment) are relatively uniform, at least when measured on a bounded numerical scale (0 to 6). With the unbounded dollar scale, however, outcomes become extremely erratic and unpredictable. Various reform proposals, designed to overcome erratic awards, are discussed, including damage caps, compensatory judgment ""multipliers,"" and conversion formulas based on jury judgments on a bounded numerical scale. Implications are also discussed for many other issues of law and economic valuation, including compensatory damages in such areas as pain and suffering, libel, sexual harassment and other civil rights violations, contingent valuation, and intentional infliction of emtional distress."
ce8d8cdbd39f736296fad2098a5647a04f13f3af,
92ceff77ba7386572c02e31f75ce0b937efe23ed,"The study of heuristics and biases in judgement has been criticized in several publications by G. Gigerenzer, who argues that ""biases are not biases"" and ""heuristics are meant to explain what does not exist"" (1991, p. 102). The article responds to Gigerenzer's critique and shows that it misrepresents the authors' theoretical position and ignores critical evidence. Contrary to Gigerenzer's central empirical claim, judgments of frequency--not only subjective probabilities--are susceptible to large and systematic biases. A postscript responds to Gigerenzer's (1996) reply."
ee450fa573c9f012a6837df63abdd5b0c2cb49f8,
64909c17d105f792de688593d77e7ed9803a2082,
9ce73a8c5a995eb6f7e3594a406f7e9b48632315,
c62fe40d2f89151a323489b83b4f3efbae98fc50,"The authors describe a method for the quantitative study of anchoring effects in estimation tasks. A calibration group provides estimates of a set of uncertain quantities. Subjects in the anchored condition first judge whether a specified number (the anchor) is higher or lower than the true value before estimating each quantity. The anchors are set at predetermined percentiles of the distribution of estimates in the calibration group (15th and 85th percentiles in this study). This procedure permits the transformation of anchored estimates into percentiles in the calibration group, allows pooling of results across problems, and provides a natural measure of the size of the effect. The authors illustrate the method by a demonstration that the initial judgment of the anchor is susceptible to an anchoring-like bias and by an analysis of the relation between anchoring and subjective confidence."
8c22e21f65db58903e7a8db602eaf62b2651632d,"A color word shown next to a color bar can facilitate color naming if it is congruent with the correct response; otherwise it will interfere with color naming. The congruence and conflict effects are both diminished (diluted) by the presentation of a color-neutral word elsewhere in the field. A row of X's also produces some dilution. The dilution effect represents attentional interference rather than sensory interaction or response conflict. Because Stroop effects are susceptible to interference, the involuntary reading of color words does not satisfy one of the standard criteria of automaticity."
5d445e5d8c1d95741f7055c2f4f27d5556cfa3aa,
6abc621e70c17217d32fa5a9a4d2a6514912778e,"This article examines the sensitivity of survey measures of willingness to pay for public goods. Visitors to a science museum in San Francisco were asked to provide estimates of their willingness to pay for saving seabirds from oil spills and for teaching English to immigrants under various experimental conditions. Willingness to pay was substantially reduced by a seemingly innocuous reminder about how many individuals would be affected by a tax or would be asked to contribute to a given cause. This finding, which cannot be explained by standard economic interpretations of willingness to pay, is consistent with previous studies showing that subtle changes in question order and wording can affect the nature of the responses. Language: en"
24572d4c8b589101164ad57a226e2618ace03b7b,"Subjects were exposed to two aversive experiences: in the short trial, they immersed one hand in water at 14 °C for 60 s; in the long trial, they immersed the other hand at 14 °C for 60 s, then kept the hand in the water 30 s longer as the temperature of the water was gradually raised to 15 °C, still painful but distinctly less so for most subjects. Subjects were later given a choice of which trial to repeat. A significant majority chose to repeat the long trial, apparently preferring more pain over less. The results add to other evidence suggesting that duration plays a small role in retrospective evaluations of aversive experiences; such evaluations are often dominated by the discomfort at the worst and at the final moments of episodes."
75f8a9976bcee53ebd10714de7a4220266cfcc33,"Many decisions are based on beliefs concerning the likelihood of uncertain events such as the outcome of an election, the guilt of a defendant, or the future value of the dollar. These beliefs are usually expressed in statements such as ""1 think that . . .,"" ""chances are . . .,"" ""it is unlikely that . . .,"" and so forth. Occasionally, beliefs concerning uncertain events are expressed in numerical form as odds or subjective probabilities. What determines such beliefs? How do people assess the probability of an uncertain event or the value of an uncertain quantity? This article shows that people rely on a limited number of heuristic principles which reduce the complex tasks of assessing probabilities and predicting values to simpler judgmental operations. In general, these heuristics are quite useful, but sometimes they lead to severe and systematic errors. The subjective assessment of probability resembles the subjective assessment of physical quantities such as distance or size. These judgments are all based on data of limited validity, which are processed according to heuristic rules. For example, the apparent distance of an object is determined in part by its clarity. The more sharply the object is seen, the closer it appears to be. This rule has some validity, because in any given scene the more distant objects are seen less sharply than nearer objects. However, the reliance on this rule leads to systematic errors in the estimation of distance. Specifically, distances are often overestimated when visibility is poor because the contours of objects are blurred. On the other hand, distances are often underestimated when visibility is good because the objects are seen sharply. Thus, the reliance on clarity as an indication of distance leads to common biases."
85c233d9075c32a1eafebb14360c320e74b7ef5b,"Two experiments documented a phenomenon of duration neglect in people's global evaluations of past affective experiences. In Study 1, 32 Ss viewed aversive film clips and pleasant film clips that varied in duration and intensity. Ss provided real-time ratings of affect during each clip and global evaluations of each clip when it was over. In Study 2, 96 Ss viewed these same clips and later ranked them by their contribution to an overall experience of pleasantness (or unpleasantness). Experimental Ss ranked the films from memory; control Ss were informed of the ranking task in advance and encouraged to make evaluations on-line. Effects of film duration on retrospective evaluations were small, entirely explained by changes in real-time affect and further reduced when made from memory. Retrospective evaluations appear to be determined by a weighted average of ""snapshots"" of the actual affective experience, as if duration did not matter."
9a7102ab38b70e6bebdee2e753bc55c6225a2e86,"In Reply. —We are grateful to Drs Shapiro and Muskin for pointing out that we did not discuss cognitive impairment or psychiatric disorders. We agree that some harmful decisions originate in emotional disturbances and pathological thought processes. Statistics on suicide rates, for example, poignantly document the consequences of depression. However, readers should not conclude that problems in judgment and decision making are confined to patients with neurological or psychiatric disorders. Our aim in the article was to emphasize that normal human intuitions are prone to predictable errors. Clinicians should be alert to the harmful mistakes that reasonable patients are likely to make in evaluating risks and benefits."
9ff8e1494c49d300b76f2cdab09a6488de00dcb5,"Decision makers have a strong tendency to consider problems as unique. They isolate the current choice from future opportunities and neglect the statistics of the past in evaluating current plans. Overly cautious attitudes to risk result from a failure to appreciate the effects of statistical aggregation in mitigating relative risk. Overly optimistic forecasts result from the adoption of an inside view of the problem, which anchors predictions on plans and scenarios. The conflicting biases are documented in psychological research. Possible implications for decision making in organizations are examined."
e15c3583e87eed5720eea36d4f54c212637cee6e,"OBJECTIVE
To describe ways in which intuitive thought processes and feelings may lead patients to make suboptimal medical decisions.


DESIGN
Review of past studies from the psychology literature.


RESULTS
Intuitive decision making is often appropriate and results in reasonable choices; in some situations, however, intuitions lead patients to make choices that are not in their best interests. People sometimes treat safety and danger categorically, undervalue the importance of a partial risk reduction, are influenced by the way in which a problem is framed, and inappropriately evaluate an action by its subsequent outcome. These strategies help explain examples where risk perceptions conflict with standard scientific analyses. In the domain of emotions, people tend to consider losses as more significant than the corresponding gains, are imperfect at predicting future preferences, distort their memories of past personal experiences, have difficulty resolving inconsistencies between emotions and rationality, and worry with an intensity disproportionate to the actual danger. In general, such intangible aspects of clinical care have received little attention in the medical literature.


CONCLUSION
We suggest that an awareness of how people reason is an important clinical skill that can be promoted by knowledge of selected past studies in psychology."
ef5f6ced6ebb9de603621929090d30273cc5f825,"In the contingent valuation method for the valuation of public goods, survey respondents are asked to indicate the amount they are willing to pay (WTP) for the provision of a good. We contrast economic and psychological analyses of WTP and describe a study in which respondents indicated their WTP to prevent or to remedy threats to public health or to the environment, attributed either to human or to natural causes. WTP was significantly higher when the cause of a harm was human, though the effect was not large. The means of WTP for 16 issues were highly correlated with the means of other measures of attitude, including a simple rating of the importance of the threat. The responses are better described as expressions of attitudes than as indications of economic value, contrary to the assumptions of the contingent valuation method."
48fb7086e2ed00c20009c85c1301a5b2b91ec7af,
4c7296279f4e6c768c0d94b912ced1e656417d07,"Intuitions relating to outcomes extended over time are examined. Utility integration is proposed as a normative rule for the evaluation of extended episodes. In Experiment 1, subjects explicitly compared aversive experiences of varying durations. By several measures, disutility was a marginally decreasing function of episode duration, even for experiences that were thought to become increasingly aversive. This pattern is a qualitative violation of the integration rule. In Experiment 2, subjects made global evaluations of a hypothetical person's aversive experiences, on the basis of a series of subjective ratings of discomfort made at periodic intervals. The results showed an extreme sensitivity to improving or deteriorating trend and a striking neglect of duration. The final moments of an extended episode appear to exert a strong influence on the overall judgment. This leads to violations of monotonicity when adding some moments of moderate pain reduces judgments of global aversiveness."
5c7a7c17cbd1300b69db1263f8b2cd824378ef9c,
693bc740d3b6cd1d1ddee88852deeb99f13eb401,
7d0f7c121a480b15fe7441e8fa8aecb705beafa2,
99a267b4b50f3611b2972671f5f6ed93e9e99b16,"A distinction is made between decision utility, experienced utility, and predicted utility and an experiment is reported addressing people's ability to forecast experienced utility. Subjects in two experiments made predictions of their future liking for stimuli to which they were then exposed daily for one week. The stimuli were ice cream in a pilot study, plain yogurt in the main study, and short musical pieces in both studies. Decreased liking was the modal prediction, even when the true outcome was increased liking, or reduced dislike. There was substantial stability of tastes, but there were also substantial individual differences in the size and even the sign of changes in liking with repeated exposure. There was little or no correlation between the predictions of hedonic change that individuals made and the changes they actually experienced."
9b129513de0da471a204b953528c437e4a4c30ef,
24b3d4d16b6020f333777f0665b25a3498f1ea27,"In many contexts people are required to assess the probability of some target event (e.g., the diagnosis of a patient or the sales of a textbook) on the basis of (a) the base-rate frequency of the target outcome in some relevant reference population (e.g., the frequency of different diagnoses or the distribution of textbook sales), (b) some specific evidence about the case at hand (e.g., the patient's response to a diagnostic test or the table of contents of the text in question). Concern with the role of base-rate data in intuitive predictions about individual cases was expressed by Meehl & Rosen (1955), who argued, using Bayes' rule, that predictions of rare outcome (e.g., suicide) on the basis of fallible data is a major source of error in clinical prediction. Meehl & Rosen did not conduct experimental studies but they cited examples from the literature on clinical diagnosis, in which base-rate information was not taken into account. To obtain an experimental test of the impact of base-rate data, we presented subjects with a description of a graduate student, or a professional, and asked them to predict his field of study or his profession, respectively (Kahneman & Tversky, 1973, 4). These studies showed that posterior probability judgments were determined primarily by the degree to which the description was similar to or representative of the respective professional stereotype (e.g., of librarians or lawyers)."
a89cfc54f3d9a16a12b04870815728ca736283b9,
bf7440b48b35de6593265a72f2fb32a03adcb320,
ee8b05e1756126396c86bc144632a2ffca934c0f,
f4e98f82bb6b958c95cfda697d7ab2bfaf19879d,
5ae8c2bf38d9085f700cad462b2776b597c32c76,
86af5b4ce3324624bbb499eb79ee0901d6375df9,"Much experimental evidence indicates that choice depends on the status quo or reference level: changes of reference point often lead to reversals of preference. We present a reference-dependent theory of consumer choice, which explains such effects by a deformation of indifference curves about the reference point. The central assumption of the theory is that losses and disadvantages have greater impact on preferences than gains and advantages. Implications of loss aversion for economic behavior are considered."
ad2de961cf5485176bb071a8f3d9d9fc1bdd14de,"The large number of textbooks recently published identifies the field of judgment and decision making as one of the areas of psychology in which research activity grew most rapidly during the past two decades. The enthusiasm is easily explained: The topic has much to make it appealing to investigators. Its focus is a large puzzle that will not go away—a search for the bounds of human rationahty. It includes a deep normative theory that offers criteria for rational action. It is also rich in amusing anecdotes and challenging brain teasers. The study of judgment and choice occasionally sheds light on events in the real world, including the decisions of world leaders, the foibles of the market and the pitfalls of medical diagnosis. The doubts that psychologists have raised about the rationality of human agents are having a modest effect on neighboring disciplines, such as economics and political science, in which the assumption of human rationality is often used to predict the outcomes of competitive interaction. The detailed study of bounded rationality also has implications for the human engineering of information systems, decision aids and organizational procedures. The following observations sketch a personal view of this exciting field, its history, accomphshments and limitations, and possible future."
b34bc9e23c88a29a24cc30dd423b1f6215306b7b,"A speaker enclosure which is particularly useful for stereophonic applications, but which also provides superior sound reproduction from monophonic sources. A reduction in size of the usual stereophonic speaker system is accomplished without sacrificing sound quality or stereo effect. Full bass reproduction is obtained from a small source, and the reflective properties of room surfaces, such as the walls and the ceiling, are taken advantage of, to enhance the sound dispersion capabilities of the system by reflective reinforcement."
be71f1bc428da3147efbe25db8b7ff4829a3f0a0,
f6d50db2c937f71f96ab4e8bdd1c85e38c42aadd,"A wine-loving economist we know purchased some nice Bordeaux wines years ago at low prices. The wines have greatly appreciated in value, so that a bottle that cost only $10 when purchased would now fetch $200 at auction. This economist now drinks some of this wine occasionally, but would neither be willing to sell the wine at the auction price nor buy an additional bottle at that price. Thaler (1980) called this pattern—the fact that people often demand much more to give up an object than they would be willing to pay to acquire it—the endowment effect. The example also illustrates what Samuelson and Zeckhauser (1988) call a status quo bias, a preference for the current state that biases the economist against both buying and selling his wine. These anomalies are a manifestation of an asymmetry of value that Kahneman and Tversky (1984) call loss aversion—the disutility of giving up an object is greater that the utility associated with acquiring it. This column documents the evidence supporting endowment effects and status quo biases, and discusses their relation to loss aversion."
2a04c1a071a3081c0349c6cd69cdc26cc6d7faa7,
350fda1ed1f795a3957d23bd6d7a69c7d833ec04,"Contrary to theoretical expectations, measures of willingness to accept greatly exceed measures of willingness to pay. This paper reports several experiments that demonstrate that this ""endowment effect"" persists even in market settings with opportunities to learn. Consumption objects (e.g., coffee mugs) are randomly given to half the subjects in an experiment. Markets for the mugs are then conducted. The Coase theorem predicts that about half the mugs will trade, but observed volume is always significantly less. When markets for ""induced-value"" tokens are conducted, the predicted volume is observed, suggesting that transactions costs cannot explain the undertrading for consumption goods."
3634d03eed3f39df9262843a401d17df222099a2,"Observed preference reversal cannot be adequately explained by violations of independence, the reduction axiom, or transitivity. The primary cause of preference reversal is the failure of procedure invariance, especially the overpricing of low-probability, high-payoff bets. This result violates regret theory and generalized (nonindependent) utility models. Preference reversal and a new reversal involving time preferences are explained by scale compatibility, which implies that payoffs are weighted more heavily in pricing than in choice. Copyright 1990 by American Economic Association."
364b5da1861ba28a4bdc05be8fad38f5db7e93fa,
8af7775ab34b4e629bb1491f8128ffa8cc0cdfc8,"Alternative descriptions of a decision problem often give rise to different preferences, contrary to the principle of invariance that underlines the rational theory of choice. Violations of this theory are traced to the rules that govern the framing of decision and to the psychological principles of evaluation embodied in prospect theory. Invariance and dominance are obeyed when their application is transparent and often violated in other situations. Because these rules are normatively essential but descriptively invalid, no theory of choice can be both normatively adequate and descriptively accurate."
f680ba08d0a6107f2c1eb61b796a5970a8465122,"Abstract : In the second year of the grant I continued three projects initiated in the first year, and began two new lines of research. A study of contingent coding in normality judgements yielded disappointing results. We started a systematic exploration of the relation between discriminability and similarity, which we plan to extend to categorization and normality. A series of studies established essentially perfect dimensional independence in object-specific priming. We conducted a theoretical and empirical examination of close counterfactuals. Two separate projects dealt with the process of comparison, continuing and extending work reported last year. Keywords: Perception, (Psychology), Categorization, Comparison processes, Normality."
6bb2fe909f872d2d72d16cf071579c13f698e713,
9350ab13bdf5ec710579de2f8c658d1817a414df,"Abstract : The three-year effort (1984-7) by the two principle investigators is summarized under four major headings: I. The framing of decisions -- Rational decision theory is characterized by two sets of assumptions: coherence and invariance. The requirements of coherence include transitivity, substitution (or cancellation), and stochastic dominance, which are generally taken as the basic axioms of the theory. In modern decision theory, the relation of preference appears as a primitive concept that is interpreted through specific empirical procedures such as binary choice, matching or pricing. The standard theory of choice assumes procedure invariance: normatively equivalent procedures for assessing preferences should give rise to the same preference order. Empirical studies of decision and judgement, however, indicate that procedure invariance is not generally satisfied and that observed choice often depends on the method of elicitation. Our analysis of risky choice assumes that the outcomes of decisions are naturally treated as gains and losses in relation to some neutral reference level. We assumed that prospects are evaluated by invoking a value function, defined on gains and convex in the domain of losses. One of the developments in the study of judgement during the current grant period was the formulation of norm theory. The central idea of this theory is that a memory probe can cause multiple representations of objects or events to be evoked, that these representations can be aggregated, and that the aggregates can be'read' and used for various judgment tasks."
a3ef0c7a1afbb4c1e13be1a41992e9d241c08635,
533136d0579feb79d9c6171b39172b26b1321a49,
6bd8eda6ca8ae5728ae6cf14e57862434488a812,"Community standards of fairness for the setting of prices and wages were elicited by telephone surveys. In customer or labor markets it isacceptable for a firm to raise prices (or cut wages) when profits arethreatened, and to maintain prices when costs diminish. It is unfair toexploit shifts in demand by raising prices or cutting wages. Several market anomalies are explained by assuming that these standards of fairness influence the behavior of firms. Copyright 1986 by American Economic Association."
84d9d84f0bb5f3b4610b715d33618d15aa458e39,
98098ee48700173e2f09aeff48c406ef943918b5,"A theory of norms and normality is presented and applied to some phenomena of emotional responses, social judgment, and conversations about causes. Norms are assumed to be constructed ad hoc by recruiting specific representations. Category norms are derived by recruiting exemplars. Specific objects or events generate their own norms by retrieval of similar experiences stored in memory or by construction of counterfactual alternatives. The normality of a stimulus is evaluated by comparing it to the norms that it evokes after the fact, rather than to precomputed expectations. Norm theory is applied in analyses of the enhanced emotional response to events that have abnormal causes, of the generation of predictions and inferences from observations of behavior, and of the role of norms in causal questions and answers. This article is concerned with category norms that represent knowledge of concepts and with stimulus norms that govern comparative judgments and designate experiences as surprising. In the tradition of adaptation level theory (Appley, 1971; Helson, 1964), the concept of norm is applied to events that range in complexity from single visual displays to social interactions. We first propose a model of an activation process that produces norms, then explore the role of norms in social cognition. The central idea of the present treatment is that norms are computed after the event rather than in advance. We sketch a supplement to the generally accepted idea that events in the stream of experience are interpreted and evaluated by consulting precomputed schemas and frames of reference. The view developed here is that each stimulus selectively recruits its own alternatives (Garner, 1962, 1970) and is interpreted in a rich context of remembered and constructed representations of what it could have been, might have been, or should have been. Thus, each event brings its own frame of reference into being. We also explore the idea that knowledge of categories (e.g., ""encounters with Jim"") can be derived on-line by selectively evoking stored representations of discrete episodes and exemplars. The present model assumes that a number of representations can be recruited in parallel, by either a stimulus event or an"
a3d2dcfed78248ac3944c6b00ef97ed36807a1c1,"The advantages and disadvantages of expanding the standard economic model by more realistic behavioral assumptions have received much attention. The issue raised in this article is whether it is useful to complicate-or perhaps to enrichthe model of the profit-seeking firm by considering the preferences that people have for being treated fairly and for treating others fairly. The absence of considerations of fairness and loyalty from standard economic theory is one of the most striking contrasts between this body of theory and other social sciences-and also between economic theory and lay intuitions about human behavior. Actions in many domains commonly conform to standards of decency that are more restrictive than the legal ones: the institutions of tipping and lost-and-found offices rest on expectations of such actions. Nevertheless, the standard microeconomic model of the profitmaximizing firm assigns essentially no role to The traditional assumption that fairness is irrelevant to economic analysis is questioned. Even profit-maximizing firms will have an incentive to act in a manner that is perceived as fair if the individuals with whom they deal are willing to resist unfair transactions and punish unfair firms at some cost to themselves. Three experiments demonstrated that willingness to enforce fairness is common. Community standards for actions affecting customers, tenants, and employees were studied in telephone surveys. The rules of fairness, some of which are not obvious, help explain some anomalous market phenomena. * The research for this paper was supported by the Department of Fisheries and Oceans Canada. Kahneman and Thaler were also supported, respectively, by the U.S. Office of Naval Research and by the Alfred P. Sloan Foundation. Conversations with J. Brander, R. Frank, and A. Tversky were very helpful. We also thank Leslie McPherson and Daniel Treisman for their assistance. The paper presented at the conference and commented on by the discussants included a detailed report of study 3, which is only summarized here. It did not contain study 1, which was incomplete at the time. Daniel Kahneman is now in the Department of Psychology, University of California, Berkeley 94720."
75b37fa03febf06892c4ff3c3c734ba19d8b9b7a,"Chapter 1 consists mostly of puffery about “Total Design Method” (TDM). Dillman implies that TDM is revolutionary in nature and that it helps to improve the effectiveness of mail and telephone surveys “in ways thought impossible only a few years ago” (p. 2). Actually. TDM merely says that many factors affect the success of mail and telephone surveys and that these factors interact with one another. Because of this interaction, he says that “exhaustive reviews of the research, of which there are several, seemed destined to be unhelpful” (p. 7). But Dillman does not provide empirical support to show the importance of interactions. Apparently not much evidence exists on this point. Furthermore, he omits reference to Wiseman (1973), a small empirical study which concluded that factor interaction was unimportant. I did not find the TDM framework to be helpful. Furthermore. I do find exhaustive reviews to be helpful. I recommend that you start the book on page 20. This last part of chapter 1 provides evidence on response rates achieved by surveys using TDM."
f4ef097d5c337e1686a663bfdae386d0df0a9bb6,
f98ebf5fdfff5f0eff32898007312749452127bf,
1ebb4a935a13ce3affd9c7c03ee9de57127683ba,
2ba40149ed0ca41961eb4d28c82af551ad21c968,
3a1bab982fc6f7b1c62f963bc946c2e04c01116b,
40d62424c46fa8b4a54214570bc5da9b7b0642f9,"that can be pressed into service for many tasks, sometimes appropriately, sometimes not. So Cohen attacks heuristics. But is this attack necessary, even from Cohen's own point of view? Why not simply regard the heuristics as tools or subroutines available to more specific mechanisms? I wish to comment also on Cohen's discussion of Bayes' theorem. As I see it, the fundamental difficulty with the Bayesian analyses Cohen criticizes is that they do not allow for an assessment of the reliability and relevance of ""base rates. ' We are not certain that the predominance of blue cabs in the city as a whole is fully relevant to the particular situation. In a recent paper (Shafer 1982) I discuss how base rates can be discounted using the theory of belief functions; it turns out that one obtains the Bayesian answer if the discount rate is sufficiently low, but that even a moderate discount rate can sharply reduce the influence of the base rate in the face of conflicting evidence. Cohen seems to feel that base rates should always be totally discounted, and this is very difficult to sustain. In Cohens medical story, for example, the Bayesian analysis seems compelling if there is no reason to discount the relative rarity of disease B."
15aa7cc63d1d710b4f6153085a5b5a6f1a33dde3,
1744e9a55a7481cb435f6e8259477f5d689b4103,
18bf42f4fe5f93871f7cabbee3e5a9cf3cdc4ec5,
1e8ae3df4e3880119d39694558d3dc9d7b6eae6f,"Our original treatment of the availability heuristic (Tversky & Kahneman, 1973, 11) discussed two classes of mental operations that “bring things to mind”: the retrieval of instances and the construction of examples or scenarios. Recall and construction are quite different ways of bringing things to mind; they are used to answer different questions, and they follow different rules. Past research has dealt mainly with the retrieval of instances from memory, and the process of mental construction has been relatively neglected. To advance the study of availability for construction, we now sketch a mental operation that we label the simulation heuristic. Our starting point is a common introspection: There appear to be many situations in which questions about events are answered by an operation that resembles the running of a simulation model. The simulation can be constrained and controlled in several ways: The starting conditions for a “run” can be left at their realistic default values or modified to assume some special contingency; the outcomes can be left unspecified, or else a target state may be set, with the task of finding a path to that state from the initial conditions. A simulation does not necessarily produce a single story, which starts at the beginning and ends with a definite outcome. Rather, we construe the output of simulation as an assessment of the ease with which the model could produce different outcomes, given its initial conditions and operating parameters."
29d4f697b2fbdbe2330e6dc65595e0c050f89f3b,
3969d703bb8b946c4b9227e8b3046c3e0e54a23e,
4fdf2c75c285a0dfd91ef0ebd492e95eaf0daedb,
5c6002943ed940009fd26e1a2a85b9e1a7022a67,"Stan ford University The study of intuitions and errors in judgment un~‘er umwtainty is complicated by several factors: discrepancies between acceptance and application of normative rules: effects of content on me application of rules; Sucratic hints that create intuitions while testing them; demand characteristics of within-subject experiments; subjects’ interpretations of experimental messages according to standard conversational rules. The positive analysis of a iudgmental error in terms of heuri.stics may be supplemented by a negative analysis, which seeks to explain why the correct rule is not intuitively compelling. A negative analysis of non-regressive prediction is outlined."
5eba9e25e802ef2a7e8eb1d822358345d8c39b31,
67a01f35771e75eee1a3a96bd8c6dce6df56bacd,
67ed8b4dea889ff81e897f89cc89653450f98dde,"Introduction Any significant activity of forecasting involves a large component of judgment, intuition, and educated guesswork. Indeed, the opinions of experts are the source of many technological, political, and social forecasts. Opinions and intuitions play an important part even where the forecasts are obtained by a mathematical model or a simulation. Intuitive judgments enter in the choice of the variables that are considered in such models, the impact factors that are assigned to them, and the initial values that are assumed to hold. The critical role of intuition in all varieties of forecasting calls for an analysis of the factors that limit the accuracy of expert judgments, and for the development of procedures designed to improve the quality of these judgments. … Singular and distributional data Experts are often required to provide a best guess, estimate, or prediction concerning an uncertain quantity such as the value of the Dow-Jones index on a particular day, the future sales of a product, or the outcome of an election. A distinction should be made between two types of information that are available to the forecaster: singular and distributional. Singular information, or case data, consists of evidence about the particular case under consideration. Distributional information, or base-rate data, consists of knowledge about the distribution of outcomes in similar situations."
87c0fcb5763f03bb16a5ce69db08b5e30d34c665,
92b70779f4365115655f44ef6605bae8c818fc1e,
95a0c7fcb2516044f88cc2a14942953fa71786eb,"Abstract : In contrast to the normative theory of evidence, where the impact of data is determined solely by their informativeness, this paper develops the thesis that the impact of evidence on intuitive judgements of probabilities depends critically on whether it is perceived as causal, diagnostic or incidental. The first part of the paper shows that people assign greater impact to causal data than to diagnostic data of equal informativeness. When the same datum has both causal and diagnostic implications, the former dominate the latter. The ease with which people explain unexpected facts and the reluctance to revise old conceptions in the light of new facts are related to the dominance of causal over diagnostic reasoning. The second part of the paper analyzes the use and neglect of base-rate data in terms of the role of these data in causal schemata. It is shown that base-rate information which is given a causal interpretation affects judgments, while base-rate information which cannot be interpreted in this manner is given little or no weight."
a0bf3dd754a62101f2f0b978ae69f6f6cf441d69,"The thirty-five chapters in this book describe various judgmental heuristics and the biases they produce, not only in laboratory experiments but in important social, medical, and political situations as well. Individual chapters discuss the representativeness and availability heuristics, problems in judging covariation and control, overconfidence, multistage inference, social perception, medical diagnosis, risk perception, and methods for correcting and improving judgments under uncertainty. About half of the chapters are edited versions of classic articles; the remaining chapters are newly written for this book. Most review multiple studies or entire subareas of research and application rather than describing single experimental studies. This book will be useful to a wide range of students and researchers, as well as to decision makers seeking to gain insight into their judgments and to improve them."
a3fd66d3405b19f666fcfecb9230430c49e0880a,
ac5a27024f483d92741b9c8ba85368aa9fa93d0c,
bd11ee4d2486aeffa61554a1f82cf974344f9f0d,
cd9263dd7972db41802e21131b45bf402e8941e1,
d52743d82f3eed3f54ce41ca144c99c1909ad2db,
e27dffc70a1cfa841ad0542987f8556b41bfd41d,
816320369b61baa2d7ddc9bd5cc69c1f9ba858ff,
da9011b57dd9b7e1de4f90c3590229d51288ee2f,
dee32cd7bbf54b1c9523cb4d4708b96266523505,"Several years ago, we presented an analysis of judgment under uncertainty that related subjective probabilities and intuitive predictions to expectations and impressions about representativeness. Two distinct hypotheses incorporated this concept: (i) people expect samples to be highly similar to their parent population and also to represent the randomness of the sampling process (Tversky & Kahneman, 1971, 2; 1974, 1); (ii) people often rely on representativeness as a heuristic for judgment and prediction (Kahneman & Tversky, 1972b, 3; 1973, 4). The first hypothesis was advanced to explain the common belief that chance processes are self-correcting, the exaggerated faith in the stability of results observed in small samples, the gambler's fallacy, and related biases in judgments of randomness. We proposed that the lay conception of chance incorporates a belief in the law of small numbers, according to which even small samples are highly representative of their parent populations (Tversky & Kahneman, 1971, 2). A similar hypothesis could also explain the common tendency to exaggerate the consistency and the predictive value of personality traits (Mischel, 1979) and to overestimate the correlations between similar variables (see Chap. 15) and behaviors (Shweder & D'Andrade, 1980). People appear to believe in a hologram-like model of personality in which any fragment of behavior represents the actor's true character (Kahneman & Tversky, 1973, 4)."
e552054dbd030b8414058639389b4a63e727aedb,"The psychological principles that govern the perception of decision problems and the evaluation of probabilities and outcomes produce predictable shifts of preference when the same problem is framed in different ways. Reversals of preference are demonstrated in choices regarding monetary outcomes, both hypothetical and real, and in questions pertaining to the loss of human lives. The effects of frames on preferences are compared to the effects of perspectives on perceptual appearance. The dependence of preferences on the formulation of decision problems is a significant concern for the theory of rational choice."
8093f7795cc9964d1b7a152d15f1ffb2df9ba223,
da37319ba64591bb74488fa341b59ed3e5be7683,"Abstract : The psychological principles that govern individual decision making produce predictable reversals of preferences when the same decision problem is framed in different ways. Inconsistencies are illustrated in choices involving monetary outcomes, both hypothetical and real, and in policy questions that pertain to the loss of human lives. Our analysis questions the descriptive adequacy of the standard rational model and highlights the dependence of the normative theory of choice on the psychology of hedonic experience. (Author)"
29846003fe46053bb7fb056e1f34ed0378110459,
3cef3fc08b6188197eb470d3a0950bdf8999ce49,"A lubricator valve apparatus adapted for use when running wireline tools into an offshore well during a production test of the well. The valve includes a valve body having a central flow passage and a ball valve element for opening and closing the passage, hydraulically operable means responsive to surface-controlled pressure for opening and closing the ball valve, latch means for releasably holding the ball valve in both the open and the closed positions, and bypass valve means for equalizing pressures across the ball valve prior to opening thereof and arranged in the event hydraulic control of the ball valve is lost to be opened in response to pressure applied at the surface to the production pipe to provide a flow path for well control fluids."
62d4ad0356401f179ad26c3f279d1ab6a7c390c9,
a8600a214ebc496d30d2e9f96d00590cfe0b70ff,
e01a13bd46de69254925a674b641145b77417e92,"Analysis of decision making under risk has been dominated by expected utility theory, which generally accounts for people's actions. Presents a critique of expected utility theory as a descriptive model of decision making under risk, and argues that common forms of utility theory are not adequate, and proposes an alternative theory of choice under risk called prospect theory. In expected utility theory, utilities of outcomes are weighted by their probabilities. Considers results of responses to various hypothetical decision situations under risk and shows results that violate the tenets of expected utility theory. People overweight outcomes considered certain, relative to outcomes that are merely probable, a situation called the ""certainty effect."" This effect contributes to risk aversion in choices involving sure gains, and to risk seeking in choices involving sure losses. In choices where gains are replaced by losses, the pattern is called the ""reflection effect."" People discard components shared by all prospects under consideration, a tendency called the ""isolation effect."" Also shows that in choice situations, preferences may be altered by different representations of probabilities. Develops an alternative theory of individual decision making under risk, called prospect theory, developed for simple prospects with monetary outcomes and stated probabilities, in which value is given to gains and losses (i.e., changes in wealth or welfare) rather than to final assets, and probabilities are replaced by decision weights. The theory has two phases. The editing phase organizes and reformulates the options to simplify later evaluation and choice. The edited prospects are evaluated and the highest value prospect chosen. Discusses and models this theory, and offers directions for extending prospect theory are offered. (TNM)"
d7ae2e300cccfe3760e6d8d45d3df046b3c90955,
2edded8fad84a090441ee623e1b65b3d8efcf536,
b0e560b17d89fe17aa8dfe60c0fd29dc7a4d7110,"Abstract : The theoretical basis of decision analysis is utility theory, which describes the principles upon which people wish to base their decisions. This article questions the validity of utility theory and offers an alternative, 'prospect theory.' In addition to providing evidence in support of prospect theory, this paper discusses its implications for the theory and practice of decision analysis. It suggests, for example, ways in which subtle changes in elicitation procedure can have marked effects on people's expressed values."
f0c6886a6b24d0264a11654aaec2ff78cd3a8600,"Subjects performed two rapidly successive tasks. They first pressed a key to stop a digital counter at one of three stopping times. The key-press was followed after a variable interval by the presentation of one of three lights, which required a choice response. 20 students and 20 flight cadets participated in the main experiment. Both responses showed impairment in the dual-task condition. The delay of the choice response was affected by the stopping time and by the interval between the key-press and the second stimulus. Complete refractoriness for a period of 200 msec. was observed only for the shortest stopping-time. In the other conditions, the delay of the second response decreased gradually with decreasing response-stimulus interval. The results do not support single-channel theory and are better explained by a capacity-sharing model on the assumption that preparation for a choice-response requires effort."
c1d2920037f4b274b8927d2f7df82bd99c3b20d5,
dbdb10f066b857baa6e129f49b8d832c3f6267ae,"This article described three heuristics that are employed in making judgements under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgements and decisions in situations of uncertainty."
4140e7481c2599604b14fcd04625274022583631,
85978718f87a0299b6b3fbbc3e8c40210d21942b,"In this paper, we explore the rules that determine intuitive predictions and judgments of confidence and contrast these rules to the normative principles of statistical prediction. Two classes of prediction are discussed: category prediction and numerical prediction. In a categorical case, the prediction is given in nominal form, for example, the winner in an election, the diagnosis of a patient, or a person's future occupation. In a numerical case, the prediction is given in numerical form, for example, the future value of a particular stock or of a student's grade point average. In making predictions and judgments under uncertainty, people do not appear to follow the calculus of chance or the statistical theory of prediction. Instead, they rely on a limited number of heuristics which sometimes yield reasonable judgments and sometimes lead to severe and systematic errors (Kahneman & Tversky, 1972b, 3; Tversky & Kahneman, 1971, 2; 1973, 11). The present paper is concerned with the role of one of these heuristics – representativeness – in intuitive predictions. Given specific evidence (e.g., a personality sketch), the outcomes under consideration (e.g., occupations or levels of achievement) can be ordered by the degree to which they are representative of that evidence. The thesis of this paper is that people predict by representativeness, that is, they select or order outcomes by the degree to which the outcomes represent the essential features of the evidence."
a07ffad799cffee3ef6a2b33f4a56bffcc5b747d,
f47b16748fa891f74033ef05c12c10ed15c47ab3,"Related a test of auditory selective attention, previously validated against criteria of flight proficiency, to the accident rate of 39 professional bus drivers. The test required the listener to monitor a relevant message and ignore a concurrent message presented to the other ear. A change in selective orientation was accompanied by a transient disruption of attention. Raven's Progressive Matrices was also administered, but results were not significantly correlated with the attention test. A measure of proneness to this type of disruption was significantly related to accident rate. (PsycINFO Database Record (c) 2012 APA, all rights reserved) Language: en"
188022ac15cc7e9d375b5b019af8c6ff119f80a8,
3e66089a1ba861eb747c31663f0779c2dbc8f4c2,"A dichotic listening test was constructed which requires S to monitor a relevant message and to ignore a concurrent message presented to the other ear. The test has promising validity for predicting different criteria of proficiency in flying high-performance aircraft. An analysis of the most valid type of errors suggests that a change in an existing orientation is accompanied by a transient instability of selective attention. Most errors in continuous attention are omissions, which indicate a failure of the listening set. Intrusions, which indicate a failure of selectivity, are rare and their frequency is not correlated to flight criteria."
54cb4aa5bea13903c00b5be6e670c2f7e9b1f8b4,"Measurements of pupil size were taken while subjects listened to sentences and either tried to repeat them (R) or answered a question about them (QuA), after either a 3 or 7 sec retention interval. Pupil dilations were larger for R than for QuA, both towards the end of presentation and during the retention interval. Similar results were obtained by Kahneman and Wright (1971) when comparing total and partial recall of word lists. They attributed the differences in pupil dilations to differences in rehearsal strategy. However, the interaction between recall condition and retention interval was less convincing with sentential material, and an alternative explanation is suggested in terms of the level of abstraction at which the sentences are processed. This interpretation is supported by other evidence relating to the existence of alternative sentence retention strategies. Pupil dilations failed to reveal phrase juncture phenomena, although two levels of ambient illumination were used in the hope of detecting such effects. In a modified question condition (QuB), where the question was given before the sentence, pupil dilations varied as a function of the part of the sentence providing the answer. These data indicated that people did not begin to frame their answer until they encountered in the sentence those words used in the question."
894fc603f9b16e775f95045fb805b5d7e6935944,"“Suppose you have run an experiment on 20 subjects, and have obtained a significant result which confirms your theory ( z = 2.23, p If you feel that the probability is somewhere around .85, you may be pleased to know that you belong to a majority group. Indeed, that was the median answer of two small groups who were kind enough to respond to a questionnaire distributed at meetings of the Mathematical Psychology Group and of the American Psychological Association. On the other hand, if you feel that the probability is around .48, you belong to a minority. Only 9 of our 84 respondents gave answers between .40 and .60. However, .48 happens to be a much more reasonable estimate than .85. Apparently, most psychologists have an exaggerated belief in the likelihood of successfully replicating an obtained finding. The sources of such beliefs, and their consequences for the conduct of scientific inquiry, are what this paper is about. Our thesis is that people have strong intuitions about random sampling; that these intuitions are wrong in fundamental respects; that these intuitions are shared by naive subjects and by trained scientists; and that they are applied with unfortunate consequences in the course of scientific inquiry."
9fc2108946dd99b49a82344a09c89a7ea950b8bf,"Twelve subjects learned lists consisting of 3 groups of 4 items each drawn from vocabularies of digits, colour names or boys' names. There were two conditions of recall, total or partial, and two retention intervals, 3 and 7 sec. A view of the function of rehearsal suggests that rehearsal for total recall should be more intense than for partial recall, but only with a brief retention interval. Measurements of pupillary diameter confirm this prediction. Conditions under which pupillary measurements can serve to test theories of psychological processes are discussed."
bab2a73a0eac21074e5ee66e4545756da3618983,
c4618c8bc7972c5e1d731c7b4be249c7d819f1f0,
38effdcf08b982c1396b227da90e0eae035c32b2,"Obtained pupillary measurements for 10 undergraduate Ss during a learning task. On a trial, 8 digit-noun pairs were presented aurally for immediate recall. The digit, even or odd, determined the monetary incentive for retaining the pair. The same nouns were paired to the Digits 2-9 for 8 trials, with a different pairing on every trial. High-reward (HR) items were learned more often than low-reward (LR) items, and occasioned larger pupillary dilations following the presentation of the response noun. Within an incentive class, pupil responses at study did not predict recall. The differential pupil response of Ss to HR and LR items corresponded closely to a behavioral index of preference for HR items. (21 ref.) (PsycINFO Database Record (c) 2012 APA, all rights reserved)"
72dc0f3fe1cb73b95b2e001986069ea58b3e90de,
db290e733994d8e77c4f3cb8bfa2354cf5ca46ec,"Classifies the various paradigms in the study of visual masking and relates them to cases of interference among cotemporaneous stimuli. The dependent variables in masking studies are described. A distinction between criterion content and criterion level is introduced in the discussion of detection under masking and metacontrast. Various conceptions of identification of forms under masking and the contributions of masking effects to the study of psychological time are reviewed. (3 p. ref.) (PsycINFO Database Record (c) 2012 APA, all rights reserved) Language: en"
eb4b291b75be3c37024b81f15d734c73f0d57d0c,
fcf3ddac49a6f529244fe2ad161d8b599253862a,"Subjects heard strings of nine digits for immediate recall, either at a monotone 1/sec. rate or in groups of three digits separated by pauses. Concurrent measurements of pupil size show a steady dilation in the monotone condition, and brief dilation-constriction waves during the pauses of grouped presentation. The results are consistent with hypotheses concerning rehearsal in the two modes of presentation. Pupillary dilations apparently accompany episodes of covert rehearsal."
28789e4fc7ce51e65a6ad0c99151f258e76cc04e,
2db8adaf7392dcda091949fdb25d2516ab3a064f,
bc77a2e76d13f179205057f5026932ba4939c8ed,Subjects monitored for a visual signal while engaged in a demanding mental task. The probability of detecting the signal depends on the time of its presentation during the 8 seconds of the task. A similar time course is observed for failures to detect and for changes of pupil size. Momentary variations in the load that the task imposes on the subject are reflected in both indices. Detection failures are not explained by the pupillary changes.
f667fa5c9136e63c0642ff79c1ca5477d5c40302,
2604830966713f83f61f7edbed0036f6bdf9ee78,"During a short-term memory task, pupil diameter is a measure of the amount of material which is under active processing at any time. The pupil dilates as the material is presented and constricts during report. The rate of change of these functions is related to task difficulty."
447b946c033fcd0084efafa377283d00ec7b93ea,
56120afb431f829839ec6771fd6550dbb726f79e,
620f1dd9a8585e2bf3a003fc04a7c9179a56c07f,"Five Ss recalled telephone numbers from long-term and short-term memory while pupil diameter was measured. Major pupillary changes were found in both tasks. Pupil diameter seems to vary with momentary load on S. Hess & Polt (1964; Hess, 1965) have reported that the pupil of the eye dilates during the solution of arithmetic problems, and that the extent of dilation is related to the difficulty of the problem. More recently, Kahneman & Beatty (submitted) showed that pupil diameter is linearly related to the amount of material stored for immediate recall in a paced digit-span task. The pupil dilates as S listens to a string of digits or words (the loading function) and constricts as he reports these items from short-term memory (the unloading function). The amount of dilation was related to both the length of the string and the difficulty of the task. These results suggested that pupil diameter may have con­ struct validity as a measure of the momentary ""load"" on S as he performs a mental task. The present study was designed to compare the loading and unloading functions in a long-term memory task with those ob­ served in short-term tasks. Method Five undergraduates (two males) served as Ss. Before the experimental session, each S provided a list of five or six telephone numbers with which he was thoroughly familiar, together with a brief name for each number. To test recall from long-term memory, E said one of these names, and S after a brief pause produced the number associated with it. To test short-term memory, E said a number (taken from the list supplied by another S) and S repeated that number. Pictures of the pupil were obtained at l/sec. during each trial. S was positioned in a chin/forehead rest, looking at a fixation surface 10 in. away. A television camera, with a 100 mm telephoto lens and extension tubes, was focused on S's right eye. A half-silvered mirror was placed before S, so that the television camera was not in his field of view. The image ofS's eye was displayed on a monitor, adjusted to provide maximum contrast between iris and pupil. Photographs of this image were made with a Grass C4D Kymograph camera on 35 mm Tri-X film. Pictures were taken at the rate of l/sec. throughout each trial. The click of the Grass camera was audible enough to serve as a pacing device for both E and S. Actual measurements of pupil diameter were later made from projections of this film. In a test of reliability among readers, the standard error of me as­ urement was found to be 0.042 mm. On long-term memory trials, the S was given a ready signal, and the camera began taking pictures at the rate"
920c8d55f10fdb3455974482782e419349a58c3a,"Many sectors of Israeli life are characterized, not only by a rapid rate of change but also by the relative prevalence of planned social change. In response to the economic and social problems ultimately related to the achievement of statehood and the absorption of mass immigration, most public institutions are engaged in inducing and/or controlling change in many spheres of behavior. Recent years have witnessed an increase in the awareness on the part of such institutions, that they may profit from a closer acquaintance with modern social science. In addition to the utilization of sociological research and counseling, requests are made for training; many individuals who have been agents of social change for years are now prepared to take some time (usually very little time) to hear what social psychology has to offer."
a66fa308a101776fdbdf1f8bf925c0a6dd5a3690,
10e7f3eb55a6844cd07ba3f9a8757e4d248d4f10,"The techniques of matched groups, analysis of covariance, and partial correlation represent various approaches to the prevention of a spurious association between Xi and X2 due to a confounding variable, Xa. In all these techniques the use of an unreliable measure for Xa leads to a systematic bias of undercorrection. Adequate corrections are possible for the case of known reliability of XB. Groups should be matched on true scores rather than observed scores, but no correction is possible for the factorial design in which groups are formed on the basis of unreliable correlated measures. Partial correlations should be corrected for the effects of unreliability of the controlled variable. Spuriously high partials are usually obtained when this correction is not applied."
f349cbe089ffc4ec33095a498ba1133d3042c099,"The pre-and post-exposure fields in the tachistoscopic presentation are assumed to reduce the apparent contrast of the figure by brightness summation. A matching procedure was used to measure this effect. Apparent contrast rises linearly with duration, but only in the upper range. Further observations confirm the suggestion that the pre-and post-exposure fields retard the formation of bounding contours with a further reduction of apparent contrast at short durations as a result. It is indicated that the contrast-matching method provides a short-cut technique for the measurement of the temporal range of brightness summation."
3bc63007d92cd82a396410b73dd554e1992ee7bc,"The duration-intensity relationship was investigated for a task in which triads of digits were to be identified. Critical durations (tc) of 200-350 msec. were found for 5 Ss. Under identical stimulus conditions tc for subjective brightness, determined by a matching method, was about 100 msec. for 2 of the Ss. It was concluded that tc varies as a function of perceptual task and that it does not represent the duration of an early ""sensory"" phase of the visual process. (21 ref.) (PsycINFO Database Record (c) 2012 APA, all rights reserved) Language: en"
73f5e191088d78f7b018d79480bb42ca23e464a3,
1ad01ab4db494d1bdf3bb51715ab90012859f8e8,"Detecting the position of the gap in a Landolt C is adversely affected by black bars placed tangential to the C and at a certain distance from it. The maximum bar separation affording interaction is proportional to the minimum angle of resolution, even in cases of amblyopia where resolution is presumably not limited by optical spread of the image. It is suggested that this contour interaction is related to the size of the receptive field (and hence to the resolving capacity) associated with the retinal region used to fixate the target."
b314f50e463dbfa88bccd589a348cfcd934fecfb,
da963f77e4782c370c077ae836b893f9708af54d,"Summary 
 
The validity of tests commonly is expressed in terms of the Pearsonian coefficient of correlation. The use of this coefficient implies that linear homoscedastic relationships hold between test scores and criteria. Therefore, the position is taken that the accuracy of prediction of criteria from test scores is the same throughout the entire range of scores and that both success and failure on a job are the result of the same factors. Examination of the relationships between test scores and criteria for three groups—executives, office workers, and autobus repairmen—indicated that descriptions of the predictive power of the tests and of the traits important in job success and failure as given by the Pearsonian validity coefficient were not entirely adequate. Use of a different index, theta, was shown to give much more meaningful and useful descriptions."
0b02955c7dd16ea323e3a5c2c1be764235aab675,"Since the early 1970s, psychologists have devoted a great deal of attention to human reasoning and decision making, and to the psychological processes that underlie them. While some of this attention was motivated by the intrinsic interest and importance of these processes, much of it was provoked by a series of experimental findings that, in the view of many, had "" bleak implications "" for human rationality (Nisbett and Borgida 1975). In this essay we'll begin, in Section 1, by presenting a brief sketch of some of these disturbing findings, most of which were reported by psychologists in what has become known as the "" heuristics and biases "" tradition. In Section 2, we'll set out three increasingly pessimistic interpretations of the findings that have been suggested by a number of authors. There have been many challenges to these pessimistic interpretations. One of the most interesting and influential challenges was launched, in the early 1990s, by a group of researchers working in the then newly emerging interdisciplinary field of evolutionary psychology. This challenge, and the experimental findings that support it, will be our focus in Section 3. On the basis of these new findings, evolutionary psychologists have suggested a variety of much more optimistic views about the rationality of ordinary people. In Section 4, we'll sketch three of these views. We are inclined to think that the We are grateful for the many valuable discussions, comments and criticisms occasioned by this earlier work. Special thanks are due to"
4822416f70b677d4d15b977ce26fc238a6f76c54,
4878ee0fb16b45c78614021130768e89e7d6c243,"We discuss the cognitive and the psy-chophysical determinants of choice in risky and risk-less contexts. The psychophysics of value induce risk aversion in the domain of gains and risk seeking in the domain of losses. The psychophysics of chance induce overweighting of sure things and of improbable events, relative to events of moderate probability. Decision problems can be described or framed in multiple ways that give rise to different preferences, contrary to the invariance criterion of rational choice. The process of mental accounting, in which people organize the outcomes of transactions, explains some anomalies of consumer behavior. In particular, the acceptability of an option can depend on whether a negative outcome is evaluated as a cost or as an uncompensated loss. The relation between decision values and experience values is discussed. Making decisions is like speaking prose—people do it all the time, knowingly or unknowingly. It is hardly surprising, then, that the topic of decision making is shared by many disciplines, from mathematics and statistics, through economics and political science, to sociology and psychology. The study of decisions addresses both normative and descriptive questions. The normative analysis is concerned with the nature of rationality and the logic of decision making. The descriptive analysis, in contrast, is concerned with peo-ple's beliefs and preferences as they are, not as they should be. The tension between normative and descriptive considerations characterizes much of the study of judgment and choice. Analyses of decision making commonly distinguish risky and riskless choices. The paradigmatic example of decision under risk is the acceptability of a gamble that yields monetary outcomes with specified probabilities. A typical riskless decision concerns the acceptability of a transaction in which a good or a service is exchanged for money or labor. In the first part of this article we present an analysis of the cog-nitive and psychophysical factors that determine the value of risky prospects. In the second part we extend this analysis to transactions and trades. Risky choices, such as whether or not to take an umbrella and whether or not to go to war, are made without advance knowledge of their consequences. Because the consequences of such actions depend on uncertain events such as the weather or the opponent's resolve, the choice of an act may be construed as the acceptance of a gamble that can yield various outcomes with different probabilities. It is therefore natural that the study of …"
79e61996fc52e0506f2b71204bf78cd15c366082,
c4ee492b6c48fee7f899476dfc788e3044f6e238,
